{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project - scraping loopnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daniel-R-Armstrong/Alice-s-Restaurant/blob/master/Project_scraping_loopnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QKSjZ0AaFXg",
        "colab_type": "text"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak9qanXoiZSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from lxml.html import fromstring #get_proxies\n",
        "from itertools import cycle # get_proxies\n",
        "import traceback #get_proxies\n",
        "import datetime\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_iYtnZ5aDsi",
        "colab_type": "code",
        "outputId": "8012c95e-a617-4de6-b615-a5e6ba915916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tnDDMFHaU9h",
        "colab_type": "code",
        "outputId": "979b464e-ed15-495b-8528-a8a6760ba1c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/'My Drive'/data/loopnet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/data/loopnet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPE-i8WuagVo",
        "colab_type": "code",
        "outputId": "9b59a27c-888c-45e6-ef44-6ad81f083956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60007.csv  60133.csv  60174.csv  60190.csv  60504.csv  60527.csv  60564.csv\n",
            "60101.csv  60137.csv  60181.csv  60191.csv  60514.csv  60532.csv  60565.csv\n",
            "60103.csv  60139.csv  60184.csv  60199.csv  60515.csv  60540.csv  60599.csv\n",
            "60106.csv  60143.csv  60185.csv  60439.csv  60516.csv  60555.csv\n",
            "60108.csv  60148.csv  60187.csv  60440.csv  60517.csv  60559.csv\n",
            "60120.csv  60157.csv  60188.csv  60502.csv  60521.csv  60561.csv\n",
            "60126.csv  60172.csv  60189.csv  60503.csv  60523.csv  60563.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTf06ROkRxoh",
        "colab_type": "text"
      },
      "source": [
        "# scrape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU0yzczZvUwY",
        "colab_type": "text"
      },
      "source": [
        "## get free proxies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJyLQgZiigmG",
        "colab_type": "code",
        "outputId": "d474c4c1-5436-4959-9f87-0b75e1111d56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "#https://www.scrapehero.com/how-to-rotate-proxies-and-ip-addresses-using-python-3/\n",
        "\n",
        "from lxml.html import fromstring\n",
        "import requests\n",
        "from itertools import cycle\n",
        "import traceback\n",
        "\n",
        "def get_proxies():\n",
        "    url = 'https://free-proxy-list.net/'\n",
        "    response = requests.get(url)\n",
        "    parser = fromstring(response.text)\n",
        "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'\n",
        "    headers = {'User-Agent': user_agent}\n",
        "    proxies = set()\n",
        "    for i in parser.xpath('//tbody/tr')[:10]:\n",
        "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
        "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
        "            proxies.add(proxy)\n",
        "    return proxies\n",
        "\n",
        "\n",
        "#If you are copy pasting proxy ips, put in the list below\n",
        "#proxies = ['121.129.127.209:80', '124.41.215.238:45169', '185.93.3.123:8080', '194.182.64.67:3128', '106.0.38.174:8080', '163.172.175.210:3128', '13.92.196.150:8080']\n",
        "proxies = get_proxies()\n",
        "proxy_pool = cycle(proxies)\n",
        "\n",
        "url = 'https://httpbin.org/ip'\n",
        "for i in range(1,11):\n",
        "    #Get a proxy from the pool\n",
        "    proxy = next(proxy_pool)\n",
        "    #print(\"Request #%d\"%i)\n",
        "    print(f\"Request #{i}.\")\n",
        "    try:\n",
        "        response = requests.get(url,proxies={\"http\": proxy, \"https\": proxy},headers=headers)\n",
        "        print(response.json())\n",
        "    except:\n",
        "        #Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. \n",
        "        #We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url \n",
        "        print(\"Skipping. Connnection error\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Request #1.\n",
            "Skipping. Connnection error\n",
            "Request #2.\n",
            "Skipping. Connnection error\n",
            "Request #3.\n",
            "Skipping. Connnection error\n",
            "Request #4.\n",
            "Skipping. Connnection error\n",
            "Request #5.\n",
            "Skipping. Connnection error\n",
            "Request #6.\n",
            "Skipping. Connnection error\n",
            "Request #7.\n",
            "Skipping. Connnection error\n",
            "Request #8.\n",
            "Skipping. Connnection error\n",
            "Request #9.\n",
            "Skipping. Connnection error\n",
            "Request #10.\n",
            "Skipping. Connnection error\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfNHlB5jigzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(proxies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Rje5VYwTFG",
        "colab_type": "text"
      },
      "source": [
        "## To do-not started"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3xb9ZVXdlXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I would like to get more data out of this website\n",
        "\n",
        "# https://www.zipdatamaps.com/60185 #60185 is zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK3b6xPav6je",
        "colab_type": "text"
      },
      "source": [
        "# LoopNet scrape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw2eJlGoz1LK",
        "colab_type": "text"
      },
      "source": [
        "## functions for loopnet scrape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNp78QxQk6IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# USED IN PROPERTIES TABLE\n",
        "def title_find(tag):\n",
        "  try:\n",
        "    title = tag.find('span', class_=\"listingTitle\").text\n",
        "    if title == None:\n",
        "      return 'NA'\n",
        "    else:\n",
        "      #print(title)\n",
        "      return title\n",
        "  except Exceptions as e:\n",
        "    print(\"issue with Title: \" + e)\n",
        "    return 'Title error'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY4PZraek6Cm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# USED IN PROPERTIES TABLE\n",
        "def href_find(tag):\n",
        "  try:\n",
        "    href = tag.find('div', class_=\"listingDescription\").a.attrs['href']\n",
        "    if href ==None:\n",
        "      return 'NA'\n",
        "    else:\n",
        "      #print(\"Href:\"+ href)\n",
        "      return href\n",
        "  except EXception as e:\n",
        "    print(\"issue with href: \" + e)\n",
        "    return 'href Error'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBh1_ji27Pe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# USED IN PROPERTIES TABLE\n",
        "def desc_find(tag):\n",
        "  try:\n",
        "    desc = tag.find('span', class_=\"propertyDescription\").text\n",
        "    if desc==None:\n",
        "      return 'NA'\n",
        "    else:\n",
        "      #print(\"Property Description: \" + desc)\n",
        "      return desc\n",
        "  except Exception as e:\n",
        "    Print(\"Issue with Description: \" + e)\n",
        "    return 'Description Error'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Iksw1tl3-qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# USED IN PROPERTIES TABLE\n",
        "def img_find(tag):\n",
        "  try:\n",
        "    img = tag.find('div',class_='listingPhoto').img['src']\n",
        "    if img == None:\n",
        "      return 'NA'\n",
        "    else:\n",
        "      #Print(\"Image: \"+ img)\n",
        "      return img\n",
        "  except Exeption as e:\n",
        "    print(\"issue with Image: \" + e)\n",
        "    return 'Image Error'\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_8nSNaev7gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def price_find(tag):\n",
        "  try:\n",
        "    price = tag.find('td', string = re.compile('Price'))\n",
        "    if price == None:\n",
        "      #print(\"Price: Not Disclosed\")\n",
        "      return 'Not Disclosed'\n",
        "    else:\n",
        "      #print(\"prince: \"+ price.next_sibling.next_sibling.text)\n",
        "      return price.next_sibling.next_sibling.text\n",
        "  except Exception as e:\n",
        "    print(\"issue with price: \" + e)\n",
        "    return 'Price Error'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB51q_uxwGM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ptype_find(tag):\n",
        "  ptype = tag.find('td', string = re.compile('Property Type'))\n",
        "  if ptype == None:\n",
        "    #print(\"Property Type: Not Disclosed\")\n",
        "    return 'Not Disclosed'\n",
        "  else:\n",
        "    #print(\"Property Type: \" + ptype.next_sibling.next_sibling.text )\n",
        "    return ptype.next_sibling.next_sibling.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIRRjATswK7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stype_find(tag):\n",
        "  stype = tag.find('td', string = re.compile('Sub-Type'))\n",
        "  if stype == None:\n",
        "    #print(\"Sub-Type: Not Disclosed\")\n",
        "    return 'Not Disclosed'\n",
        "  else:\n",
        "    #print(\"Sub-Type: \"+ stype.next_sibling.next_sibling.text )\n",
        "    return stype.next_sibling.next_sibling.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0szMSlRWwK2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spaces_find(tag):\n",
        "  spaces = tag.find('td', string = re.compile('Spaces'))\n",
        "  if spaces == None:\n",
        "    #print(\"Spaces: Not Disclosed\")\n",
        "    return 'Not Disclosed'\n",
        "  else:\n",
        "    #print(\"Spaces: \"+ spaces.next_sibling.next_sibling.text)\n",
        "    return spaces.next_sibling.next_sibling.text "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY2XLWPswKy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def size_find(tag):\n",
        "  size = tag.find('td', string = re.compile('Building Size'))\n",
        "  if size == None:\n",
        "    #print(\"Buliding Size: Not Disclosed\")\n",
        "    return 'Not Disclosed'\n",
        "  else:\n",
        "    #print(\"Building Size: \"+ size.next_sibling.next_sibling.text)\n",
        "    return size.next_sibling.next_sibling.text  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10C1QS4cwUUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def space_find(tag):\n",
        "  space = tag.find('td', string = re.compile('Space Available'))\n",
        "  if space == None:\n",
        "    #print(\"Space Available: Not Disclosed\")\n",
        "    return 'Not Disclosed'\n",
        "  else:\n",
        "    #print(\"Space Available: \"+ space.next_sibling.next_sibling.text)\n",
        "    return space.next_sibling.next_sibling.text "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoekLTwpwUXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cap_find(tag):\n",
        "  cap = tag.find('td', string = re.compile('Cap Rate'))\n",
        "  if cap == None:\n",
        "    #print(\"Cap Rate: Not Disclosed\")\n",
        "    return 'Not Disclosed'\n",
        "  else:\n",
        "    #print(\"Cap Rate: \"+ cap.next_sibling.next_sibling.text)\n",
        "    return cap.next_sibling.next_sibling.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWezsm4vwUQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_find(tag):\n",
        "  bclass = tag.find('td', string = re.compile('Building Class'))\n",
        "  if bclass == None:\n",
        "    #print(\"Building Class: Not Disclosed\")\n",
        "    return 'Not Disclosed'\n",
        "  else:\n",
        "    #print(\"Building Class: \"+ bclass.next_sibling.next_sibling.text)\n",
        "    return bclass.next_sibling.next_sibling.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-BwtGRtwKtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lot_find(tag):\n",
        "  lot = tag.find('td', string = re.compile('Lot Size'))\n",
        "  if lot == None:\n",
        "    #print(\"Lot Size: Not Disclosed\")\n",
        "    return 'Not Disclosed'\n",
        "  else:\n",
        "    #print(\"Lot Size: \" + lot.next_sibling.next_sibling.text)\n",
        "    return lot.next_sibling.next_sibling.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSTARpJ4mqhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pg_limit(tag):\n",
        "  try:\n",
        "    page_nums = tag.find('div', class_ = 'searchPaging').get_text()\n",
        "    borderless = tag.find('a', class_ = 'searchPagingBorderless').get_text()\n",
        "    result = re.sub('[^0-9.]','', page_nums)\n",
        "    dots = result.find('...')\n",
        "    if dots > 0:\n",
        "        return int(result[dots+3:])\n",
        "    else:\n",
        "        return int(borderless)\n",
        "        \n",
        "  except Exception as e:\n",
        "      print (e)\n",
        "      return int(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTs06oYWJ-A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = []\n",
        "def properties_table_maker(soup):\n",
        "  page = soup.find_all('div', {re.compile('listingContainer*')})\n",
        "                               \n",
        "  for con in page:\n",
        "    listing = {}\n",
        "    listing['Property Name'] = title_find(con)\n",
        "    listing['Listing URL'] = href_find(con)\n",
        "    listing['Short Description']= desc_find(con)\n",
        "    listing['Listing Image']= img_find(con)\n",
        "    listing['Price']= price_find(con)\n",
        "    listing['Property Type']= ptype_find(con)\n",
        "    listing['Sub-Type']= stype_find(con)\n",
        "    listing['Spaces']= spaces_find(con)\n",
        "    listing['Building Size']= size_find(con)\n",
        "    listing['Space Available']= space_find(con)\n",
        "    listing['Cap Rate']= cap_find(con)\n",
        "    listing['Building Class']= class_find(con)\n",
        "    listing['Lot Size']= lot_find(con)\n",
        "    listing['Date checked']= datetime.datetime.now(datetime.timezone.utc)\n",
        "    results.append(listing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKqz7IouloKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#todo get headers\n",
        "#todo buy and use proxie list\n",
        "from warnings import warn # warning about response(status code not 200)\n",
        "from time import time # time functions\n",
        "from time import sleep # sleeping functions\n",
        "from random import randint # for random integer used in sleep\n",
        "import requests\n",
        "from IPython.core.display import clear_output #for clear output\n",
        "\n",
        "#proxies = {'http':'https://45.229.104.185:33903'}\n",
        "#url = 'https://httpbin.org/user-agent' # tests your user agent\n",
        "#url = 'https://httpbin.org/ip' # test proxies\n",
        "#url = 'https://ipinfo.io #test ip address too\n",
        "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\"\n",
        "headers = {'User-Agent': user_agent}\n",
        "\n",
        "\n",
        "def listings_scrape(zipcode):\n",
        "  url_one = 'https://www.loopnet.com/zip/' + zipcode + '-commercial-real-estate/'\n",
        "  #page_position = url_one.index('')\n",
        "  start_time = time()\n",
        "  num_requests = 0\n",
        "  try:\n",
        "    results =[]\n",
        "    page_one = requests.get(url_one,headers=headers)# add proxies=proxies\n",
        "    page_one_soup = BeautifulSoup(page_one.content.decode(\"utf-8\"),\"html.parser\")# pick a parser['lxml', 'html.parser', 'html5lib', 'xml']\n",
        "    print(\"there is \"+ str(pg_limit(page_one_soup))+\" pages of listings for zipcode: \"+ zipcode)\n",
        "    properties_table_maker(page_one_soup)\n",
        "    sleep(randint(8,15))\n",
        "    num_requests += 1\n",
        "    elapsed_time = time() - start_time\n",
        "    file_name = zipcode + \".csv\"\n",
        "    frequency = num_requests/elapsed_time\n",
        "    print('Request:{}; Frequency: {} num_requests/s'.format(num_requests, frequency))\n",
        "    clear_output(wait = True)\n",
        "    \n",
        "    #pages iterator\n",
        "    for i in range(2, pg_limit(page_one_soup) + 1):\n",
        "      i_str = str(i)\n",
        "      url = url_one + i_str + '/'\n",
        "      page = requests.get(url, headers=headers)# add proxies=proxies)\n",
        "      page_soup = BeautifulSoup(page.content.decode(\"utf-8\"), \"html.parser\")\n",
        "      properties_table_maker(page_soup)\n",
        "      \n",
        "      #sleep and track\n",
        "      sleep(randint(4,10)) #8,15 worked for dupage on 06/23/2019\n",
        "      num_requests += 1\n",
        "      elapsed_time = time() - start_time\n",
        "      print('Request:{}; Frequency: {} num_requests/s'.format(num_requests, frequency))\n",
        "      clear_output(wait = True)\n",
        "      \n",
        "      # Throw a warning for non-200 status codes\n",
        "      if page.status_code != 200:\n",
        "        warn('Request: {}; Status code: {}'.format(num_requests, page.status_code))\n",
        "      \n",
        "      # Break the loop if the number of requests is greater than expected\n",
        "      if num_requests > 72:\n",
        "        warn('Number of requests was greater than expected.')\n",
        "        break \n",
        "        \n",
        "    keys = results[0].keys()\n",
        "    with open(file_name, 'w') as output_file:\n",
        "        dict_writer = csv.DictWriter(output_file, keys)\n",
        "        dict_writer.writeheader()\n",
        "        dict_writer.writerows(results)\n",
        "    return results\n",
        "      #return spaces_df.to_csv(zipcode + '.csv')\n",
        "        \n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hIDjJ_fz_jy",
        "colab_type": "text"
      },
      "source": [
        "##Loopnet by zipcode scrape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iA9R3xHYQRD",
        "colab_type": "text"
      },
      "source": [
        "##zips by county"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By7m9SxPiTcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cook_zips=['60004','60005','60007','60008','60016','60018',\n",
        "           '60022','60025','60026','60029','60043','60053',\n",
        "           '60056','60062','60067','60068','60070','60074',\n",
        "           '60076','60077','60090','60091','60093','60104',\n",
        "           '60107','60130','60131','60133','60141','60153',\n",
        "           '60154','60155','60160','60162','60163','60164',\n",
        "           '60165','60168','60169','60171','60173','60176',\n",
        "           '60192','60193','60194','60195','60201','60202',\n",
        "           '60203','60301','60302','60304','60305','60402',\n",
        "           '60406','60409','60411','60415','60419','60422',\n",
        "           '60425','60426','60428','60429','60430','60438',\n",
        "           '60439','60443','60445','60452','60453','60455',\n",
        "           '60456','60457','60458','60459','60461','60462',\n",
        "           '60463','60464','60465','60466','60467','60469',\n",
        "           '60471','60472','60473','60475','60476','60477',\n",
        "           '60478','60480','60482','60483','60487','60501',\n",
        "           '60513','60525','60526','60534','60546','60558',\n",
        "           '60601','60602','60603','60604','60605','60606',\n",
        "           '60607','60608','60609','60610','60611','60612',\n",
        "           '60613','60614','60615','60616','60617','60618',\n",
        "           '60619','60620','60621','60622','60623','60624',\n",
        "           '60625','60626','60628','60629','60630','60631',\n",
        "           '60632','60633','60634','60636','60637','60638',\n",
        "           '60639','60640','60641','60643','60644','60645',\n",
        "           '60646','60647','60649','60651','60652','60653',\n",
        "           '60654','60655','60656','60657','60659','60660',\n",
        "           '60661','60706','60707','60712','60714','60803',\n",
        "           '60804','60805','60827']\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UDm9pdZnA63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dupage_zips=['60101','60103','60106','60108','60126', \n",
        "             '60137','60139','60143','60148','60157', \n",
        "             '60172','60181','60184','60185','60187', \n",
        "             '60188','60189','60190','60191','60502', \n",
        "             '60504','60514','60515','60516','60517', \n",
        "             '60521','60523','60527','60532','60540',\n",
        "             '60555','60559','60561','60563','60565']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3w5pdQwYWfr",
        "colab_type": "text"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWNyFXbwFtuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# USED IN PROPERTIES TABLE\n",
        "def title_find(tag):\n",
        "    try:\n",
        "        title = tag.find('span', class_=\"listingTitle\").text\n",
        "        if title == None:\n",
        "            return 'NA'\n",
        "        else:\n",
        "            #print(title)\n",
        "            return title\n",
        "    except Exceptions as e:\n",
        "        print(\"issue with Title: \" + e)\n",
        "        return 'Title error'\n",
        "    \n",
        "def href_find(tag):\n",
        "    try:\n",
        "        href = tag.find('div', class_=\"listingDescription\").a.attrs['href']\n",
        "        if href ==None:\n",
        "            return 'NA'\n",
        "        else:\n",
        "            #print(\"Href:\"+ href)\n",
        "            return href\n",
        "    except EXception as e:\n",
        "        print(\"issue with href: \" + e)\n",
        "        return 'href Error'\n",
        "    \n",
        "def desc_find(tag):\n",
        "    try:\n",
        "        desc = tag.find('span', class_=\"propertyDescription\").text\n",
        "        if desc==None:\n",
        "            return 'NA'\n",
        "        else:\n",
        "            #print(\"Property Description: \" + desc)\n",
        "            return desc\n",
        "    except Exception as e:\n",
        "        Print(\"Issue with Description: \" + e)\n",
        "        return 'Description Error'\n",
        "    \n",
        "def img_find(tag):\n",
        "    try:\n",
        "        img = tag.find('div',class_='listingPhoto').img['src']\n",
        "        if img == None:\n",
        "            return 'NA'\n",
        "        else:\n",
        "            #Print(\"Image: \"+ img)\n",
        "            return img\n",
        "    except Exeption as e:\n",
        "        print(\"issue with Image: \" + e)\n",
        "        return 'Image Error'\n",
        "    \n",
        "def price_find(tag):\n",
        "    try:\n",
        "        price = tag.find('td', string = re.compile('Price'))\n",
        "        if price == None:\n",
        "            #print(\"Price: Not Disclosed\")\n",
        "            return 'Not Disclosed'\n",
        "        else:\n",
        "            #print(\"prince: \"+ price.next_sibling.next_sibling.text)\n",
        "            return price.next_sibling.next_sibling.text\n",
        "    except Exception as e:\n",
        "        print(\"issue with price: \" + e)\n",
        "        return 'Price Error'\n",
        "    \n",
        "def ptype_find(tag):\n",
        "    ptype = tag.find('td', string = re.compile('Property Type'))\n",
        "    if ptype == None:\n",
        "        #print(\"Property Type: Not Disclosed\")\n",
        "        return 'Not Disclosed'\n",
        "    else:\n",
        "        #print(\"Property Type: \" + ptype.next_sibling.next_sibling.text )\n",
        "        return ptype.next_sibling.next_sibling.text\n",
        "    \n",
        "def stype_find(tag):\n",
        "    stype = tag.find('td', string = re.compile('Sub-Type'))\n",
        "    if stype == None:\n",
        "        #print(\"Sub-Type: Not Disclosed\")\n",
        "        return 'Not Disclosed'\n",
        "    else:\n",
        "        #print(\"Sub-Type: \"+ stype.next_sibling.next_sibling.text )\n",
        "        return stype.next_sibling.next_sibling.text\n",
        "    \n",
        "def spaces_find(tag):\n",
        "    spaces = tag.find('td', string = re.compile('Spaces'))\n",
        "    if spaces == None:\n",
        "        #print(\"Spaces: Not Disclosed\")\n",
        "        return 'Not Disclosed'\n",
        "    else:\n",
        "        #print(\"Spaces: \"+ spaces.next_sibling.next_sibling.text)\n",
        "        return spaces.next_sibling.next_sibling.text\n",
        "    \n",
        "def size_find(tag):\n",
        "    size = tag.find('td', string = re.compile('Building Size'))\n",
        "    if size == None:\n",
        "        #print(\"Buliding Size: Not Disclosed\")\n",
        "        return 'Not Disclosed'\n",
        "    else:\n",
        "        #print(\"Building Size: \"+ size.next_sibling.next_sibling.text)\n",
        "        return size.next_sibling.next_sibling.text    \n",
        "    \n",
        "def space_find(tag):\n",
        "    space = tag.find('td', string = re.compile('Space Available'))\n",
        "    if space == None:\n",
        "        #print(\"Space Available: Not Disclosed\")\n",
        "        return 'Not Disclosed'\n",
        "    else:\n",
        "        #print(\"Space Available: \"+ space.next_sibling.next_sibling.text)\n",
        "        return space.next_sibling.next_sibling.text     \n",
        "\n",
        "def cap_find(tag):\n",
        "    cap = tag.find('td', string = re.compile('Cap Rate'))\n",
        "    if cap == None:\n",
        "        #print(\"Cap Rate: Not Disclosed\")\n",
        "        return 'Not Disclosed'\n",
        "    else:\n",
        "        #print(\"Cap Rate: \"+ cap.next_sibling.next_sibling.text)\n",
        "        return cap.next_sibling.next_sibling.text    \n",
        "    \n",
        "def class_find(tag):\n",
        "    bclass = tag.find('td', string = re.compile('Building Class'))\n",
        "    if bclass == None:\n",
        "        #print(\"Building Class: Not Disclosed\")\n",
        "        return 'Not Disclosed'\n",
        "    else:\n",
        "        #print(\"Building Class: \"+ bclass.next_sibling.next_sibling.text)\n",
        "        return bclass.next_sibling.next_sibling.text\n",
        "    \n",
        "def lot_find(tag):\n",
        "    lot = tag.find('td', string = re.compile('Lot Size'))\n",
        "    if lot == None:\n",
        "        #print(\"Lot Size: Not Disclosed\")\n",
        "        return 'Not Disclosed'\n",
        "    else:\n",
        "        #print(\"Lot Size: \" + lot.next_sibling.next_sibling.text)\n",
        "        return lot.next_sibling.next_sibling.text\n",
        "    \n",
        "def pg_limit(tag):\n",
        "    try:\n",
        "        page_nums = tag.find('div', class_ = 'searchPaging').get_text()\n",
        "        borderless = tag.find('a', class_ = 'searchPagingBorderless').get_text()\n",
        "        result = re.sub('[^0-9.]','', page_nums)\n",
        "        dots = result.find('...')\n",
        "        if dots > 0:\n",
        "            return int(result[dots+3:])\n",
        "        else:\n",
        "            return int(borderless)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print (e)\n",
        "        return int(1)    \n",
        "    \n",
        "def properties_table_maker(soup):\n",
        "    page = soup.find_all('div', {re.compile('listingContainer*')})\n",
        "    # results = []                           \n",
        "    for con in page:\n",
        "        listing = {}\n",
        "        listing['Property Name'] = title_find(con)\n",
        "        listing['Listing URL'] = href_find(con)\n",
        "        listing['Short Description']= desc_find(con)\n",
        "        listing['Listing Image']= img_find(con)\n",
        "        listing['Price']= price_find(con)\n",
        "        listing['Property Type']= ptype_find(con)\n",
        "        listing['Sub-Type']= stype_find(con)\n",
        "        listing['Spaces']= spaces_find(con)\n",
        "        listing['Building Size']= size_find(con)\n",
        "        listing['Space Available']= space_find(con)\n",
        "        listing['Cap Rate']= cap_find(con)\n",
        "        listing['Building Class']= class_find(con)\n",
        "        listing['Lot Size']= lot_find(con)\n",
        "        listing['Date checked']= datetime.datetime.now(datetime.timezone.utc)\n",
        "        results.append(listing)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqhPbZXYMD8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#todo get headers\n",
        "#todo buy and use proxie list\n",
        "from warnings import warn # warning about response(status code not 200)\n",
        "from time import time # time functions\n",
        "from time import sleep # sleeping functions\n",
        "from random import randint # for random integer used in sleep\n",
        "import requests\n",
        "from IPython.core.display import clear_output #for clear output\n",
        "\n",
        "#proxies = {'http':'https://45.229.104.185:33903'}\n",
        "#url = 'https://httpbin.org/user-agent' # tests your user agent\n",
        "#url = 'https://httpbin.org/ip' # test proxies\n",
        "#url = 'https://ipinfo.io #test ip address too\n",
        "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\"\n",
        "headers = {'User-Agent': user_agent}\n",
        "results =[]\n",
        "\n",
        "def listings_scrape(zipcode):\n",
        "    url_one = 'https://www.loopnet.com/zip/' + zipcode + '-commercial-real-estate/'\n",
        "    #page_position = url_one.index('')\n",
        "    start_time = time()\n",
        "    num_requests = 0\n",
        "    results.clear()\n",
        "    try:\n",
        "        page_one = requests.get(url_one,headers=headers)# add proxies=proxies\n",
        "        page_one_soup = BeautifulSoup(page_one.content.decode(\"utf-8\"),\"html.parser\")# pick a parser['lxml', 'html.parser', 'html5lib', 'xml']\n",
        "        print(\"there is \"+ str(pg_limit(page_one_soup))+\" pages of listings for zipcode: \"+ zipcode)\n",
        "        properties_table_maker(page_one_soup)\n",
        "        num_requests += 1\n",
        "        elapsed_time = time() - start_time\n",
        "        file_name = zipcode + \".csv\"\n",
        "        frequency = num_requests/elapsed_time\n",
        "        print('zipcode: {}; Request:{}; Frequency: {} num_requests/s'.format(zipcode, num_requests, frequency))\n",
        "        clear_output(wait = True)\n",
        "    \n",
        "        # pages iterator\n",
        "        for i in range(2, pg_limit(page_one_soup) + 1):\n",
        "            i_str = str(i)\n",
        "            url = url_one + i_str + '/'\n",
        "            page = requests.get(url, headers=headers)# add proxies=proxies)\n",
        "            page_soup = BeautifulSoup(page.content.decode(\"utf-8\"), \"html.parser\")\n",
        "            properties_table_maker(page_soup)\n",
        "            # sleep and track\n",
        "            sleep(randint(1,3)) #8,15 worked for dupage on 06/23/2019\n",
        "            num_requests += 1\n",
        "            elapsed_time = time() - start_time\n",
        "            print('Request:{}; Frequency: {} num_requests/s'.format(num_requests, frequency))\n",
        "            clear_output(wait = True)\n",
        "            # Throw a warning for non-200 status codes\n",
        "            if page.status_code != 200:\n",
        "                warn('Request: {}; Status code: {}'.format(num_requests, page.status_code))\n",
        "            # Break the loop if the number of requests is greater than expected\n",
        "            if num_requests > 72:\n",
        "                warn('Number of requests was greater than expected.')\n",
        "                break \n",
        "            \n",
        "        keys = results[0].keys()\n",
        "        with open(file_name, 'w') as output_file:\n",
        "            dict_writer = csv.DictWriter(output_file, keys)\n",
        "            dict_writer.writeheader()\n",
        "            dict_writer.writerows(results)\n",
        "    \n",
        "        # return spaces_df.to_csv(zipcode + '.csv')\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6htZSo4n25W",
        "colab_type": "code",
        "outputId": "0b5e3bcc-1f27-4cdd-cb19-fe3758f8a8f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i in dupage_zips:\n",
        "    listings_scrape(i)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'get_text'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwWhM-wmBCxs",
        "colab_type": "code",
        "outputId": "495ae8b3-cd50-4f0a-86cd-71b329ff886a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i in cook_zips:\n",
        "    listings_scrape(i)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'get_text'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMe0KBPBlxd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# results =[]\n",
        "# listings_scrape('60199')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFulwPkm7MXb",
        "colab_type": "code",
        "outputId": "59c8339e-e3f3-4244-f5c1-1562b059e87d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60101.csv  60137.csv  60181.csv  60189.csv  60514.csv  60523.csv  60559.csv\n",
            "60103.csv  60139.csv  60184.csv  60190.csv  60515.csv  60527.csv  60561.csv\n",
            "60106.csv  60143.csv  60185.csv  60191.csv  60516.csv  60532.csv  60563.csv\n",
            "60108.csv  60148.csv  60187.csv  60502.csv  60517.csv  60540.csv  60565.csv\n",
            "60126.csv  60172.csv  60188.csv  60504.csv  60521.csv  60555.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKlx6Jxn5qGQ",
        "colab_type": "text"
      },
      "source": [
        "Check the length of each of the csv files\n",
        "```\n",
        "! wc -l *.csv \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W1GoMchj8fi",
        "colab_type": "code",
        "outputId": "20969c76-d742-4121-bdd9-8a7af9ae2374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3449
        }
      },
      "source": [
        "! wc -l *.csv "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    165 60004.csv\n",
            "     98 60005.csv\n",
            "    283 60007.csv\n",
            "    113 60008.csv\n",
            "     74 60016.csv\n",
            "    158 60018.csv\n",
            "     14 60022.csv\n",
            "    102 60025.csv\n",
            "     39 60026.csv\n",
            "      8 60043.csv\n",
            "     67 60053.csv\n",
            "    120 60056.csv\n",
            "    258 60062.csv\n",
            "    150 60067.csv\n",
            "     75 60068.csv\n",
            "     27 60070.csv\n",
            "    105 60074.csv\n",
            "    128 60076.csv\n",
            "    111 60077.csv\n",
            "    165 60090.csv\n",
            "     32 60091.csv\n",
            "     60 60093.csv\n",
            "    151 60101.csv\n",
            "    108 60103.csv\n",
            "     29 60104.csv\n",
            "    142 60106.csv\n",
            "     95 60107.csv\n",
            "     74 60108.csv\n",
            "    147 60126.csv\n",
            "     39 60130.csv\n",
            "    156 60131.csv\n",
            "     55 60133.csv\n",
            "     85 60137.csv\n",
            "     60 60139.csv\n",
            "     97 60143.csv\n",
            "    206 60148.csv\n",
            "     38 60153.csv\n",
            "     46 60154.csv\n",
            "     37 60155.csv\n",
            "    141 60160.csv\n",
            "     42 60162.csv\n",
            "      2 60163.csv\n",
            "     46 60164.csv\n",
            "      8 60165.csv\n",
            "     75 60169.csv\n",
            "     17 60171.csv\n",
            "     53 60172.csv\n",
            "    290 60173.csv\n",
            "     41 60176.csv\n",
            "    103 60181.csv\n",
            "      2 60184.csv\n",
            "    211 60185.csv\n",
            "    142 60187.csv\n",
            "    140 60188.csv\n",
            "     60 60189.csv\n",
            "     29 60190.csv\n",
            "     63 60191.csv\n",
            "     58 60192.csv\n",
            "     87 60193.csv\n",
            "     27 60194.csv\n",
            "     54 60195.csv\n",
            "    104 60201.csv\n",
            "     38 60202.csv\n",
            "     43 60301.csv\n",
            "     56 60302.csv\n",
            "     32 60304.csv\n",
            "     18 60305.csv\n",
            "     40 60402.csv\n",
            "     46 60406.csv\n",
            "     65 60409.csv\n",
            "    105 60411.csv\n",
            "     29 60415.csv\n",
            "     30 60419.csv\n",
            "     16 60422.csv\n",
            "     15 60425.csv\n",
            "     33 60426.csv\n",
            "     19 60428.csv\n",
            "     30 60429.csv\n",
            "     67 60430.csv\n",
            "     52 60438.csv\n",
            "     50 60439.csv\n",
            "     86 60443.csv\n",
            "     51 60445.csv\n",
            "     52 60452.csv\n",
            "     92 60453.csv\n",
            "     50 60455.csv\n",
            "      5 60456.csv\n",
            "     25 60457.csv\n",
            "     20 60458.csv\n",
            "     24 60459.csv\n",
            "     48 60461.csv\n",
            "    148 60462.csv\n",
            "     45 60463.csv\n",
            "      5 60464.csv\n",
            "     18 60465.csv\n",
            "     53 60466.csv\n",
            "    117 60467.csv\n",
            "      3 60469.csv\n",
            "     29 60471.csv\n",
            "      3 60472.csv\n",
            "     60 60473.csv\n",
            "     10 60475.csv\n",
            "      7 60476.csv\n",
            "    123 60477.csv\n",
            "     18 60478.csv\n",
            "     16 60480.csv\n",
            "     22 60482.csv\n",
            "     38 60487.csv\n",
            "     21 60501.csv\n",
            "    143 60502.csv\n",
            "    119 60504.csv\n",
            "     13 60513.csv\n",
            "     15 60514.csv\n",
            "    241 60515.csv\n",
            "     24 60516.csv\n",
            "    113 60517.csv\n",
            "     70 60521.csv\n",
            "    125 60523.csv\n",
            "    127 60525.csv\n",
            "     21 60526.csv\n",
            "    121 60527.csv\n",
            "    151 60532.csv\n",
            "     28 60534.csv\n",
            "    122 60540.csv\n",
            "     36 60546.csv\n",
            "    122 60555.csv\n",
            "     15 60558.csv\n",
            "     95 60559.csv\n",
            "     35 60561.csv\n",
            "    259 60563.csv\n",
            "     17 60565.csv\n",
            "     80 60601.csv\n",
            "     84 60602.csv\n",
            "     51 60603.csv\n",
            "     50 60604.csv\n",
            "     75 60605.csv\n",
            "    118 60606.csv\n",
            "    216 60607.csv\n",
            "    195 60608.csv\n",
            "    163 60609.csv\n",
            "     99 60610.csv\n",
            "    117 60611.csv\n",
            "    163 60612.csv\n",
            "     78 60613.csv\n",
            "    199 60614.csv\n",
            "     37 60615.csv\n",
            "    106 60616.csv\n",
            "     96 60617.csv\n",
            "    222 60618.csv\n",
            "    101 60619.csv\n",
            "     89 60620.csv\n",
            "     21 60621.csv\n",
            "    236 60622.csv\n",
            "    137 60623.csv\n",
            "     65 60624.csv\n",
            "     86 60625.csv\n",
            "     59 60626.csv\n",
            "     90 60628.csv\n",
            "     78 60629.csv\n",
            "    120 60630.csv\n",
            "     67 60631.csv\n",
            "    126 60632.csv\n",
            "     13 60633.csv\n",
            "     95 60634.csv\n",
            "     74 60636.csv\n",
            "     44 60637.csv\n",
            "    104 60638.csv\n",
            "    130 60639.csv\n",
            "    128 60640.csv\n",
            "    155 60641.csv\n",
            "     54 60643.csv\n",
            "     86 60644.csv\n",
            "     36 60645.csv\n",
            "     99 60646.csv\n",
            "    258 60647.csv\n",
            "    101 60649.csv\n",
            "    114 60651.csv\n",
            "     30 60652.csv\n",
            "     24 60653.csv\n",
            "    166 60654.csv\n",
            "     17 60655.csv\n",
            "     24 60656.csv\n",
            "    179 60657.csv\n",
            "     71 60659.csv\n",
            "     57 60660.csv\n",
            "     67 60661.csv\n",
            "     31 60706.csv\n",
            "     65 60707.csv\n",
            "     90 60712.csv\n",
            "    115 60714.csv\n",
            "     65 60803.csv\n",
            "     89 60804.csv\n",
            "     57 60805.csv\n",
            "     35 60827.csv\n",
            "  15794 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiR4SAmZu4OY",
        "colab_type": "code",
        "outputId": "7726a9bf-3f42-4baf-9e5a-626ddeb0778a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c=pd.read_csv(\"60190.csv\")\n",
        "c['Listing URL'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/Listing/27W093-27W121-Geneva-Rd-Winfield-IL/11956940/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2dRVchq8_kI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "https://www.loopnet.com/Listing/27W230-Beecher-Ave-Winfield-IL/9456034/\t\t\n",
        "\n",
        "\n",
        "Listing ID: 9456034 Date Created: 10/5/2017 Last Updated: 6/10/2019\n",
        "\n",
        "DESCRIPTION, HIGHLIGHTS, more pictures, SALE NOTES, AMENITIES, UTILITIES"
      ]
    }
  ]
}