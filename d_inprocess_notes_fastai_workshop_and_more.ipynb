{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "d_inprocess_notes_fastai_workshop and more.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daniel-R-Armstrong/Alice-s-Restaurant/blob/master/d_inprocess_notes_fastai_workshop_and_more.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YZISf9oADFtE"
      },
      "cell_type": "markdown",
      "source": [
        "Congrats to all winner teams and new grandmaster sjv. Thanks to kaggle for hosting and Favorita for sponsoring this great competition. Special thanks to @sjv, @senkin13, @tunguz, @ceshine, we build our models based on your kernels.\n",
        "\n",
        "https://github.com/sjvasquez/web-traffic-forecasting/blob/master/cnn.py\n",
        "https://www.kaggle.com/senkin13/lstm-starter/code\n",
        "https://www.kaggle.com/tunguz/lgbm-one-step-ahead-lb-0-513\n",
        "https://www.kaggle.com/ceshine/lgbm-starter\n",
        "Like the Rossmann competiton, the private leaderboard shaked up again this time. I think luck is on our side finally.\n",
        "\n",
        "Sample Selection\n",
        "we used only 2017 data to extract features and construct samples.\n",
        "\n",
        "train data：20170531 - 20170719 or 20170614 - 20170719， different models are trained with different data set.\n",
        "\n",
        "validition: 20170726 - 20170810\n",
        "\n",
        "In fact, we tried to use more data but failed. The gap between public and private leadboard is not very stable. If we train a single model for data of 16 days, the gap will be smaller(0.002-0.003).\n",
        "\n",
        "Preprocessing\n",
        "We just filled missing or negtive promotion and target values with 0.\n",
        "\n",
        "Feature Engineering\n",
        "basic features\n",
        "category features: store, item, famlily, class, cluster...\n",
        "promotion\n",
        "dayofweek(only for model 3)\n",
        "statitical features: we use some methods to stat some targets for different keys in different time windows\n",
        "time windows\n",
        "nearest days: [1,3,5,7,14,30,60,140]\n",
        "equal time windows: [1] * 16, [7] * 20...\n",
        "key：store x item, item, store x class\n",
        "target: promotion, unit_sales, zeros\n",
        "method\n",
        "mean, median, max, min, std\n",
        "days since last appearance\n",
        "difference of mean value between adjacent time windows(only for equal time windows)\n",
        "useless features\n",
        "holidays\n",
        "other keys such as: cluster x item, store x family...\n",
        "Single Model\n",
        "model_1 : 0.506 / 0.511 , 16 lgb models trained for each day source code\n",
        "model_2 : 0.507 / 0.513 , 16 nn models trained for each day source code\n",
        "model_3 : 0.512 / 0.515，1 lgb model for 16 days with almost same features as model_1\n",
        "model_4 : 0.517 / 0.519，1 nn model based on @sjv's code\n",
        "Ensemble\n",
        "Stacking doesn't work well this time, our best model is linear blend of 4 single models.\n",
        "\n",
        "final submission = 0.42*model_1 + 0.28 * model_2 + 0.18 * model_3 + 0.12 * model_4\n",
        "\n",
        "public = 0.504 , private = 0.509"
      ]
    },
    {
      "metadata": {
        "id": "Pj56pIKjfnUj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "m8_s21Qnjvgw"
      },
      "cell_type": "markdown",
      "source": [
        "**cat** (Linux) reads the file/concatenates lines, example: __> $ cat output.txt"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "m5nWonB-n-Bk"
      },
      "cell_type": "markdown",
      "source": [
        " **grep** (Linux) "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cOb5RvMSs1nS"
      },
      "cell_type": "markdown",
      "source": [
        "**du** (Linux)  shows the sizes of directories and files."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BUOgKRQ3tFft"
      },
      "cell_type": "markdown",
      "source": [
        "**mkdir:** (Linux) creates new directories."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vYV0B1CttWyS"
      },
      "cell_type": "markdown",
      "source": [
        "** pwd:**  (Linux) short for present working directory, it displays the full path to the current directory."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GIeMKjX-t1Ht"
      },
      "cell_type": "markdown",
      "source": [
        "**rm: ** (Linux)   deletes the specified files and directories."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NRpDiq4qt3Vl"
      },
      "cell_type": "markdown",
      "source": [
        "**rmdir:**  (Linux) deletes the specified empty directories"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "SKiombWLgLf2"
      },
      "cell_type": "markdown",
      "source": [
        "**touch** (Linux)- used to create file, example:  __>   *$ touch output.txt*\n",
        "\n",
        "> torch is a standard Unix command-line interface program which is used to update the access date and/or modification date of a file or directory. In its default usage, it is the equivalent of creating or opening a file and saving it without any change to the file contents.*`italicized text`*"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4_zD3obVyLty",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!touch output.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8mCIuCJIySP2",
        "outputId": "a11c8a6b-09de-4470-9bfa-3af7fb8172d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "22CrLnqXgKp6"
      },
      "cell_type": "markdown",
      "source": [
        " **shuf** (Linux) shuffles/creates random permutaiton\n",
        "\n",
        ">-o is outuput\n",
        ">>$ shuf ostechnix.txt -o output.txt\n",
        "\n",
        ">-n is number random entries you  want\n",
        ">>$ shuf -n 5 ostechnix.txt\n",
        "\n",
        ">The Shuf command is used to generate random permutations in Unix-like operating systems. Using shuf command, we can shuffle the lines of a given input file randomly. if the file is larger it uses [Reserve Sampeling](https://en.wikipedia.org/wiki/Reservoir_sampling) Shuf is also par of GNU Core utils, which means you sould be able to use it with out dowdloading on most systems\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OPdIq8sVuWEW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "$ shuf -n $(( $(wc -l < $file) / 100)) $file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0MGCRr9lbAJf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "echo \"$(head -n1 my_data.csv)\" \"$(tail -n+2 my_data.csv | shuf -n 10)\"\n",
        "\n",
        "#The tail call is there to ensure that we don’t sample the header, thus duplicating it in the body of the file.\n",
        "\n",
        "#If you’d rather write to a file, you can simply redirect the output of the above, or you can break the process \n",
        "#into two parts and dispense with echo:\n",
        "\n",
        "\n",
        "head -n1 my_data.csv > downsampled.csv\n",
        "tail -n+2 my_data.csv | shuf -n 100000 >> downsampled.csv\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_POmESctgH7s",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Xl5QYVsbolBV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#http://data-analytics-tools.blogspot.com/2009/09/reservoir-sampling-algorithm-in-perl.html\n",
        "#!/usr/bin/python\n",
        "import sys\n",
        "import random\n",
        " \n",
        "if len(sys.argv) == 3:\n",
        "    input = open(sys.argv[2],'r')\n",
        "elif len(sys.argv) == 2:\n",
        "    input = sys.stdin;\n",
        "else:\n",
        "    sys.exit(\"Usage:  python samplen.py <lines> <?file>\")\n",
        " \n",
        "N = int(sys.argv[1]);\n",
        "sample = [];\n",
        " \n",
        "for i,line in enumerate(input):\n",
        "    if i < N:\n",
        "        sample.append(line)\n",
        "    elif i >= N and random.random() < N/float(i+1):\n",
        "        replace = random.randint(0,len(sample)-1)\n",
        "        sample[replace] = line\n",
        " \n",
        "for line in sample:\n",
        "    sys.stdout.write(li\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zbMfNCA0opJJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Read the line locations into memory once.  (If the lines are long,\n",
        "# this should take substantially less memory than the file itself.)\n",
        "fname = 'big_file'\n",
        "s = [0]\n",
        "linelocs = [s.append(s[0]+len(n)) or s.pop(0) for n in open(fname)]\n",
        "f = open(fname) # Reopen the file.\n",
        "\n",
        "# Each subsequent iteration uses only the code below:\n",
        "# Grab a 1,000,000 line sample\n",
        "# I sorted these because I assume the seeks are faster that way.\n",
        "chosen = sorted(random.sample(linelocs, 1000000))\n",
        "sampleLines = []\n",
        "for offset in chosen:\n",
        "  f.seek(offset)\n",
        "  sampleLines.append(f.readline())\n",
        "# Now we can randomize if need be.\n",
        "random.shuffle(sampleLines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "IHsG0vTt1SWC"
      },
      "cell_type": "markdown",
      "source": [
        "###Objective\n",
        "\n",
        ">Define Objective, what are we trying to achive\n",
        "\n",
        "###Levers\n",
        "\n",
        ">what Levers are available to change the business outcomes\n",
        "\n",
        "###Data\n",
        "\n",
        ">what data do they have or can they collect at the time of the decision. \n",
        "\n",
        ">What data does the company have that could help them understand reactions to levers\n",
        "\n",
        "###Models\n",
        ">**Elasticity model**\n",
        ">>if you rase the price this is number or percentage of custoemr that will stay or sign up \n",
        "\n",
        ">**Optimation model**\n",
        ">>attempting to maximize revenue or change a behaivor\n",
        "\n",
        ">**Simimulation model**\n",
        ">>used to run simimulation on changes effect business outcomes. \n",
        "\n",
        "####Validation\n",
        "\n",
        ">how good do you want the model to be\n",
        "\n",
        ">Generally speaking you what the number of observations of the thing of intrest to be at least 22, in each data sets(traing, validation, and test)\n",
        ">>22 is picked bucause this is where thet t-distribution roughtly becomes a normal distribution. \n",
        "\n",
        ">the binomial distribution of n samples and probability p:\n",
        "mean= n * p \n",
        "std = n * p * (1-p), npq, but really sqrt(npq)\n",
        "standard error = std/sqrt(n)\n",
        "train 5 , exactly same hyperparmeters, look at validation set acdcuracy each time to see how much they vary. \n",
        "\n",
        "#### oversampeling the less common class\n",
        "\n",
        "this is the best way to handle unblance data sets\n",
        "\n",
        ">copy them \n",
        "\n",
        ">use class weights parameter so that they oversample the less common class(es)\n",
        "\n",
        ">in deep learing you would use stratified samples, in your mini batches\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PL46728b7hYp"
      },
      "cell_type": "markdown",
      "source": [
        "Overview of understanding trees\n",
        "\n",
        "\n",
        "\n",
        "1.   Confidence based on tree variance\n",
        ">how confident are we based on what we know\n",
        "2. Feature importance\n",
        ">how much of the change is impacted by this feature\n",
        "\n",
        ">>completely randomizing the feature will give you the change in  model \n",
        "\n",
        ">>spend time with the features that actually make the different in the model\n",
        "\n",
        "3. Removing redundant features\n",
        "\n",
        "4. Partial dependence\n",
        ">help you to understand why univariate charts show relationships\n",
        "\n",
        ">>the world is a very interconnected place, we need to try to understand what are the reasons to explain what we are seeing.\n",
        "\n",
        ">>partial dependence charts give you a better look at the relationship between variables, because they attempt to make all other things being equal. \n",
        "\n",
        ">>an example of how this works is if you are using year made you would take all the rows and replace the year made with 1960 and then see thesults. On the partial dependency plot you will see that each line is a row and the medium is yellowish, which could be considered the real relationship between variable of interest and result(time and sale price)\n",
        "\n",
        ">> using cluster analysis you can see a few of the different shapes/relationships, differnt types of products or customers\n",
        "\n",
        ">>befor you run it think about what you expect to see. \n",
        "\n",
        "5. Tree interpreter\n",
        "\n",
        ">>Show .\n",
        "6. Extrapolation\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7sjGGBWXyNDm"
      },
      "cell_type": "markdown",
      "source": [
        "##     Intro workshop\n",
        "[AWS SSH google doc](https://docs.google.com/document/d/1nNvT1tFcxXeDlqkkllrliRh9SKl3K5RhIlXoKL5IeRk/edit)\n",
        "\n",
        "\n",
        "[00:00](https://www.youtube.com/watch?v=wa0GRQCcOpI&feature=youtu.be&t=1) : Windows setup\n",
        "  \n",
        "[04:03](https://youtu.be/wa0GRQCcOpI?t=243) :: Mac setup\n",
        "\n",
        "[09:23](https://youtu.be/wa0GRQCcOpI?t=563): Initial AWS setup \n",
        "* There is an ami you can set up but it in not available in all location I think you have to pick washington, I will add the blog post if I find it again\n",
        "\n",
        "[20:26](https://youtu.be/wa0GRQCcOpI?t=1226) : Initial Paperspace setup\n",
        "\n",
        "[45:24](https://youtu.be/wa0GRQCcOpI?t=2724) : Initial Crestle setup\n",
        "\n",
        "[53:07](https://youtu.be/wa0GRQCcOpI?t=3187) : Using the terminal\n",
        "\n",
        "[1:02:01](https://youtu.be/wa0GRQCcOpI?t=3721) : Anaconda on laptop\n",
        "\n",
        "[1:11:50](https://youtu.be/wa0GRQCcOpI?t=4310) : Pros and cons: Crestle, AWS, Paperspace, laptop, workstation\n",
        "\n",
        "[1:30:52](https://youtu.be/wa0GRQCcOpI?t=5452) : Introduction to Jupyter notebooks\n",
        "\n",
        "[2:06:05](https://youtu.be/wa0GRQCcOpI?t=7565) : Introduction to numeric programming in Python\n",
        "\n",
        "[2:27:38](https://youtu.be/wa0GRQCcOpI?t=8858) : ssh into Paperspace\n",
        "\n",
        "[2:48:11](https://youtu.be/wa0GRQCcOpI?t=10091) : ssh into AWS\n",
        "\n",
        "[3:11:49](https://youtu.be/wa0GRQCcOpI?t=11509) : Machine Learning Foundations   \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2ZIIe7Pp36eg"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Activation function\n",
        "\n",
        ">#### Softmax \n",
        "Used for categorical precictions, used only when you want to predict one of thoses things.\n",
        "\n",
        ">outputs act like probabilities, sum to 1\n",
        "\n",
        ">small imput differences, become large output differences(which mean,there will be a buch of small numbers and a few large numbers)\n",
        "\n",
        ">it uses e^ which mean\n",
        "\n",
        ">#### Sigmoid\n",
        ">muli-class categorical predictions\n",
        "\n",
        ">forces output to be between 0-1, but they no longer add to one. \n",
        "\n",
        "\n",
        "Softmax\n",
        "binary classification\n",
        "\n",
        "sigmoid\n",
        "muli-lable classifications - where your predicting multipel things\n",
        "\n",
        "ReLU\n",
        "mostly used for transformaitons, converts all negative numbers to 0\n",
        "\n",
        "leaky ReLU\n",
        "transformation converts negative numbers to much smaller negative numbers\n",
        "\n",
        "elu\n",
        "regression often times doesnt use one, that is the ending activation like sigmoid or softmax, but sometime using sigmoid is helpful"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eerQnENw4AJr"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###Bash\n",
        "     \n",
        " [Handy Bash tips56]( https://linuxacademy.com/blog/linux/tutorial-the-best-tips-tricks-for-bash-explained/)   \n",
        "     \n",
        "[Linux Terminal Basic Commands16](https://gist.github.com/shubhambhattar/53059f08da48ea69d87ac1e62dd4d7ff)   \n",
        "\n",
        ">`wc -l (filename)  ` \n",
        "* Returns the number of lines in the file(csv in this case)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ">`ls -lh   `    \n",
        "* Returns all the files in current directory\n",
        "\n",
        ">`unzip (filename) `.    \n",
        "*  Unzips the file\n",
        "\n",
        "### Visualizaiton tips/examples\n",
        "[3d iceberg vs ship](https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d)\n",
        "\n",
        "###Git\n",
        "https://try.github.io/levels/1/challenges/124\n",
        "http://gitimmersion.com/8\n",
        "https://speckyboy.com/resources-for-learning-git/3\n",
        "https://www.quora.com/What-are-some-good-resources-for-learning-to-use-Git2\n",
        "https://www.atlassian.com/git3\n",
        "https://speckyboy.com/resources-for-learning-git/3\n",
        "https://www.atlassian.com/git3\n",
        "https://www.quora.com/I-am-new-to-programming-what-is-the-best-way-to-learn-how-to-use-Git-and-eventually-start-a-project-on-GitHub4\n",
        "https://www.udacity.com/course/how-to-use-git-and-github--ud775\n",
        "\n",
        "###Markdown\n",
        " make a keyboard button or better looking keyboard shortcut: \"< kbd>ctrl-b</ kbd>\" (without spaces)\n",
        " \n",
        " <kbd>ctrl-b</kbd>\n",
        " \n",
        "###PyTorch\n",
        " \n",
        "[PyTorch tutorials](http://pytorch.org/tutorials/)\n",
        "\n",
        "[PyTorch Tutorial for Deep Learning researchers](https://github.com/yunjey/pytorch-tutorial)\n",
        "\n",
        "[PyTorch, Dynamic Computational Graphs and Modular Deep Learning](https://medium.com/intuitionmachine/pytorch-dynamic-computational-graphs-and-modular-deep-learning-7e7f89f18d1)\n",
        "\n",
        "[Deep Learning with PyTorch: A 60 min Blitz](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "\n",
        "[PyTorch examples](https://github.com/pytorch/examples)\n",
        "[PyTorch: First program and walk through](https://medium.com/towards-data-science/pytorch-first-program-and-walk-through-ceb739134ab9)\n",
        "\n",
        "[PyTorch tutorial distilled](https://medium.com/towards-data-science/pytorch-tutorial-distilled-95ce8781a89c)\n",
        "\n",
        "[PyTorch Kaggle Starter](https://github.com/bfortuner/pytorch-kaggle-starter)\n",
        "\n",
        "[Deep Learning in PyTorch](https://iamtrask.github.io/2017/01/15/pytorch-tutorial/)\n",
        "\n",
        "[Practical Deep Learning with PyTorch on Udemy16](https://www.udemy.com/practical-deep-learning-with-pytorch/)\n",
        "\n",
        "[Deep Learning Course (with PyTorch)](https://fleuret.org/dlc24)\n",
        "\n",
        "[The incredible Pytorch](https://github.com/ritchieng/the-incredible-pytorch)\n",
        "\n",
        "[Pytorch Tutorial - Deep Learning in Python](http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning18)\n",
        "\n",
        "[PyTorch with no Deep Learning background](https://drive.google.com/drive/folders/0B41Zbb4c8HVyUndGdGdJSXd5d3M)\n",
        "\n",
        "[Deep Learning essentials in Pytorch](http://www.deeplearningessentials.science/)\n",
        "\n",
        "[PyTorchZeroToAll](https://github.com/hunkim/PyTorchZeroToAll)\n",
        "\n",
        "[Deep Learning with PyTorch - Video Tutorials from Anand Saha](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-pytorch-video)\n",
        "\n",
        ">#### Pytorch data loaders\n",
        "they grab the data and creat the mini batches, and make them available, it python they are called generators\n",
        "\n",
        "\n",
        "*   ImageClassifierData.from_arrays\n",
        "*   etc\n",
        "\n",
        "\n",
        "\n",
        "### Python\n",
        "\n",
        ">#### Generators\n",
        ">I want another, I want another, please give me another\n",
        "\n",
        ">#### Inheritance\n",
        ">if you are building a class and you what it to get/inherite certain aspects of a different class you can put it it in the parames, \n",
        "\n",
        "\n",
        "\n",
        ">```\n",
        "class LogReg(nn.Module):   \n",
        "   def __init__(self):\n",
        "        super().__init__()\n",
        "```\n",
        ">class name = LogReg\n",
        "\n",
        ">Inheriates from =  nn.Module \n",
        "\n",
        ">first line in in __init__ that has inheritance is the super().__init__(), which is the initilization of the inherited class.\n",
        "\n",
        "\n",
        "[Useful Resources on Python\n",
        "Python/ Numpy/ Scipy/ Matplotlib- Combined Demo with code from excellent Stanford cs231 course](http://cs231n.github.io/python-numpy-tutorial/)\n",
        "\n",
        "[A Byte of Python15](https://python.swaroopch.com/)\n",
        "\n",
        "[Learn Python The Hard Way (Frankly the easier and consistent way](https://learnpythonthehardway.org/book/)\n",
        "\n",
        "[Solo Learn Python](https://www.sololearn.com/Course/Python/)\n",
        "\n",
        "[The HitchHacker’s Guide to Python (Best Practice Handbook)](http://docs.python-guide.org/en/latest/)\n",
        "\n",
        "[DataCamp (an interactive crash-course on Python for ML: 20 x 4-hour_modules for 29.95/month)](https://www.datacamp.com/tracks/data-scientist-with-python)\n",
        "\n",
        "[LearnPython.org](https://www.learnpython.org/)\n",
        "[Ultimate Resource Repo](https://wiki.python.org/moin/BeginnersGuide/Programmers)\n",
        "\n",
        "[Python Graph Gallery](https://python-graph-gallery.com/)\n",
        "\n",
        "###Charting \n",
        "[waterfall chart - pip install waterfallcharts ](https://github.com/chrispaulca/waterfall)\n",
        "\n",
        "[waterfall chart example](https://github.com/chrispaulca/waterfall/blob/master/Tree_interpreter_Example.ipynb)\n",
        "\n",
        "###tmux\n",
        "\n",
        "[tmux on AWS](https://github.com/reshamas/fastai_deeplearn_part1/blob/master/tools/tmux.md)\n",
        "* ctrl-b followed by ? brings up a full list of keyboard shortcuts\n",
        "* [tmux-plugins/tmux-resurrect - Persists tmux environment across system restarts.](https://github.com/tmux-plugins/tmux-resurrect)\n",
        "\n",
        "\n",
        "###sim link\n",
        ">**ln -s path_to_the_source name_of_destination**\n",
        ">>path can be relative or absolute\n",
        ">ln -s ../../fastai ./\n",
        ">>if you put the current directory as the destination it wiell use the same name as it comes from like a alias on mac, or shortcut on windows\n",
        "\n",
        "\n",
        "###Career advice\n",
        "[contribute to open sourced libraries](https://youtu.be/BFIYUvBRTpE?t=1h10m45s) using [hub.github.com](http://hub.github.com/) which is a  a command-line wrapper for git that makes you better at GitHub.\n",
        "\n",
        "remember the busines reasoning using the [drivetrain approach](https://www.oreilly.com/ideas/drivetrain-approach-data-products)\n",
        "\n",
        "\n",
        "     \n",
        "###f-strings in Python 3.6\n",
        "\n",
        "\n",
        "\n",
        "name=“vijay”\n",
        "\n",
        "print(f’ My name is {name}’)==>“My name is vijay”\n",
        "\n",
        "The curly braces hold the values and even integers work unlike the normal string formatting where you to typecast int to str to print them. read this blog post  https://cito.github.io/blog/f-strings/\n",
        "\n",
        "Some Unix/Linux stuff\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###RANDOM FOREST\n",
        "\n",
        "Jeremy provides a brief overview on why Random Forest. He’ll diving into further details in the following lectures. The following is what I picked from the current lecture.\n",
        "\n",
        "It can be considered a Universal Machine Learning Technique.\n",
        "It basically can be used to target any type of ML problem like Classification, Continuous variable prediction(Regression).\n",
        "It generally does not over fit and if it does, it can be easily corrected.\n",
        "Very few statistical assumption are made on the data\n",
        "It usually does not require a seperate validation set.\n",
        "Curse of Dimensionality\n",
        "\n",
        "This is an assumption in the ML community that more the number of columns, it creates a space which is more and more empty. More dimensions lead to most of the points sitting on the edge of that space. The distance between the points become less meaningful.\n",
        "\n",
        "Jeremy considers this as largely meaningless and stupid. Later on the class, he also suggested to break the data into as many columns as possible if you think that’s going to add more meaning to the data. Though the points are on the edge of the space, they still do have different distances away from each other and are still useful.\n",
        "\n",
        "This notion basically came from the 90’s where theory took over.\n",
        "\n",
        "I’m not pretty much clear on the space part of this though I understood the overall gist of what Jeremy said. I’ll be happy if someone can add some more meaningful explanation here.\n",
        "\n",
        "No Free Lunch Theorem\n",
        "This states that there is no type of model that works well for any kind of data set.\n",
        "\n",
        "Contrast to this, in real world, there are techniques that work better than other techniques for a given data set.\n",
        "\n",
        "Also this assumes theorem assumes that we work on random data sets in real life which is actually not the case. Every column in the data set will definitely have a relation to other columns in the data set.\n",
        "\n",
        "Regression: A ML model trying to predict continuous variables(a dependent variable) based on other variables in the given data set. People often refer regression to logistic regression which is not true.\n",
        "\n",
        "Feature Engineering\n",
        "\n",
        "Wikipedia says - Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work.\n",
        "\n",
        "Jeremy threw a very simple example at us in the class. In the bulldozers data set, we had a date column. What all extra features/information can we get from that column.\n",
        "\n",
        "Which quarter of the year it is? - Useful if we’re predicting sales\n",
        "What day of the week is it ? - We can observe if there is a pattern for a particular day of the week(People usually go to church on sundays at a particular time and Uber can plan a surge)\n",
        "What week of the month?\n",
        "Is it a holiday? - Eateries in movie stalls can plan their logistics\n",
        "Is it summer/winter/rainy season? - Seasonal sales say Umbrellas in summer/rainy or jackets in winter\n",
        "Is there a game going on that day ? -\n",
        "Pandas Stuff\n",
        "A basic data structure in pandas is called a data frame. Data frames in Python and R function in a similar way.\n",
        "Pandas is the gold standard for dealing with structured data in Python and am amazed sometimes with what it can given a simple csv/excel file with some data in it.\n",
        "\n",
        "Few methods from Pandas\n",
        "\n",
        "df.tail() - Prints last few rows from the dataframe\n",
        "\n",
        "df.head() - Prints first few rows from the dataframe\n",
        "\n",
        "df.tail().transpose() - Usually columns are printed at top and rows are printed at side. When you’ve lots of columns, some columns are formatted and not displayed. Transpose method prints columns to the side and rows to the top so that you can view all the data at once.\n",
        "\n",
        "drop() - This method return a new dataframe with few columns or rows removed which can be specified in a list and passed into the method. The axis parameter decides whether rows or columns to be removed. axis = 1 removes columns.\n",
        "\n",
        "Use square brackes [column name] to grab a column from dataframe in a safe way to perform some operation on it.\n",
        "df[‘salesdata’] = df[‘salesdata’] * 2\n",
        "\n",
        "Pandas inherently supports categories, datetime datatypes.\n",
        "\n",
        "fast.ai libray functions\n",
        "\n",
        "add_datepart - Seperates datetime64 type column into multiple columns like discussed above in feature engineering.\n",
        "\n",
        "train_cats - Creates categorical variables for any column that contains a string(If there’s a column called severity with labels High, Low and Medium, they will be converted to numbers say 0,1 and 2). I did not know that Pandas can natively do this. I earlier used the preprocessing.LabelEncoder() from scikit learn.The ordering of the labels can be changed which may be helpful in improving the model structure, say reducing the number of decision trees. There is a parameter called inplace which when set to True does not alter the df but reorders the values properly. Any missing values are filled in with -1\n",
        "\n",
        "One point to be careful while labeling is that, make sure you’re using same labels for training and testing the model otherwise, your model will never seem to work properly. (I have faced this issue in my work when working on some classification tasks.)\n",
        "\n",
        "apply_cats - This method makes sure that the same labeling is used for test data which was earlier used for training data.\n",
        "\n",
        "Feather format\n",
        "\n",
        "Feather provides binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy.\n",
        "\n",
        "This format saves the data to disk in a similar way the data is stored on RAM. This significantly improves the read time from the disk and we’ve experienced this in the class when Jeremy read an df from the disk thats saved in this format\n",
        "\n",
        "njobs\n",
        "\n",
        "This variable is passed to the randomforestregressorobject.fit() method. This specifies to create seperate process for the training to run across the seperate CPU cores if available.\n",
        "\n",
        "The above notes has been taken down while listening to Jeremy’s video on YouTube. The notes is in pretty much the same order as the class except that I grouped the pandas utilities that are covered over the entire session under one heading. I’ll be adding more notes once I listen to the lecture again. Please feel free to update the notes or suggest any chagnes that I need to make if the post is not editable"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tlCjk265I3pM"
      },
      "cell_type": "markdown",
      "source": [
        "##Intro to Machine learning\n",
        ">This timeline was created by [Eric Perbos-Brinck](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/319) I just added my notes and more infromation that I found helpful \n",
        "\n",
        "###Lesson 1 video timeline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "00:02:14 AWS or Crestle Deep Learning\n",
        "\n",
        "00:05:14 lesson1-rf notebook Random Forests\n",
        "\n",
        "00:10:14 ?display documentation, ??display source code\n",
        "\n",
        "00:12:14 Blue Book for Bulldozers Kaggle competition: predict auction sale price,\n",
        "Download Kaggle data to AWS using a nice trick with FireFox javascript console, getting a full cURL link,\n",
        "Using Jupyter “New Terminal”\n",
        "\n",
        "[00:23:55](https://youtu.be/CzdWqFTmn0Y?t=23m55s) using !ls {PATH} in Jupyter Notebook\n",
        "> if the command starts with ! it means it is a bash command, if you want to pass it a python varable you would put it in {curly brackets}, this is a jupyter notebook thing\n",
        "\n",
        "00:26:14 Structured Data vs data like Computer Vision, NLP, Audio, ‘low_memory=False’, ‘parse_dates’,\n",
        "Python 3.6 format string f’{PATH}Train.csv’,\n",
        "‘display_all()’\n",
        "\n",
        ">low_memory=False makes it read more of the file to determin what type of variable each column is\n",
        "\n",
        ">inside the f'string{curlies} you and write just about anything python string related, like other functions,ect.\n",
        "\n",
        ">!head data/bulldozers/train.csv\n",
        "\n",
        ">I think it is hard to read but but a very easy way to take a quick peek. \n",
        "\n",
        "**!ls -lh**   #read all the files in current directory\n",
        "\n",
        "**!wc -l **filename.csv #number of rows\n",
        "\n",
        "pd.read_csv, you can pass it information like:\n",
        ">**Parse_dates**=['saledate'], \n",
        "\n",
        ">this makes sure it is labled as a datetime object, dt, which mean it will have saledate.dt.objects\n",
        "\n",
        ">once data is in pandas dataframe, you can use this function **display_all**, it is used in part to avoid the normal truncation of data if there is a lot of columns or rows.\n",
        "\n",
        ">**def display_all(df):**\n",
        "    >>**with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): **\n",
        "        >>>**display(df)**\n",
        "       \n",
        ">`#transpose so you can scroll down not over, better way to see data if there is a lot of columns`\n",
        "\n",
        ">**display_all(df_raw.tail().T)**\n",
        "\n",
        ">**display_all(df_raw.describe(include='all').T)**\n",
        "\n",
        "[00:33:14](https://youtu.be/CzdWqFTmn0Y?t=33m14s) Root mean squared log error,(RMSLE)difference between the log of prices\n",
        "\n",
        ">use numpy to get the log of the SalePrice\n",
        "\n",
        ">**df_raw.SalePrice = np.log(df_raw.SalePrice)**\n",
        "df_raw.SalePrice is a column from the df_raw data frame, which is also considered a pandas series, and that series can be passed to an array. \n",
        "\n",
        "[00:36:14](https://youtu.be/CzdWqFTmn0Y?t=36m14s) Intro to Random Forests, in general doesn’t overfit, no need to setup a validation set, doesnt make statistical assumptions, you dont need a seperate test and valadation set, it will tell you how well it generalizes, works with almost all date types, requires only a very few pieces fo feature engeneering, for may different types of situations, dont have to take the log of the data, it is a great place to start\n",
        "\n",
        "The Silly Concepts of Cursive Dimensionality \n",
        ">this is the idea that the more dementions/features that you have the more empty space that you have and the data points tend to reside on the edges, so in high dementional space everything sits on the edges, in theory that mean the distance between points is much less meaning full. Jeremy states that this doesnt really matter becase they still have a distance that is meaningful and even k nearest neighbors works in high dementions dispite what the theoreticians claim about dementiontionality. \n",
        "\n",
        " No Free Lunch theorem\n",
        " \n",
        " >There is not model that works well for any random data set than any other model. this may be true if data set were random, which they are not. Data is created by an underlieing cause and have structure. tree based models like random forest have been known to work on a wide array of problems. \n",
        "\n",
        ">support vector machines are very easy to understand in theory but dont work much in practice. \n",
        "\n",
        "[00:43:14](https://youtu.be/CzdWqFTmn0Y?t=43m14s) RandomForestRegressor, RandomForestClassifier\n",
        "Stack Trace: how to fix an error\n",
        "\n",
        ">RandomForestRegressor is part of sklearn, scikit-learn, there is also RandomForestClassifier\n",
        "\n",
        ">Regression is for prediction of continuous variables, called Regressor, and classification is for prediction of categorical variables, things that do that are called classifiers.\n",
        "\n",
        "**SKLEARN**\n",
        ">1st create and instance of the model that you want.\n",
        "\n",
        ">>m  =  RandomForestRegressor(n_jobs=-1)\n",
        "\n",
        ">2nd call .fit, passing in the indepedent variables(things used to make prediction), dependent variable(thing you want to predict)\n",
        "\n",
        ">>m.fit(df_raw.drop('SalePrice', axis=1), df_raw.SalePrice)\n",
        "\n",
        "tip* reading docs, *shift tab* ,*shift tab*, *shift tab* \n",
        "or ** ??df.drop**-source code, **?df.drop**-documentation\n",
        "\n",
        "list-like -is any thing you can index,\n",
        "\n",
        ">>>.drop returns dataframe with the list of rows or columns removed, this removed the column SalePrice, the axis=1, means remove columns\n",
        "\n",
        "[00:48:14](https://youtu.be/CzdWqFTmn0Y?t=48m14s) Continuous and categorical variables, add_datepart()\n",
        "\n",
        "> takes in data frame and fldname, used the getattr function(get attributet), meaning it goes into an object and finds an attributed that are in the list of attributes we want to use, year, month, week..... Is_year_end, etc. \n",
        ">pandas splits out methods inside attributes that are specific to what they are, so a date/time objects will have a .dt attribute defined.\n",
        "\n",
        ">add_datepart() creates a ton of new columns with pretty much every aspect of data time possible then removes the original column.\n",
        "\n",
        "[00:57:14](https://youtu.be/CzdWqFTmn0Y?t=57m14s) Dealing with strings in data (“low, medium, high” etc.), which must be converted into numeric coding, with train_cats() creating a mapping of integers to the strings.\n",
        "Warning: make sure to use the same mapping string-numbers in Training and Test sets,\n",
        "Use “apply_cats” for that,\n",
        "Change order of index of .cat.categories with .cat.set_categories.\n",
        "\n",
        ">**train_cats(df_raw)**\n",
        "\n",
        ">Pandas has the lable as category, train_cates(df), which takes call the string dtypes(data types), converts them to category, and uses .as_ordered\n",
        "\n",
        ">>there is also an appy_cats, that allows you to appy the existing training set so that you apply the same mapping to the train,test and valadation sets. \n",
        "\n",
        ">> there is a .cat attrubute just like .dt\n",
        "\n",
        ">>.cat is a pandas data class\n",
        "\n",
        ">>df_raw.UsageBand.cat.categories, but you might not like the order, if you want to reorder them you can use:\n",
        "\n",
        ">>df_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)#inplace means it will not return a new dataframe it will change the existing one. \n",
        "\n",
        ">> reordering is more about model understanding, but order can be important because if it is labled as ordinal categorical, the trees will use order as features, apposed to being bolean/binary\n",
        "\n",
        ">> negative one, in categories is considered NA, or missing, that is why people add 1 to there code to make NA = 0\n",
        "\n",
        ">Missing Vaues\n",
        ">**display_all(df_raw.isnull().sum().sort_index()/len(df_raw))**\n",
        ">>.isnull() - looks for nulls/NA/NAN\n",
        "\n",
        ">>.sum() -  sums them up\n",
        "\n",
        ">>.sort_index()  -  sorts by index\n",
        "\n",
        ">>/len(df_raw) -  length of df_raw\n",
        "\n",
        ">>sums up all the missing values and divides by total number of rows, to finde percentage of missing vaues. \n",
        "\n",
        ">>pip install feather-format\n",
        "\n",
        "[01:07:14](https://youtu.be/CzdWqFTmn0Y?t=1h7m14s) Pre-processing to replace categories with their numeric codes,\n",
        "Handle missing continuous values,\n",
        "And split the dependant variable into a separate variable.\n",
        "proc_df() and fix_missing()\n",
        "\n",
        ">**df, y, nas = proc_df(df_raw, 'SalePrice')**\n",
        ">> proc_df(dataframe, y -variable(thing you are trying to predict) \n",
        ">>makes a copy of the dataframe, grab the y value, drop the y value(dependent variable), then .fix_missing(df), nas creates a dictinary where the keys are the names of the columns that had missing values, and the values of the dictonary are the medians, this is done so you can pass the nas as an additional argument to proc_df and it will make sure that it adds those specific columns and uses those specific medians.(gives you the abiltiy to say process this test set the same way you did to this training set)\n",
        "\n",
        ">>>.fix_missing (for continious varables) \n",
        "\n",
        ">>>it creates another column with the name as the same as the original with a _na at the end, Name becomes Name_na, and makes it a boolean dtype with a 1 to signify that it was missing, and put the median in the place of the orignal column\n",
        "\n",
        ">>>.numericalize(for categorical)\n",
        ">>> it replaces the srings with the coresponding category codes and adds 1 to each of them, this changes the -1 for missing to zero, and all the others up 1. \n",
        "\n",
        "\n",
        "\n",
        "**m  =  RandomForestRegressor (n_jobs=-1)**\n",
        "\n",
        ">n_jobs = -1, tells python to create a seperate process for each CPU you have (except 1)\n",
        "\n",
        "**m.fit(df, y)** -fit the model\n",
        "**m.score(df,y)** returns r squared 1 is good 0 is bad\n",
        "\n",
        "01:14:01 ‘split_vals()’\n",
        "\n",
        "**split into train and validation**\n",
        "\n",
        "this is important because if you only have the test set you might be overfitting your model based on the model you are using, if you you use enought models one is sure to have a better results. sometimes refered to p-value hacking, a very bad way to practice science, but good way to get a write up in a journal. this is when they try lot of variations untill that have something that works, smoking cigarettes with brandy, makes your leg grow longer. It cause big issues with replication. \n",
        "\n",
        "\n",
        ">if you can never look at test set untill your model is done. \n",
        "\n",
        "In this example we are using 12000 for the test set so it would be best to use 12000 row for the validation set as well. \n",
        ">split into two parts the( n - validation set), and (1st n rows for the validation set). \n",
        "\n",
        "``def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n",
        "n_valid = 12000  # same as Kaggle's test set size\n",
        "n_trn = len(df)-n_valid\n",
        "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
        "X_train, X_valid = split_vals(df, n_trn)\n",
        "y_train, y_valid = split_vals(y, n_trn)\n",
        "X_train.shape, y_train.shape, X_valid.shape``\n",
        "\n",
        "\n",
        "``def rmse(x,y): return math.sqrt(((x-y)**2).mean())``\n",
        "\n",
        "``def print_score(m):\n",
        "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n",
        "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
        "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
        "    print(res)``\n",
        "    \n",
        " ``m = RandomForestRegressor(n_jobs=-1)\n",
        "%time m.fit(X_train, y_train)\n",
        "print_score(m) ``\n",
        "\n",
        "\n",
        "CPU times: user 1min 3s, sys: 356 ms, total: 1min 3s\n",
        "Wall time: 8.46 s\n",
        "[0.09044244804386327, 0.2508166961122146, 0.98290459302099709, 0.88765316048270615]\n",
        "rmse tran, rmse validation,\n",
        "r-sqrd training, r-sqrd validation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "k5nuE7cvVQFW"
      },
      "cell_type": "markdown",
      "source": [
        "###Lesson 2\n",
        "\n",
        "[00:03:30](https://youtu.be/blyXCk4sgEg?t=3m30s) simlink sim link to fastai directory\n",
        "\n",
        ">sim link\n",
        "ln -s path_to_the_source name_of_destination\n",
        "\n",
        ">path can be relative or absolute ln -s ../../fastai ./ if you put the current directory as the destination it wiell use the same name as it comes from like a alias on mac, or shortcut on windows\n",
        "\n",
        "00:06:15 understand the RMSLE relation to RMSE, and why use np.log(‘SalePrice’) with RMSE as a result\n",
        "\n",
        ">I need to get a better understanding of the math of the rmse, right now I understand why we use it but not intuative on how it is better thena other options. \n",
        "\n",
        ">sum(  ( ln(actuals)-ln(predictions ) )^2)\n",
        "\n",
        "00:09:01 proc_df, numericalize\n",
        "\n",
        ">did in lesson one, if not numeric, do _____ \n",
        "\n",
        "\n",
        "00:11:01 rsquare root square of mean errors RMSE,\n",
        "What the formula rsquare (and others in general) does and understand it\n",
        "\n",
        ">r2 how much of the variance is described the model, it is now much better that just taking the mean\n",
        ">r^2 is 1-SSres/SStot\n",
        ">you have data you subtract the mean from each of the values, give ss/tot, total sum of squares\n",
        ">data subtract predictions, ss/res\n",
        ">then you take the ratio, of these two\n",
        ">essentialling leaving you with how much better your model is than just picking the mean. \n",
        ">the range of possible values is anything less than one. if you get a negative valu your model is just worse that using the mean. \n",
        ">your model could be infintly worse than actual. meaning your residual would be 1-infinity, \n",
        "\n",
        "00:17:30 Creating a good validation set, ‘split_vals()’ explained\n",
        "“I don’t trust ML, we tried it, it looked great, we put it in production, it didn’t work” because the validation set was not representative !\n",
        "\n",
        ">did in lession 1\n",
        "\n",
        "\n",
        "00:21:01 overfitting over-fitting underfitting ‘don’t look at test set !’,\n",
        "Example of failed methodology in sociology, psychology,\n",
        "Hyperparameters,\n",
        "Using PEP8 (or not) for ML prototyping models\n",
        "\n",
        ">look into pep8 to learn the proper way to write python code, better variable names, documentation, etc. \n",
        "\n",
        "00:29:01 RMSE function and RandomForestRegressor,\n",
        "Speeding things up with a smaller dataset (subset = ),\n",
        "Use of ‘_’ underscore in Python, \n",
        "\n",
        ">speeing up \n",
        "\n",
        ">pass the subset=30000, to the proc_df, which randomly samples data\n",
        "\n",
        ">```df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice', subset=30000, na_dict=nas)\n",
        "```\n",
        "\n",
        ">be carefull not to change the validation set, and the training doesnt overlap with the dates(becasue that would be cheating)\n",
        "\n",
        ">need to call split_vals() again \n",
        "\n",
        ">`X_train, _ = split_vals(df_trn, 20000)`\n",
        "\n",
        ">`y_train, _ = split_vals(y_trn, 20000)`\n",
        "\n",
        ">these are putting validation in a variable called \"_\", \"underscore\", we dont want to change the validation set, we want to use the same validaiton set on all the different models that we are going to be creating. So what this is doing is taking the first 20000 of a 30000 subset, so we can run models on it faster. \n",
        "\n",
        "*In practice you want to use set_rf_sample(20000) instead. \n",
        "\n",
        ">>`df_trn,  y_trn, nas =  proc_df(df_raw,  'SalePrice')\n",
        "X_train, X_valid = split_vals(df_trn, n_trn)\n",
        "y_train, y_valid = split_vals(y_trn, n_trn)`\n",
        "\n",
        "The basic idea is this: rather than limit the total amount of data that our model can access, let's instead limit it to a different random subset per tree. That way, given enough trees, the model can still see all the data, but for each individual tree it'll be just as fast as if we had cut down our dataset as before\n",
        "\n",
        "*set_rf_samples\n",
        "this is a function that jeremy created that uses the sorce code that scikit learn uses and repaces it with a lambda function, it doesnt change how oob scores are calculated so you cant use both becasue oob score would essentially run on the whole data set not just the sample that you are expecting. \n",
        ">>`set_rf_samples(20000)`#remember to use oob_score=False\n",
        ">>if you want to turn it off use \n",
        "`reset_rf_samples()`\n",
        "\n",
        "\n",
        "\n",
        "00:32:01 Single Tree model and visualize it,\n",
        "max_depth=3,\n",
        "bootstrap=False\n",
        "\n",
        ">trees are called estimators in scikit-learn. \n",
        "\n",
        ">pass into RandomForestRegressor\n",
        "\n",
        ">n_estimators = 1, #number of estimators you want\n",
        "\n",
        ">max_depth = 3, #length of tree\n",
        "\n",
        ">bootstrap = False #this turns off the randomness\n",
        "\n",
        ">m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\n",
        "m.fit(X_train, y_train)\n",
        "print_score(m)\n",
        "\n",
        ">draw the tree\n",
        "\n",
        ">`draw_tree(m.estimators_[0], df_trn, precision=3)`\n",
        "\n",
        ">**make a bigger tree**\n",
        "\n",
        "`\n",
        "m =  RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1)\n",
        "m.fit(X_train, y_train)\n",
        "print_score(m)`\n",
        "\n",
        ">see how it is better but still worse than the origianal, this is why we need more than one tree, which is called bagging.\n",
        "\n",
        "\n",
        "[00:47:01](https://youtu.be/blyXCk4sgEg?t=47m1s) Bagging of little Boostraps, ensembling\n",
        "\n",
        ">taking different models and combining them, it is important that they dont overlap\n",
        "\n",
        ">different trees or models overfitting to differnet parameters,and when their randomness is combined the rendom errors will average to zero and you will be left with be the true relationship. this is based on the thought that their errors are not correlated with each other because they were trained on different data. \n",
        "\n",
        ">key is using different data sets, \n",
        "\n",
        ">by default it creates 10 trees. by default it pick out nrows with replacement(jeremy called it bootstraping) and you end up with something like 63.2% of rows being represented, some alot more than others. \n",
        "\n",
        ">the goal is predictive but poorly correlated trees\n",
        "\n",
        "\n",
        "\n",
        "[00:57:01](https://youtu.be/blyXCk4sgEg?t=57m1s) scikit-learn ExtraTreeRegressor randomly tries variables\n",
        "\n",
        ">there another class you can use called extra tress regressor, or extra trees classifier, they are called extreamly randomiszed trees models, and what they do is instead of trying every variable they randomly pick a few variables, and tries a few splits of a few variables. It is much faster and has more randomness, but you can build more trees, that will hopefull generalize better. \n",
        "\n",
        ">Machine learning community is focusing more the benefits of randomness over accuracy of the individual trees. \n",
        "\n",
        "\n",
        "[01:04:01](https://youtu.be/blyXCk4sgEg?t=1h4m1s) m.estimators_,\n",
        "Using list comprehension\n",
        "\n",
        "\n",
        "`\n",
        "preds  =  np.stack([t.predict(X_valid)  \n",
        "for  t in  m.estimators_])`\n",
        "\n",
        "`preds[:,0], np.mean(preds[:,0]), np.std(preds[:,0]), y_valid[0]`\n",
        "\n",
        "\n",
        "\n",
        ">each tree is stored in m.extimators_\n",
        ">call .predict on it with my validaiton set\n",
        ">gives me a list of arrays of predictions.\n",
        ">np.stack, concatenates the ten trees togeather on a new axis.\n",
        "> calling .shape will give you (10,12000) 10 trees, 12000 predictions.\n",
        "\n",
        "`preds[:,0], np.mean(preds[:,0]), y_valid[0]`\n",
        "\n",
        ">preds[:,0] = 1st row, returns an array with the 10 predictions\n",
        "\n",
        ">np.mean(preds[:,0])  = mean of 1st row\n",
        "\n",
        ">np.std(preds[:,0]) = standard deviation\n",
        "\n",
        ">y_valid[0] = actual value\n",
        "\n",
        "\n",
        "that too slow because list comprehentions by default are just python and they run in serial which means they dont take advantage of multi cores or the gpu, that is why the fast AI Library has a fuction called **parallel_trees(m=randomforest_model,get_preds=function_call)**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def  get_preds(t): return t.predict(X_valid)\n",
        "%time preds = np.stack(parallel_trees(m, get_preds))\n",
        "\n",
        "\n",
        "np.mean(preds[:,0]), np.std(preds[:,0])\n",
        "\n",
        "really the same function but it does it in parallel\n",
        "\n",
        "\n",
        "\n",
        "*interesting plot\n",
        "\n",
        "`plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)]);`\n",
        "\n",
        ">take the r squared of the mean of the predictions and plot it up to the iTH tree, for i to 10\n",
        ">1 tree, 2 trees, 3 trees, etc.\n",
        "\n",
        ">Jeremy most of the time uses 20 to 30 trees and at the end of the night will run it on more trees, like 1000. \n",
        "\n",
        "\n",
        "\n",
        "[01:10:00](https://youtu.be/blyXCk4sgEg?t=1h10m) Out-of-bag (OOB) score\n",
        "\n",
        "**Dont use on big data sets!**\n",
        "\n",
        "set oob_score=True, will return an attribute called oob_score_\n",
        "\n",
        "\n",
        ">this is in the print_score function:\n",
        "\n",
        "`def rmse(x,y): return math.sqrt(((x-y)**2).mean())`\n",
        "\n",
        "`def print_score(m):\n",
        "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n",
        "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
        "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
        "    print(res)`\n",
        "    \n",
        ">if there is an oob_score_ it will add it to print()    \n",
        "    \n",
        "\n",
        "\n",
        "[01:13:45](https://youtu.be/blyXCk4sgEg?t=1h13m45s) Automate hyperparameters hyper-parameters with grid-search gridsearch\n",
        "Randomly subsample the dataset to reduce overfitting with ‘set_rf_samples()’, code detail at 1h18m25s\n",
        "\n",
        ">using grid search to tune hyperparameters, and using oob_score as a way to test is helpful \n",
        "\n",
        "\n",
        "\n",
        "[01:17:20](https://youtu.be/blyXCk4sgEg?t=1h17m20s) Tip for Favorita Grocery competition,\n",
        "‘set_rf_samples()’,\n",
        "‘reset_rf_samples()’,\n",
        "‘min_samples_leaf=’,\n",
        "‘max_features=’\n",
        "\n",
        "**min_samples_leaf=' '**  #creates a tree untill leaf size is '_', default is 1, this is a way to reduce the length of the tree, he likes 1,3,5,10,25\n",
        "\n",
        "**max_features='_', .5 would choose half of the columns to randomly sample for use in the trees. You can choose('auto','sqrt','log2', none) if value is int, it is considered the number of max features, if float, it is considerd a percent.\n",
        ">likes 1, .5, log2, sqrt, \n",
        "\n",
        "\n",
        "01:30:20 Looking at ‘fiProductClassDesc’ column with .cat.categories and .cat.codes\n",
        "\n",
        ">dont forget to learn from the tree(s). What are they telling you"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Wg2u2G3UVmjs"
      },
      "cell_type": "markdown",
      "source": [
        "###Lesson 3\n",
        "\n",
        "00:02:4416 When to use or not Random Forests (unstructured data like CV or Sound works better with DL),\n",
        "Collaborative filtering for Favorita\n",
        "\n",
        "00:05:106 dealing with missing values present in Test but not Train (or vice-versa) in ‘proc_df()’ with “nas” dictionary whose keys are names of columns with missing values, and the values are the medians.\n",
        "\n",
        "00:09:301 Starting Favorita notebook,\n",
        "The ability to explain the goal of a Kaggle competition or a project,\n",
        "What are independent and dependant variables ?\n",
        "Star schema warehouse database, snowflake schema\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "00:15:302 Use dtypes to read data without ‘low_memory = False’\n",
        "\n",
        ">Dont use low_memory = False, on large data sets\n",
        "\n",
        ">create a dictionary of columns and their dtypes, this is used to limit the memory\n",
        "\n",
        "``\n",
        "\n",
        "00:20:306 Use ‘shuf’ to read a sample of large dataset at start\n",
        "\n",
        "00:26:303 Take the Log of the sales with ‘np.log1p()’,\n",
        "Apply ‘add_datepart)’,\n",
        "‘split_vals(a,n)’,\n",
        "\n",
        "00:28:305 Models,\n",
        "‘set_rf_samples’,\n",
        "‘np.array(trn, dtype=np.float32’,\n",
        "Use ‘%prun’ to find lines of code that takes a long time to run\n",
        "\n",
        "00:33:302 We only get reasonable results, but nothing great on the leaderboard: WHY ?\n",
        "\n",
        "00:43:302 Quick look at Rossmann grocery competition winners,\n",
        "Looking at the choice of validation set with Favorita Leaderboard by Terence Parr (his @ pseudo here ?)\n",
        "\n",
        "00:50:306 Lesson2-rf interpretation,\n",
        "Why is ‘nas’ an input AND an output variable in ‘proc_df()’\n",
        "\n",
        "00:55:3010 How confident are we in our predictions (based on tree variance) ?\n",
        "Using ‘set_rf_samples()’ again.\n",
        "‘parallel_trees()’ for multithreads parallel processing,\n",
        "EROPS, OROPS, Enclosure\n",
        "\n",
        "01:07:153 Feature importance with ‘rf_feat_importance()’\n",
        "\n",
        "01:12:154 Data leakage example,\n",
        "Colinearity"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "GVETkzjkar8s",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PATH =\"data/grocery-sales/\"\n",
        "!ls [PATH]\n",
        "#add helper code to pick data types\n",
        "#shuf\n",
        "#onpromotion is a is a boolean (T/F) with missing vaues, so we have to deal with \n",
        "#missing vaues, convert the strings then conver to to boolean. \n",
        "types = {'id' : 'int64',\n",
        "        'item_nbr': 'int32',\n",
        "        'store_nbr':'int8',\n",
        "        'unit_sales': 'float32',\n",
        "        'onpromotion': 'object'} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "GzJ-_8NLbcUX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# %%time to time it\n",
        "#parse_dates = the column(s) with dates in them, creates a dt class in pandas\n",
        "# dytype = types assigns the dtypes to the types dictionry you defined \n",
        "#infer_datetime_format, need to look a source code but name says it all\n",
        "\n",
        "\n",
        "%%time\n",
        "df_all = pd.read_csv(f'{PATH}tranin.csv' parse_dates = ['date'], dtype = types,\n",
        "                    infer_datetime_format = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "BoHouZcEg2lB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(f'{PATH}test.csv' parse_dates = ['date'], dtype = types,\n",
        "                    infer_datetime_format = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "Aje80hNcdbGE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_all.onpromotion.fillna(False, inplace =True)\n",
        "df_all.onpromotion = df_all.onpromotion.map({'False': False, 'True' : True})\n",
        "df_all.onpromotion =df_all.onpromotion.astype(bool)\n",
        "\n",
        "df_test.onpromotion.fillna(False, inplace =True)\n",
        "df_test.onpromotion = df_test.onpromotion.map({'False': False, 'True' : True})\n",
        "df_test.onpromotion =df_test.onpromotion.astype(bool)\n",
        "\n",
        "%time df_all.to_feather('tmp/raw_groceries')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "ctoq0cN3eaVd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%time df_all.describe(include='all')\n",
        "\n",
        "#looking at data\n",
        "# what time frame does the data cover"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "DUz6N44xhBs1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "%time df_test.describe(include='all')\n",
        "\n",
        "# look at data\n",
        "# what time frame is the test set?\n",
        "# how can we create a valadation set that will look like test set?\n",
        "# random sampling?\n",
        "# Is there anything we should know about this period of time? how can we recreate?\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "xXXW_LhfitTa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_all.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "yLLy780YiwSR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_all.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "b54uVJI3i-fT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_all = pd.read_feather('tmp/raw_groceries')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "FBu5ncZVjHI8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#log of sales\n",
        "\n",
        "#competition stated negative sales are returns and we should consider them zero\n",
        "#we care about the log mean squared error\n",
        "#.clip(   dataframe_name{df_all}.dataframe_column{unit_sales}, minimum{0}, maximum{None}   )\n",
        "# np.log1p (data) = log plus 1 (that is what competition details stated), log of zero doesnt make sense\n",
        "\n",
        "\n",
        "\n",
        "df_all.unit_sales + np.log1p(np.clip(df_all.unit_sales, 0,None))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "83juquvnlxP2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "pgupKRgYlxeo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_valid = len(df_test)\n",
        "n_trn = len(df_all)-n_valid\n",
        "train, valid = split_vals(df_all, n_trn)\n",
        "\n",
        "train.shape, valid.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "eTY02aLSlxlE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#dont need to do because already numeric\n",
        "\n",
        "#train_cats(raw_train)\n",
        "#apply_cats(raw_)valid, raw_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "6YftYs0Jlxui",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trn, y = proc_df(train, 'unit_sales')\n",
        "val, y_val = Proc_df(valid, 'unit_sale')\n",
        "               "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "-3zny2IEkghO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n",
        "\n",
        "def print_score(m): \n",
        "  res = [rmse(m.predict(x), y), rmse(m.predict(val), y_val),\n",
        "              m.score(x, y), m.score(val, y_val)]\n",
        "  if hasattr(m, 'oob_score_'): res.append(m.oob_score_) \n",
        "  print(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "xddIKc11mEha",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "set_rf_aamples(1_000_000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "q99Yp4nrmMHY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#internally random forest converts the dataframe into an array of floats, so instead of\n",
        "#having to do this everytime we trained trees we can do it once and feed that to the \n",
        "#different models that you are going to create. \n",
        "%time \n",
        "\n",
        "x = np.array(trn, dtype=np.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "W6rz0XKwmogT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#n_jobs=8, uses 8 cores, most of the time you will use:\n",
        "#n_jobs=-1, which means all of the cores except 1\n",
        "\n",
        "m = RandomForestRegressor(n_estimators=20, min_sample_leaf=100, n_jobs=8, oob_score=False)\n",
        "\n",
        "%time m.fit(x,y)\n",
        "#%prun, runs a profiler it will tell you what is taking longest, \n",
        "%prun m.fit(x,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "Iub8-2h5q7fH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print_score(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "EQzjk66-6sBQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#calibration of valadation set\n",
        "#Using a graph to figure out how good your valadation set is\n",
        "#compairing random model on your test and validation sets to see how close the scores are\n",
        "#a good validation set will have similar model scores to the test\n",
        "\n",
        "ax1.scatter(kaggle, validation_a, alpha = 0.5 color = 'blue') #alpha is opacity\n",
        "axl.plot(x, ml*x + b1,'-' color-'red', alpha 0.5)\n",
        "\n",
        "ax2.set xlabel('Kaggle scores')\n",
        "ax2.set ylabel('Validation b set scores')\n",
        "ax2.scatter(kaggle, validation_b, alpha = 0.5, color ='green')\n",
        "ax2. plot(x, m2*x + b2, '-', colors'red', alpha=0.5)\n",
        "          \n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bQTjgH2A9DZ5"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://trello-attachments.s3.amazonaws.com/5b4e4c98425d3c2907e22c20/5b909dd28c56e7476e452ca0/4d7476ef24b1202a40edc62fe1bd8a3f/trello1337943234671236216.jpg)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5r7BDf7_Ei9D"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "H0eaprlaEkLl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "set_rf_samples(50000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "LloqGmxJEkcR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
        "m.fit(X_train, y_train)\n",
        "print_score(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "Jj9QzCBqEkov",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oRCgHgiRdtL7"
      },
      "cell_type": "markdown",
      "source": [
        "####feature importance"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "40qth2pGVM5b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from fast.ai library, uses scikit-learn function\n",
        "#m = model\n",
        "#df_trn is dataframe\n",
        "\n",
        "fi = rf_feat_importance(m, df_trn);\n",
        "fi[:10]#pandas data frame with order of importance, [:10]= is top 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "WJELRdXrfN_h",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#because it is a datafram you can use plotting commands\n",
        "#can see how many featurs are important\n",
        "fi.plot('cols', 'imp', figsize=(10,6), legend=False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "lJz8zgHDfwII",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#function that will plot bars\n",
        "\n",
        "#cols = \n",
        "#imp =\n",
        "#barh = \n",
        "#figsize= (12,7)\n",
        "\n",
        "def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "KV_yUOtTfwSu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_fi(fi[:30]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "oMOiU7nQE9DE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_preds(t): return t.predict(X_valid)\n",
        "%time preds = np.stack(parallel_trees(m, get_preds))\n",
        "\n",
        "np.mean(preds[:,0]), np.std(preds[:,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "RJrNlICrE9UV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " #make a copy of your data\n",
        "x = raw_valid.copy()\n",
        "#add a column for the standard deviation of the predictions\n",
        "x['pred_std'] = np.std(preds, axis=0)\n",
        "#add a column for the mean\n",
        "x['pred'] = np.mean(preds, axis=0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "0foMVaReiOyD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#plot Enclosure, which is one of the features of the data set. \n",
        "x.Enclosure.value_counts().plot.barh();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "_n9SV3lPE9kZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#take a deeper look \n",
        "\n",
        "#list of feature of interest, dependent variable, prediction, and prediction std.\n",
        "flds = ['Enclosure', 'SalePrice', 'pred', 'pred_std']\n",
        "\n",
        "#group by feature of interest('Enclosure') and get the mean\n",
        "enc_summ = x[flds].groupby('Enclosure', as_index=False).mean()\n",
        "enc_summ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "wqjdrA0iFPOR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#sale price for each level of enclosure\n",
        "enc_summenc_summ = enc_summ[~pd.isnull(enc_summ.SalePrice)]\n",
        "enc_summ.plot('Enclosure', 'SalePrice', 'barh', xlim=(0,11));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "imwcO4duTu2_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#plot predictions\n",
        "#little bars(error bars) would be confidence intervals thats the 'xerr = _'\n",
        "\n",
        "enc_summenc_summ.plot('Enclosure', 'pred', 'barh', xerr='pred_std', alpha=0.6, xlim=(0,11));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "jgPjR2-hVMoj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_valid.ProductSize.value_counts().plot.barh();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "u-dnrRoaVMyB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "flds  =  [['ProductSize''Product , 'SalePrice', 'pred', 'pred_std']\n",
        "summ = x[flds].groupby(flds[0]).mean()\n",
        "summ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "6dXZVxxjVNC7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#sort by the ratio of the standard deviations to the predictions\n",
        "#the values on the top, will have the biggest relative difference so they could \n",
        "#be considered less accurate\n",
        "#keep in mind this is normal in small groups because you have less of them \n",
        "\n",
        "(summ.pred_std/summ.pred).sort_values(ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "ld5k2MPlVM79",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vlfVRsAljB4S"
      },
      "cell_type": "markdown",
      "source": [
        "after you are done looking around you can remove some of the features that arent helpful"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "MLmMRNaZiMbX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "to_keep = fi[fi.imp>0.005].cols; len(to_keep)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "eEj0vIjihzng",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_keep = df_trn[to_keep].copy()\n",
        "X_train, X_valid = split_vals(df_keep, n_trn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "8yUuFvPdjTdb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "m =  RandomForestRegressor (n_estimators=40, min_samples_leaf=3, max_features=0.5,\n",
        "                          n_jobs=-1, oob_score=True)\n",
        "m.fit(X_train, y_train)\n",
        "print_score(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "aoRO15YGjT32",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fi = rf_feat_importance(m, df_keep)\n",
        "plot_fi(fi);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "RRsvPWDRjT0X",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#default sampling method is bootstraping, selecting data with replacement\n",
        "\n",
        "#sample size, \n",
        "#min leaf size, \n",
        "#max_features, the amount of features that are in a random tree. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7mjzootVtazs"
      },
      "cell_type": "markdown",
      "source": [
        "####One hot encoding"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "YjXZHqXfjTx4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#one hot encoding is not required for randome forest but it often gives you a \n",
        "# better understanding of the data. \n",
        "\n",
        "#rule of thumb is picking 6 or 7 is pretty good\n",
        "#max_n_cat='_' is the number of value it will convert to one hot encoded, \n",
        "#columns with more levels than this will be left unchanged. \n",
        "#\n",
        "df_trn2,  y_trn,  nas =  proc_df(df_raw, 'SalePrice', max_n_cat=7)\n",
        "X_train, X_valid = split_vals(df_trn2, n_trn)\n",
        "\n",
        "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)\n",
        "m.fit(X_train, y_train)\n",
        "print_score(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "00iLfGrMjTsK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fi = rf_feat_importance(m, df_trn2)\n",
        "plot_fi(fi[:25]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PxEAAR6eh1KV"
      },
      "cell_type": "markdown",
      "source": [
        "####Remove Redundant Features"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "BgRGW7Be9kNT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#used for clustering\n",
        "from scipy.cluster import hierarchy as hc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "WtQEHTWD9rd2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#dendrogram - from Greek dendro \"tree\" and gramma \"drawing\"\n",
        "# a way to visualize clusters produced by hierarchical clustering.\n",
        "# it use the greedy method/algorithum which means every cluster is made based on \n",
        "# the locally optimal choice at each stage\n",
        "# what to objects are closest, merge them and repalce with mean\n",
        "\n",
        "\n",
        "#we are using the spearman rank correlation, very similar to the r^2 but compares \n",
        "#two variables opposed to a variable and its prediction\n",
        "#we use a correlation on its rank, not value, trees dont care about linerality \n",
        "# they just care about rank\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\n",
        "corr_condensed = hc.distance.squareform(1-corr)\n",
        "z = hc.linkage(corr_condensed, method='average')\n",
        "fig = plt.figure(figsize=(16,10))\n",
        "dendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "xyxc0_mH9xab",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def get_oob(df):\n",
        "         m  =  RandomForestRegressor(n_estimators=30, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)\n",
        "    x, _ = split_vals(df, n_trn)\n",
        "    m.fit(x, y_train)\n",
        "    return m.oob_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "wHh7iiLq-G-S",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_oob(df_keep)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "Z5nV3bL8-Yyp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for c in ('saleYear', 'saleElapsed', 'fiModelDesc', 'fiBaseModel', 'Grouser_Tracks', 'Coupler_System'):\n",
        "    print(c, get_oob(df_keep.drop(c, axis=1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "-7Fsag3h-YvA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#get ride of one of each, as long as your model doesnt suffer. \n",
        "\n",
        "to_drop = ['saleYear', 'fiBaseModel', 'Grouser_Tracks']\n",
        "get_oob(df_keep.drop(to_drop, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "hWUIX-MF-YsP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_keep.drop(to_drop, axis=1, inplace=True)\n",
        "X_train, X_valid = split_vals(df_keep, n_trn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "QSw2f3dj-Yom",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save('tmp/keep_cols.npy', np.array(df_keep.columns))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "oRb-8MUc-Ykx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keep_cols = np.load('tmp/keep_cols.npy')\n",
        "df_keep = df_trn[keep_cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "V57ITJw_-Yhx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_rf_samples()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "RMGs3VBg-rul",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
        "m.fit(X_train, y_train)\n",
        "print_score(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "94mvTbdy-sGB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1jF44JZxJC95"
      },
      "cell_type": "markdown",
      "source": [
        "####Partial Dependence"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fbzpsmt2JHok"
      },
      "cell_type": "markdown",
      "source": [
        "of the features that are important, how do they relate to the dependent variable."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "zFn1XmnJJmpf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pdpbox import pdp\n",
        "from plotnine import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "UwrVhtwwJmmm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "set_rf_samples(50000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dyJdd9caKCYs"
      },
      "cell_type": "markdown",
      "source": [
        "This next analysis will be a little easier if we use the 1-hot encoded categorical variables, so let's load them up again."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "2UuZsbPqJmju",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\n",
        "X_train, X_valid = split_vals(df_trn2, n_trn)\n",
        "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\n",
        "m.fit(X_train, y_train);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "WoJyNl9KJmf9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_fi(rf_feat_importance(m, df_trn2)[:10]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "VhPlV1zzJmdH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_raw.plot('YearMade', 'saleElapsed', 'scatter', alpha=0.01, figsize=(10,8));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "hCWzfNoFJmZv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#there was some bad data that we dont want to use, \n",
        "#we also dont want to plot all the points, so we use get_sample to get a sample of 500\n",
        "\n",
        "\n",
        "x_all = get_sample(df_raw[df_raw.YearMade>1930], 500)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "9Ln8FU3XJmVP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ggplot(x_all, aes('YearMade', 'SalePrice'))+stat_smooth(se=True, method='loess')\n",
        "\n",
        "#often times these plots are not as clear/linear as we would expect, and this \n",
        "#can often be caused by other variables like a sale or blizzard, etc. we really \n",
        "#want to know what the relationship given everything else being even. We do this\n",
        "#with a partial dependence plot\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "Lq314lN7N3zU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = get_sample(X_train[X_train.YearMade>1930], 500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "vdw6E8GGJmQo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_pdp(feat, clusters=None, feat_name=None):\n",
        "    feat_name = feat_name or feat\n",
        "    p = pdp.pdp_isolate(m, x, feat)\n",
        "    return pdp.pdp_plot(p, feat_name, plot_lines=True, \n",
        "                        cluster=clusters is not None, n_cluster_centers=clusters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "3i2FXBanN-Az",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#it basicly replaces the whole column with a constant, and determins how much of \n",
        "#the model changes with the different constants\n",
        "#\n",
        "#helps us to understand what is going on on average\n",
        "#this is an attempt to remove all the exteranilities. \n",
        "plot_pdp('YearMade')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "En92_ii9OCOa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#using clusters can see different clusters to see if there is specific behaivior \n",
        "#of different clusters. \n",
        "#used to look for other aspects you might want to understand\n",
        "plot_pdp('YearMade', clusters=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "ILigFbIeOJ0W",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#PDP interaction plot\n",
        "\n",
        "#shows 'saleElapsed' vs 'price'\n",
        "#'YearMade' vs 'price'\n",
        "# and combination of the two(keep in mind it is log of price)\n",
        "\n",
        "feats  =['saleElapsed' , 'YearMade']\n",
        "p = pdp.pdp_interact(m, x, feats)\n",
        "pdp.pdp_interact_plot(p, feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "2IibXINHOVWm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#if you have one hot encoed variables you can pass and array of them to pdp, and\n",
        "#pdp will treat them as categories\n",
        "\n",
        "plot_pdp(['Enclosure_EROPS w AC', 'Enclosure_EROPS', 'Enclosure_OROPS'], 5, 'Enclosure')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "rpcmtkx9OZYf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#clean up and create feature\n",
        "df_raw.YearMade[df_raw.YearMade<1950] = 1950\n",
        "df_keep['age'] = df_raw['age'] = df_raw.saleYear-df_raw.YearMade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "0vz5lYAROZn6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "X_train,  X_valid=  split_vals(df_keep, n_trn)\n",
        "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\n",
        "m.fit(X_train, y_train)\n",
        "plot_fi(rf_feat_importance(m, df_keep));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2ZxWU7xVclH3"
      },
      "cell_type": "markdown",
      "source": [
        "###Tree interpreter"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "RU6SDnRicrRB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#the why in data science\n",
        "from  treeinterpreter  import treeinterpreter as ti"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "j6UXu3ckcxN6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#set up data\n",
        "df_train, df_valid = split_vals(df_raw[df_keep.columns], n_trn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "4u2Geli8cxtZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#seperate a row\n",
        "row = X_valid.values[None,0]; row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "PwbMLUu8cyF5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pass to treeinterpreter the random forest model and row\n",
        "#and get back the prediction, bias, and contributions\n",
        "#in this example the bias is the price across the whole data set\n",
        "#at each level of the tree we are adding or subtracting with each branch of the tree, \n",
        "#these are then averaged accross all the trees, and that is the contribution. \n",
        "# the contributions are how much a feature is worth\n",
        "\n",
        "prediction, bias, contributions = ti.predict(m, row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "AhvrumcMcyAs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prediction[0], bias[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "_0p7iWmacx9M",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#need to do an index sort, if not they contribution wont be matched with name\n",
        "#argsort, doesnt actually sort it just tells you where it would be moved to if it\n",
        "#was sorted\n",
        "\n",
        "idxs = np.argsort(contributions[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "VKPQl_ekcxqK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#grabs column by idxs, finds row 0's idex, \n",
        "#and that rows associateed contribution scores/adjustments\n",
        "\n",
        "#column name, feature present(name), contribution\n",
        "\n",
        "[o for o in zip(df_keep.columns[idxs], df_valid.iloc[0][idxs], contributions[0][idxs])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "yhqsniKIcxd8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "contributions[0].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HQbMCi-Ckjow"
      },
      "cell_type": "markdown",
      "source": [
        "Interaction feature importance\n",
        "\n",
        "showing the relationship between the variables in the branch. \n",
        "\n",
        "blank interacts with.blank"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JNBkQk1ZdLAA"
      },
      "cell_type": "markdown",
      "source": [
        "#Extrapolation"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uG-IAgHtkiRG"
      },
      "cell_type": "markdown",
      "source": [
        "we are using oob score as a way to test our model but it is not quite the same, it is just sliglty less good, this is becase testing the validation set we get the change to use all the trees, but with oob score each row will be using a subset of the trees, with less trees you will have less accurate prediction. overall still pretty close. "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6a8ekJBwnTZY"
      },
      "cell_type": "markdown",
      "source": [
        "when dealing with data that has a time element, we want to be very carfull picking a validation set that is as close to the test as possible. to accomplish this:\n",
        "\n",
        "*   make 5 models, some good, some less good\n",
        "*   plot results on validation x-axis, and test y-axis\n",
        "*   pick different models, trained on different data/features\n",
        "*   the goal is to see if validation set is as similar to test set as possible\n",
        "\n",
        "Different ways to pick validation set based on time\n",
        "\n",
        "\n",
        "*   last two weeks\n",
        "*   Last month\n",
        "* last year\n",
        "* most recent period that had same structure(holiday weekeds,that had payday, etc.)\n",
        "\n",
        "\n",
        "Once you have a validation set that you are fairly confident that generalized to the test set. you can create a model and once that model is complete you can rerun it with the whole data set-test set, and then again on all the data. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cVy9eEjfrLXI"
      },
      "cell_type": "markdown",
      "source": [
        "#####cross valadation \n",
        "is not always benefital because of temporal issues, sometime it dont matter but things change over time, just using a random sample of your data make it much harder to uncover the temporal issues with the data, often we want a validation set that is most similar to the training set as possible. "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MrAo2VT9ltvy"
      },
      "cell_type": "markdown",
      "source": [
        "###Crete your own data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4khdDEjyl008",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from fastai.imports import*\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "G5moha9yobDo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WZ0QklLHmJUm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#linspace creates evenly spaced data between, start(0) and stop(1)\n",
        "\n",
        "x = np.linspace(0,1); x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VrX4E_3jmunc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#add a little randomness to the data \n",
        "#np.random.uniform(low, hight, size)\n",
        "#x.shape is size\n",
        "\n",
        "x.shape#is a tuple with one thing in it (50,)\n",
        "\n",
        "y = x + np.random.uniform(-0.2, 0.2, x.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "I85sXwx9nJaY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(x,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vrRdZMDJqs04",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#seperating train and test\n",
        "#train is first 40, test is last 10\n",
        "x_trn, x_val = x[:40],x[40:]\n",
        "y_trn, y_val = y[:40],y[40:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Bmf9yVD0tINa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#shape of data \n",
        "\n",
        "#Currently x is a 1 dimentional array/vector/rank 1 tensor\n",
        "#To convert it into a two dimentional array/matrix/rank 2 tensor\n",
        "# we need to slice it\n",
        "#x[:] = give me everything on that axis\n",
        "#x[;,None] = give me everyting on the 1st axis, None is a special indexer,\n",
        "#which means add a unit axis here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aFgxnSVZvsP_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WpQ2T_FPvso7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x[None,:].shape #you can leave off the ,:\n",
        "#x[None]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XZNASyzuv6Hf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x[:,None].shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gBZc9lVL1ban",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x1[...,None].shape   #gives you the same thing but this is saying to add as many tensors to get\n",
        "#it to work"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vlFS9O8h1lzg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#seperating train and test\n",
        "#train is first 40, test is last 10\n",
        "x_trn, x_val = x1[:40],x1[40:]\n",
        "y_trn, y_val = y[:40],y[40:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XuaVU_3Dsqem",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create a random forest\n",
        "m= RandomForestRegressor().fit(x_trn,y_trn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "R16Zp00QqtNQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.ploy(y_trn, m.predict(x_trn))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "52Op1Wug2EIS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.ploy(y_val, m.predict(x_val))\n",
        "#this really highlights the problem with using randome forest because they cant extrapolate to\n",
        "#something that they havent seen befor. there are several approcahes to fix this, one would be useing\n",
        "#a time series tool to detrend it, \n",
        "#use a GBM which makes a small tree, and calculates the residules,\n",
        "#then the next tree just predictis the residules\n",
        "#nural nets would also work\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "aLLAevw6xQhr"
      },
      "cell_type": "markdown",
      "source": [
        "###dealing with time/temporal issues"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DbVanNirmI29"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6Gck66BsxcqY"
      },
      "cell_type": "markdown",
      "source": [
        "one wasy to imporve your model, getting oob to be closer to validation score, is to remove features that have a very high time corrolation. \n",
        "\n",
        "Random forest dont understand the association of time and price. one way to help your model generalize better is to remove features that have a time component. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "88YFmq4-cxYL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#train a random forest to predict if a row is in the validation set\n",
        "#this is also a good way to test if you have been given a random sample or not\n",
        "df_ext=  df_kee.copy()\n",
        "df_ext['is_valid'] = 1\n",
        "df_ext.is_valid[:n_trn] = 0\n",
        "x, y, nas = proc_df(df_ext, 'is_valid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BFKF5tNbcxKZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#train random forest\n",
        "m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
        "m.fit(x, y);\n",
        "m.oob_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LbpKDPASdZdx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#grabs to ten features base on importance\n",
        "fi = rf_feat_importance(m, x); fi[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pp3W01BcdZzc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create a list of features to look at\n",
        "\n",
        "feats=['SalesID', 'saleElapsed', 'MachineID']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yUT1QVmudcCe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#look at traing set\n",
        "(X_train[feats]/1000).describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3T73BPKfdcYH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#look at difference in validation set\n",
        "(X_valid[feats]/1000).describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4AjK1W66dZUS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#drop them\n",
        "x.drop(feats, axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fo0J5YAKdZKA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#retrain\n",
        "m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
        "m.fit(x, y);\n",
        "m.oob_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Y76-XfUHdwfI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#relook at features\n",
        "fi = rf_feat_importance(m, x); fi[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dogxMJDCdwaw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#because we are going to try to drop one feature at a time we want to reduce the train\n",
        "#time by setting the sample size.\n",
        "set_rf_samples(50000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FlsZKjasdwWO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#since main temporal features have been removed others have a chance to come to \n",
        "#the top\n",
        "\n",
        "feats=['SalesID', 'saleElapsed', 'MachineID', 'age', 'YearMade', 'saleDayofyear']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mm8g-d_TdwRw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#get model baseline with smaller set sample size\n",
        "X_train, X_valid = split_vals(df_keep, n_trn)\n",
        "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
        "m.fit(X_train, y_train)\n",
        "print_score(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "i9l-tYWNdwNu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#one at a time remove the selected featurs and look at the impact\n",
        "\n",
        "for f in feats:\n",
        "    df_subs = df_keep.drop(f, axis=1)\n",
        "    X_train, X_valid = split_vals(df_subs, n_trn)\n",
        "    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
        "    m.fit(X_train, y_train)\n",
        "    print(f)\n",
        "    print_score(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MMpJGRoWdwJU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#reset the samples becuase we are going to train RF again after removing \n",
        "#more temporal features that dont benefit the model\n",
        "reset_rf_samples()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lnsedxw7dwDo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pick which featurs to drop\n",
        "\n",
        "df_subs = df_keep.drop(['SalesID', 'MachineID', 'saleDayofyear'], axis=1)\n",
        "X_train, X_valid = split_vals(df_subs, n_trn)\n",
        "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
        "m.fit(X_train, y_train)\n",
        "print_score(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XJODhNQ6dv_p",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#replot importance\n",
        "plot_fi(rf_feat_importance(m, X_train));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "o061Z6jFdvvZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save('tmp/subs_cols.npy', np.array(df_subs.columns))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vT0xsQmTdvri",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#train again with more trees\n",
        "m = RandomForestRegressor(n_estimators=160, max_features=0.5, n_jobs=-1, oob_score=True)\n",
        "%time m.fit(X_train, y_train)\n",
        "print_score(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VpAuDS495nmA"
      },
      "cell_type": "markdown",
      "source": [
        "#Creating a Random Forest from Scratch"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bHt_yii85zDD"
      },
      "cell_type": "markdown",
      "source": [
        "assume everything is wrong\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XiFDO3y3dvnc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from fastai.imports import *\n",
        "from fastai.structured import *\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from IPython.display import display\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ly4h0S-Vdvjd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load data\n",
        "\n",
        "PATHPATH  ==  \"data/bulldozers/\"\"data/b \n",
        "\n",
        "#existing data set\n",
        "df_raw = pd.read_feather('tmp/bulldozers-raw')\n",
        "df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')\n",
        "\n",
        "#existing validation set\n",
        "def split_vals(a,n): return a[:n], a[n:]\n",
        "n_valid = 12000\n",
        "n_trn = len(df_trn)-n_valid\n",
        "X_train, X_valid = split_vals(df_trn, n_trn)\n",
        "y_train, y_valid = split_vals(y_trn, n_trn)\n",
        "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
        "\n",
        "#for simplicity just start with two columns\n",
        "x_sub = X_train[['YearMade', 'MachineHoursCurrentMeter']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EYmYiBnT6ZwE"
      },
      "cell_type": "markdown",
      "source": [
        "##Writting classes"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jQf4O0P28tr7"
      },
      "cell_type": "markdown",
      "source": [
        "[01:21:04](https://youtu.be/3jl2h9hSRvc?t=1h21m) class ‘DecisionTree()’,\n",
        "Bonus: Object-Oriented-Programming (OOP) overview, critical for PyTorch\n",
        "\n",
        "\n",
        "write code from top down, thinking that everything that you need is already created, then go deeper. \n",
        "\n",
        "methods have () after it\n",
        "properties or atributes are things you can grab\n",
        "\n",
        "constructior is something that creates an object\n",
        "it uses a magic method called duntor init or __init__\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BjWYry42t6ue"
      },
      "cell_type": "markdown",
      "source": [
        "Without notes"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "Kl18Q_PvuDpr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TreeEnsemble():\n",
        "    def __init__(self, x, y, n_trees, sample_sz, min_leaf=5):\n",
        "        np.random.seed(42)\n",
        "        self.x,self.y,self.sample_sz,self.min_leaf = x,y,sample_sz,min_leaf\n",
        "        self.trees = [self.create_tree() for i in range(n_trees)]\n",
        "\n",
        "    def create_tree(self):\n",
        "        rnd_idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
        "        return DecisionTree(self.x.iloc[rnd_idxs], self.y[rnd_idxs], min_leaf=self.min_leaf)\n",
        "        \n",
        "    def predict(self, x):\n",
        "        return np.mean([t.predict(x) for t in self.trees], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "dzGsScoYuNqq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DecisionTree():\n",
        "    def __init__(self, x, y, idxs=None, min_leaf=5):\n",
        "        if idxs is None: idxs=np.arange(len(y))\n",
        "        self.x,self.y,self.idxs,self.min_leaf = x,y,idxs,min_leaf\n",
        "        self.n,self.c = len(idxs), x.shape[1]\n",
        "        self.val = np.mean(y[idxs])\n",
        "        self.score = float('inf')\n",
        "        self.find_varsplit()\n",
        "        \n",
        "    # This just does one decision; we'll make it recursive later\n",
        "    def find_varsplit(self):\n",
        "        for i in range(self.c): self.find_better_split(i)\n",
        "    \n",
        "    @property\n",
        "    def split_name(self): return self.x.columns[self.var_idx]\n",
        "    \n",
        "    @property\n",
        "    def split_col(self): return self.x.values[self.idxs, self.var_idx]\n",
        "\n",
        "    @property\n",
        "    def is_leaf(self): return self.score == float('inf')\n",
        "    \n",
        "    def __repr__(self):\n",
        "        s = f'n: {self.n}; val:{self.val}'\n",
        "        if not self.is_leaf:\n",
        "            s += f'; score:{self.score}; split:{self.split}; var:{self.split_name}'\n",
        "        return s\n",
        "      \n",
        "    def find_better_split (self, var_idx):\n",
        "        x,y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n",
        "\n",
        "        for i in range(1,self.n-1):\n",
        "            lhs = x<=x[i]\n",
        "            rhs = x>x[i]\n",
        "            if rhs.sum()==0: continue\n",
        "            lhs_std = y[lhs].std()\n",
        "            rhs_std = y[rhs].std()\n",
        "            curr_score = lhs_std*lhs.sum() + rhs_std*rhs.sum()\n",
        "            if curr_score<self.score: \n",
        "                self.var_idx,self.score,self.split = var_idx,curr_score,x[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "dLa66u597g_L",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_better_split (self, var_idx):\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YnXplsqgt-h1"
      },
      "cell_type": "markdown",
      "source": [
        "With notes"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "PVBAVw7_6X3l",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#every class need a dundor init, double underscore, and it needs to be initialized, \n",
        "#what do you need to pass in\n",
        "#you can use any nameing convention as long as it is self\n",
        "\n",
        "#self-self is alway required\n",
        "#x- independent vairables\n",
        "#y- dependent variables\n",
        "#n_trees- number of trees\n",
        "#sample_sz= sample size\n",
        "# min_leaf=5, is an optional parameter for minimum leaf size\n",
        "class  TreeEnsemble ():\n",
        "    def __init__(self, x, y, n_trees, sample_sz, min_leaf=5):\n",
        "        np.random.seed(42) #for testing it is nice to use a consistant seed\n",
        "         #since you passed in 5 things to this object you need  to assign them withself.x = x format\n",
        "         # you can assign from a tuple to a tuple\n",
        "        self.x,self.y,self.sample_sz,self.min_leaf = x,y,sample_sz,min_leaf \n",
        "        #list comprenetion to creat the trees, call a function.create_trees for each of the n_trees\n",
        "        self.trees = [self.create_tree() for i in range(n_trees)]\n",
        "\n",
        "    def create_tree(self):\n",
        "      #set random indexes\n",
        "      #np.random.permutation passing it an int,len(self.y = number of independent varalbles), \n",
        "      #it will return a randomly shuffleed sequesnce with length = int,\n",
        "      #grab the first \":n items\" in this case the sample_sz, it is now a random subsample\n",
        "      #this is not bootstrapping, it is subsampeling, since we dont have replacement\n",
        "        rnd_idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
        "        #np.random.choice is slightly more direct way to do this\n",
        "        #this assumes we have writen a class called DecisionTree\n",
        "        \n",
        "        #we dont watnt to pass in all of x, we want just the specific index\n",
        "        #x is a pandas dataframe, and if we want to index into it with specific integers\n",
        "        #we use .iloc(interger locations), it makes it behave indexing wise just like numpy\n",
        "        \n",
        "        #x --- uses iloc to look up integer\n",
        "        #y ---  is a numpy array, so we can just index into it. \n",
        "        #min_leaf --- we are going to keep track of our minimum leaf size\n",
        "        return DecisionTree(self.x.iloc[rnd_idxs], self.y[rnd_idxs], min_leaf=self.min_leaf)\n",
        "      #this tree is going to give you what % of that leaf node containts the item of interest,19% cats, etc\n",
        "        \n",
        "    def predict(self, x):\n",
        "      #x is number of rows\n",
        "      #for every row take the prediction for each tree, and take the average of it\n",
        "      #you can use numpy.mean with not np list you just need to tell it axis=0 with means \n",
        "      #to average across the lists\n",
        "      #using tree.predict \n",
        "        return np.mean([t.predict(x) for t in self.trees], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "xJi17vYjdvZj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#in video he assigned idxs in create_trees, this one does it in this class\n",
        "#pass in argumentns\n",
        "#n is the number of rows, and is calculated by using length of idxs(which is len(y))\n",
        "#c is columns, using .shape on x, which is the number of columns in our independent variables\n",
        "#val is the mean of the dependent variable at this part of the tree\n",
        "#all the randomeness is in the trees, the DecisionTree class donesnt have any randomness in it\n",
        "\n",
        "#we know we are passing into the DecisionTree, x,y and min_leaf\n",
        "\n",
        "\n",
        "class DecisionTree():\n",
        "    def __init__(self, x, y, idxs=None, min_leaf=5):#we need to keep track of which row went \n",
        "      #in which tree, so we need to keep track of idexes, with crete a default of inxs=None\n",
        "        if idxs is None: idxs=np.arange(len(y)) #if None(default), create a idex with \n",
        "          #np.arrage(np version of range, as a np array), this is done so root of a decision \n",
        "          #tree contains all of the rows\n",
        "        self.x,self.y,self.idxs,self.min_leaf = x,y,idxs,min_leaf #store all the information given\n",
        "        self.n,self.c = len(idxs), x.shape[1] #defines n =number of rows, c = number of columns\n",
        "        self.val = np.mean(y[idxs])#every leaf and every node in a tree has a value(prediction)\n",
        "        #which is the average of the dependent variable, y indexed with the indexes, is the vaules \n",
        "        #of the dependent variables that are in this branch of the tree\n",
        "        self.score = float('inf')#branch nodes(not leaves) of the tree also have  a score, which \n",
        "        #tracks how efective was the split, only true if they are branch, starts as infinity\n",
        "        self.find_varsplit()#find a better split\n",
        "        \n",
        "    # This just does one decision; \n",
        "    def find_varsplit(self):\n",
        "        for i in range(self.c): self.find_better_split(i)#go through each potental variable (c) \n",
        "          #find a better split \n",
        "          #this is considering max features is set to all \n",
        "            \n",
        "    # take the index of a variable, and find out if there is an split beter then what we had befor\n",
        "    \n",
        "    def  find_better_split (self, var_idx): \n",
        "        x,y = self.x.values[self.idxs,var_idx], self.y[self.idxs]# define our x and y variables, \n",
        "        # x= take all values of x(x.values), but only indexes in our node(self.indxs], \n",
        "        # and just this one varable(var_idx), and for y it is just whatever our independent variable \n",
        "        # is at the indexeses in this node\n",
        "\n",
        "        #we are trying to uncover our infomation gain if we split at any given level\n",
        "        #common methods for measuring information gain are gini, cross entropy, root-mean-squared error,etc\n",
        "        \n",
        "        #there is an alternative to root mean squared error that is mathmaticly the same within a constant scale \n",
        "        #that is a little eaiser to deal with, \n",
        "        #we are going to try to find a split that minimizes the standard deviations of the two groups\n",
        "        #if you find two groups that minimizes the weighted average of the two standared deviations \n",
        "        #it is the same as minimizing the root mean squared error\n",
        "        for i in range(1,self.n-1):\n",
        "          #lhs, and rhs are arrays of booleans\n",
        "            lhs = x<=x[i]  #left hand side, any value in x less then [i]paticular value\n",
        "            rhs = x>x[i]   #right hand side, any value in x greater then [i]paticular value\n",
        "            if rhs.sum()==0: continue #this is because cant take a standard deviation of an empty set\n",
        "            lhs_std = y[lhs].std() # calc standard deviation of left hand side\n",
        "            rhs_std = y[rhs].std() # calc standard deviation of right hand side\n",
        "            curr_score = lhs_std*lhs.sum() + rhs_std*rhs.sum() #take the weight average, same as sums to a scaler\n",
        "            if curr_score<self.score: #is this better than our best score so far\n",
        "                self.var_idx,self.score,self.split = var_idx,curr_score,x[i]#if better store index, score, value we split\n",
        "    \n",
        "    @property\n",
        "    def split_name(self): return self.x.columns[self.var_idx]\n",
        "    \n",
        "    @property\n",
        "    def split_col(self): return self.x.values[self.idxs,self.var_idx]\n",
        "\n",
        "    @property\n",
        "    def is_leaf(self): return self.score == float('inf')\n",
        "    #a leaf is something we dont split on so it is still set to \"infinity\"\n",
        "    \n",
        "    \n",
        "  #dundor representation\n",
        "  #when you print the object you want something useful to print the defaults are pretty bad, if \n",
        "  #you wnat to make is useful you will us the __repr__ \n",
        "  \n",
        "  #f' = is a format string\n",
        "  #  n: {self.n}; val:{self.val}   = pring n: n and val:val, how many rows and average value\n",
        "  #if not self.is_leaf:   = if its not a leaf do:\n",
        "  #.is_leaf is a special type of method called a property, it doesnt have (). it looks like a \n",
        "  #regualar variable but it is calculated on the fly, it uses a special thing called a decorator\n",
        "  #one of the predefined decorators is @property, decorators are a way to tell python more infomration\n",
        "  #about a method they are used alot in web frameworks like flask\n",
        "  \n",
        "  #print score(.score), value we split out(.split) and the variable we split at(.split_name)\n",
        "  \n",
        "  \n",
        "  \n",
        "    def __repr__(self):\n",
        "        s = f'n: {self.n}; val:{self.val}'\n",
        "        if not self.is_leaf:\n",
        "            s += f'; score:{self.score}; split:{self.split}; var:{self.split_name}'\n",
        "        return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "QiRF1Wam2Dos",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#putting all the parts togeather\n",
        "m = TreeEnsemble(X_train, y_train, n_trees=10, sample_sz=1000, min_leaf=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "Wm8vGKfV2Ps4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#now print out the n: n, val: val\n",
        "m.trees[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "IZNIVrkS2ohD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#you can also print out the indexes\n",
        "\n",
        "m.trees[0].idxs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YnK3cXAG3tiK"
      },
      "cell_type": "markdown",
      "source": [
        "###Imitation is the sincerest [form] of flattery(using scikit learn)\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "O5xifjcV7r0F"
      },
      "cell_type": "markdown",
      "source": [
        "We arent really imitating Scikit learn, we are more or less using its to confirm what we are doing is correct. oddly enough this can lead to you finding problems with some of the best know implementations. [example of how faster might not be better](http://explained.ai/rf-importance/index.html)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "gv_Fc5-i2zz1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "ens  =  TreeEnsembl(x_sub, y_train, 1, 1000) #create an ensembel\n",
        "tree = ens.trees[0]#grab a tree\n",
        "x_samp,y_samp = tree.x, tree.y#find out which random sample of x and y did this tree use, and store'em\n",
        "x_samp.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "A-VFCiQx3EFh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "gQqcdS_g3F1i",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = RandomForestRegressor(n_estimators=1, max_depth=1, bootstrap=False)#1 tree, 1 decision, no replacement\n",
        "m.fit(x_samp, y_samp)\n",
        "draw_tree(m.estimators_[0], x_samp, precision=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "61OAZvPd3FqK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def  find_better_split (self, var_idx): \n",
        "    x,y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n",
        "\n",
        "    for i in range(1,self.n-1):\n",
        "        lhs = x<=x[i]\n",
        "        rhs = x>x[i]\n",
        "        if rhs.sum()==0: continue\n",
        "        lhs_std = y[lhs].std()\n",
        "        rhs_std = y[rhs].std()\n",
        "        curr_score = lhs_std*lhs.sum() + rhs_std*rhs.sum()\n",
        "        if curr_score<self.score: \n",
        "            self.var_idx,self.score,self.split = var_idx,curr_score,x[i]\n",
        "            \n",
        "            \n",
        "#when thinking about performace you need to understand computational complexity\n",
        "#for this you would have to check all the vaules less than x[i]\n",
        "#is there a loop, then we are doing this n times,\n",
        "#is there a loop inside the loop, you have to multiply them togeather\n",
        "#is there anything inside the loop that is not a constant time thing, like a sort, which is n log n\n",
        "# how to calculate matrix multipy\n",
        "#watch out for element wise array opperations, doing something to every value in an array,\n",
        " #like checking value against a constant\n",
        "  #take the number of things in the loop, multipy it by the highest computational complexity inside the loop\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "sUeh7vVTNVU0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# the normal way to calculate standard deviation where you \n",
        "#take the difference between value and its mean, square it, \n",
        "#sum up all the observation and devide by n, take the square root of that\n",
        "#is not very efficient becasue you have to calculate x minus the mean lots of times\n",
        "#a very common alternative to variance is  \"mean of squares\" minus \"square of means\"\n",
        "#std div = square root of variance so we can use the squared root of the mean of squares minus square of means\n",
        "# or         square root((sum squares/count)- (sum/count)^2)\n",
        "# or         square root(\"mean of squares\" - \"square of means\") \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#if we sort the items we can then go down the list or array one at a time and all we need to do is \n",
        "#one more item to the left had side and subtract that item from the right had side.\n",
        "#this is called an order n algorithem\n",
        "\n",
        "def std_agg(cnt, s1, s2): return math.sqrt((s2/cnt) - (s1/cnt)**2)\n",
        "\n",
        "def find_better_split_foo(self, var_idx):\n",
        "    x,y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n",
        "    \n",
        "    sort_idx = np.argsort(x) #sort it\n",
        "    sort_y,sort_x = y[sort_idx], x[sort_idx] #sort it\n",
        "    rhs_cnt,rhs_sum,rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
        "    #rhs_cnt -  count things on right, \n",
        "    #rhs_sum -  sum of thing on right,\n",
        "    #rhs_sum2 - sum of squared on right,\n",
        "    #initially everything is on the right\n",
        "    # n is count\n",
        "    #sort_y.sum() is sum\n",
        "    #(sort_y**2).sum() is sum of squares\n",
        "    \n",
        "    lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.   #starts with nothing on left\n",
        "\n",
        "    for i in range(0,self.n-self.min_leaf-1):#loop throught each observation\n",
        "        xi,yi = sort_x[i],sort_y[i]#sort them\n",
        "        lhs_cnt += 1; rhs_cnt -= 1 #add one to left hand side count, subtract one form right had side count\n",
        "        lhs_sum += yi; rhs_sum -= yi #add the value to the Left hand sum, subtract from right had sum\n",
        "        lhs_sum2 += yi**2; rhs_sum2 -= yi**2 #add the value squared to the left hand sum squared and subtract from right\n",
        "        if i<self.min_leaf or xi==sort_x[i+1]:#check if this value and the next one are not the same, if they are skip over it\n",
        "            continue\n",
        "            \n",
        "        lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
        "          #calc std deviation,using left hand count, sum, and sum squared\n",
        "          #math.sqrt((lhs_sum2/lhs_cnt) - (lhs_sum/lhs_cnt)**2)\n",
        "          #square root(sum squares/count)- (sum/count)^2\n",
        "        rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
        "            #calc std deviation,using right hand count, sum, and sum squared\n",
        "          \n",
        "          \n",
        "        curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt#calc weighted average score\n",
        "        if curr_score<self.score: \n",
        "            self.var_idx,self.score,self.split = var_idx,curr_score,xi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "b4gHFqYr3FhV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#runs it ten times to get an aveage and std dev\n",
        "%%timeittimeit  find_better_split (tree,1)\n",
        "tree\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "1fAvi6qg3FTb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "find_better_split(tree,0); tree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "6tXz3DZ308FN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "DecisionTree.find_better_split = find_better_split_foo\n",
        "#left side can be thought of as find_better_split as defined in DecissionTree class\n",
        "#right sid is find_better_split_foo in the global name space\n",
        "#they can have the same same and they dont have to be the same thing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "0xltQxy12YUN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "tree  =  TreeEnsemble (x_sub, y_train, 1, 1000).trees[0]; tree\n",
        "\n",
        "#when we call TreeEnsemble is called create_tree, create_tree instantiated DecisionTree, \n",
        "#DecisionTree called find_better_split(which was defined buy find_better_split_foo),\n",
        "#which went through every column to see if we could find a better split.\n",
        "\n",
        "#https://youtu.be/O5F9vR2CNYI?t=55m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ydPHBQVB7DwO"
      },
      "cell_type": "markdown",
      "source": [
        "Whole tree"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bobXpN_Z7Ngo"
      },
      "cell_type": "markdown",
      "source": [
        "So fare we have made a tree that can make one decision"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "5tErD2RF7M5n",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tree  =  TreeEnsemble (x_sub, y_train, 1, 1000).trees[0]; tree\n",
        "\n",
        "#when we call TreeEnsemble is called create_tree, create_tree instantiated DecisionTree, \n",
        "#DecisionTree called find_better_split(which was defined buy find_better_split_foo),\n",
        "#which went through every column to see if we could find a better split.\n",
        "\n",
        "#https://youtu.be/O5F9vR2CNYI?t=55m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "v88CSZoO6wp-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# take a look at scikit learn with a max depth of 2\n",
        "m = RandomForestRegressor(n_estimators=1, max_depth=2, bootstrap=False)\n",
        "m.fit(x_samp, y_samp)\n",
        "draw_tree(m.estimators_[0], x_samp, precision=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "w4DO-Z61_Bkd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_varsplit(self):\n",
        "    for i in range(self.c): self.find_better_split(i) # loop through c, columns as defined in __jnit__\n",
        "    if self.is_leaf: return  #.is_leaf is defined as self.score ==folat('inf'):, it checks if it is a leaf\n",
        "      #if its not a leaf we need to split into right and left side, earilier we created an array of booleans \n",
        "      #for the rhs and lhs, with would be beter to use indexes because we dont want to have a full list of all\n",
        "      #the boolens in every single node, if you just store the indexes will get smaller and smaller the deeper \n",
        "      #you get into the tree\n",
        "    x = self.split_col\n",
        "    lhs = np.nonzero(x<=self.split)[0]#same as the boolean array, but turns into indexes of the trues.\n",
        "      #lhs = indexes on left hand side\n",
        "    rhs = np.nonzero(x>self.split)[0]\n",
        "      #rhs = indexes on right hand side\n",
        "    self.lhs = DecisionTree(self.x, self.y, self.idxs[lhs])#create decision tree for left\n",
        "    self.rhs = DecisionTree(self.x, self.y, self.idxs[rhs])#create decision tree for right\n",
        "    \n",
        "    \n",
        "#find_varsplit was called because it is called by the DesicionTree constructor, \n",
        "#then find_varsplit itself calls the DecissionTree contstructor, this is called circular recursion,\n",
        "#from my understanding recursion is when a funcion calles it self\n",
        "\n",
        "#an interesting method for thinking about recursion is to just not think about it. write what you need\n",
        "#what do you want and write it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "bJYq2ZC1_Bfn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#you can pass the updated .find_varsplit to the DecisionTree class\n",
        "DecisionTree.find_varsplit = find_varsplit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "llngILg0_BYx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tree = TreeEnsemble(x_sub, y_train, 1, 1000).trees[0]; tree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "XmUyrejC_BTh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test\n",
        "tree.lhs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "KK31il3o_BDM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test\n",
        "tree.rhs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "GfP3qwytGsis",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test\n",
        "tree.lhs.lhs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "NnhQChurGseb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test\n",
        "tree.lhs.rhs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "R1xNKHP6GsOe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PQdiWG1jHSC4"
      },
      "cell_type": "markdown",
      "source": [
        "Predictions"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HP57EDAOHU5j"
      },
      "cell_type": "markdown",
      "source": [
        "we have a tree now we wnat to be able to make predictions. We had something called .perdict in the TreeEnsemble() class but we didnt define .predict in the tree. it didnt generate an error because it didnt get called. This allow us to write with a top down approach, TreeEnsemble.predict() uses the mean of tree predictions, you wil now need to define .predict for the trees."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "qV4juAYCKUaR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cols = ['MachineID', 'YearMade', 'MachineHoursCurrentMeter', 'ProductSize', 'Enclosure',\n",
        "        'Coupler_System', 'saleYear']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "cTEXmJiaKUSt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%time tree = TreeEnsemble(X_train[cols], y_train, 1, 1000).trees[0]\n",
        "x_samp,y_samp = tree.x, tree.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "QllALoUOKUGI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False)\n",
        "m.fit(x_samp, y_samp)\n",
        "draw_tree(m.estimators_[0], x_samp, precision=2, ratio=0.9, size=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "68ZPgTTJKUAZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#self.predict_row(xi) - prediction for row\n",
        "#for xi in x  - for every row\n",
        "\n",
        "#you can do 'BLANK' in array,\"for xi in x\", reguardless of the number of axes in of the array (rank)\n",
        "#numpy loops throught the leading axis.\n",
        "#the leading axis of a vector is a vector itself\n",
        "#the leading axis of a matrix are the rows\n",
        "#the leading asis of a treee dementional tensor is the matrices that represent the slices\n",
        "\n",
        "def predict(self, x): return np.array([self.predict_row(xi) for xi in x])#.predict_row to be defined\n",
        "DecisionTree.predict = predict#pass it to DecisionTree.predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "utJ9C2yUKT7c",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def  predict_row(self, xi):\n",
        "    if self.is_leaf: return self.val#if leaf return value\n",
        "    \n",
        "      #determin if you need to go to lhs or rhs\n",
        "      #xi[self.var_idx  - value for this variable in this row\n",
        "      #self.split   - value we decided to split on \n",
        "      # choose left if value of this variable in this row is less then the split if not go right\n",
        "    t = self.lhs if xi[self.var_idx]<=self.split else self.rhs\n",
        "    return t.predict_row(xi)#do it again and again, untill you get to a leaf \n",
        "\n",
        "DecisionTree.predict_row = predict_row\n",
        "\n",
        "#this uses a ternary operator this if ______, else this other thing\n",
        "#it is used as an operator that returns a value,\n",
        "\n",
        "#normally if/else statements are used as control flow statements, telling python to go down this path \n",
        "#or that path to do a something:\n",
        "\n",
        "#  if  something:\n",
        "#      x= do1()\n",
        "#  else:\n",
        "#      x= do2()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "xFfb2IGxKToP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AGdj8vE5J4Hs"
      },
      "cell_type": "markdown",
      "source": [
        "this regression model could create a classification model with minor changes, it would be the same code for binary classification, for multi class classification you would need to changing data structre so that you have a one hot encoded matrix or a list of integers that you treat as a one hot encoded matrix\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-GgNpnCUTOfY"
      },
      "cell_type": "markdown",
      "source": [
        "###Cython"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZEkBLZXCTscf"
      },
      "cell_type": "markdown",
      "source": [
        "Load cython, use cdef package to define data types"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "GxQpmeb2TSGD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%load_ext Cython"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "x78qbWalTUWf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fib1(n):\n",
        "    a, b = 0, 1\n",
        "    while b < n:\n",
        "        a, b = b, a + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "5_WlcYi-TUO6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%cython\n",
        "def fib2(n):\n",
        "    a, b = 0, 1\n",
        "    while b < n:\n",
        "        a, b = b, a + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "EvUWV5kxTUJO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%cython\n",
        "def fib3(int n):\n",
        "    cdef int b = 1\n",
        "    cdef int a = 0\n",
        "    cdef int t = 0\n",
        "    while b < n:\n",
        "        t = a\n",
        "        a = b\n",
        "        b = a + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "Rmu6jB6sTktG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%timeit fib1(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "hlSpikXcTlXA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%timeit fib2(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "75t5Fpy5TknJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%timeit fib3(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EbrDuqZDnO9Y"
      },
      "cell_type": "markdown",
      "source": [
        "###Recap"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9WB5y10unXNq"
      },
      "cell_type": "markdown",
      "source": [
        "https://medium.com/usf-msds/intuitive-interpretation-of-random-forest-2238687cae45"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YFvTBjUrXcO6"
      },
      "cell_type": "markdown",
      "source": [
        "##getting started in deep learning"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "doouxWGsXquV"
      },
      "cell_type": "markdown",
      "source": [
        "there are a lots of limiation of tree based models, they cant extrapolate well to unseen data, they can only do log base 2 n decissions, there is a limited amout of computation it can do. there is a limited complexity of the model it can learn."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "W0JjpqzCbR3P"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "qiSguhAzbTZ6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "from fastai.imports import *\n",
        "from fastai.torch_imports import *\n",
        "from fastai.io import *\n",
        "\n",
        "path = 'data/mnist/'\n",
        "Let's download, unzip, and format the data.\n",
        "\n",
        "\n",
        "import os\n",
        "os.makedirs(path, exist_ok=True)\n",
        "\n",
        "URL='http://deeplearning.net/data/mnist/'\n",
        "FILENAME='mnist.pkl.gz'\n",
        "\n",
        "def load_mnist(filename):\n",
        "    return pickle.load(gzip.open(filename, 'rb'), encoding='latin-1')\n",
        "  #encoding='latin-1' tell pickle that it is using this specific python2 character set\n",
        "\n",
        "  \n",
        "#destructuring, \n",
        "load_mnist is giving us a tuple of tuples,\n",
        "#we can feed all these things in \n",
        "#(x, y) is a tuple of traing data, \n",
        "#(x_valid, y_valid) is a tupal of validation data, \n",
        "#_ is a tupal of test data. it is assigned to _, which generally means we are going to trow it away\n",
        "\n",
        "get_data(URL+FILENAME, path+FILENAME)\n",
        "((x, y), (x_valid, y_valid), _) = load_mnist(path+FILENAME)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "CFMh35minljz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#what do i have and how much\n",
        "type(x), x.shape, type(y), y.shape\n",
        "\n",
        "#these images were flatened out, they are 28 x 28\n",
        "\n",
        "#the person that saved these files made a rank two tensor into a rank 1 tensor\n",
        "\n",
        "#rows are dimention 0\n",
        "#columns are dimention 1\n",
        "#unless you are an image person then columns are axis 0, and row are axis 1\n",
        "#you would have to remember this if you are using the python library.\n",
        "\n",
        "#very often things have different names depending on what background you come from\n",
        "\n",
        "#negative log likelihood is binomial or categorical creoss entropy\n",
        "#one hot encoding is also called dummy variables\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "peUzZDV-e6Yc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Normalize\n",
        "#Many machine learning algorithms behave better when the data is normalized, \n",
        "#that is when the mean is 0 and the standard deviation is 1. \n",
        "#We will subtract off the mean and standard deviation from our training set in order to normalize the data:\n",
        "\n",
        "\n",
        "#generally tree based models generally dont care about value they care about sort, is 'this' more than 'this'\n",
        "# this is also true in different metrix too, like roc curve, and dendrogram spearmans correlation \n",
        "\n",
        "\n",
        "mean = x.mean()\n",
        "std = x.std()\n",
        "\n",
        "x=(x-mean)/std #Normalize, subtract the meand devide by standard deviation, give you\n",
        "#mean of 0 and a standared deviation of 1\n",
        "mean, std, x.mean(), x.std()\n",
        "\n",
        "#keep in mind that you need to make the same adjustment to the validation and test set\n",
        "#we use the mean and standard dev. of the training data to make these normalizing adjustments\n",
        "\n",
        "\n",
        "#test it on the validation set to see if it is pretty close to (0,1)\n",
        "x_valid = (x_valid-mean)/std\n",
        "x_valid.mean(), x_valid.std()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "MlKxE4ihfdhl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Look at the data\n",
        "#In any sort of data science work, it's important to look at your data, \n",
        "#to make sure you understand the format, how it's stored, what type of values it holds, etc. \n",
        "#To make it easier to work with, let's reshape it into 2d images from the flattened 1d format.\n",
        "\n",
        "\n",
        "def show(img, title=None):\n",
        "    plt.imshow(img, cmap=\"gray\")\n",
        "    if title is not None: plt.title(title)\n",
        "\n",
        "def plots(ims, figsize=(12,6), rows=2, titles=None):\n",
        "    f = plt.figure(figsize=figsize)\n",
        "    cols = len(ims)//rows\n",
        "    for i in range(len(ims)):\n",
        "        sp = f.add_subplot(rows, cols, i+1)\n",
        "        sp.axis('Off')\n",
        "        if titles is not None: sp.set_title(titles[i], fontsize=16)\n",
        "        plt.imshow(ims[i], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nx4Ntmsru-q6"
      },
      "cell_type": "markdown",
      "source": [
        "min 22 of lesson 8"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "RJrIsDpwuNU-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_valid.shape #you have 10000 images as a rank one tensor of length 784"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "z8pVVllZuNO9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#np.reshape takes a tensor in (x_vlid), and reshapes it to whetever size tensor\n",
        "#in this case it is  (-1,28,28)\n",
        "# if there are \"d\" number of axes you only need to tell it about \"d\"-1, axes, the \n",
        "#other one it can figure out by itself.\n",
        "\n",
        "#generally the fist axis is the number of items you have, 10000 in this example\n",
        "# if you tell it you whan the last to be 28 by 28 it will figure out 10000, so\n",
        "#putting -1 tells it to make it as big or as small as you have to to make it fit\n",
        "#you want to get in the habbit of doing this becasue if you resample, or use stratified \n",
        "#sample(if unbalanced)\n",
        "#it will make it more resilient and less suseptable to break if you make changes\n",
        "\n",
        "x_imgs = np.reshape(x_valid, (-1,28,28)); x_imgs.shape\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ogF2Hq5D0Qdf"
      },
      "cell_type": "markdown",
      "source": [
        "####OpenCV (move)\n",
        "\n",
        "orders channels blue green red, everyone else expects red green blue, so you will need to reverse the last axis"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bY3qExZQ1Zpt"
      },
      "cell_type": "markdown",
      "source": [
        "####python Image library (move)\n",
        "\n",
        "orders by row by columns by channels, pytorch expects channels by rows by columns"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "paxuCUgGuNJB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show(x_imgs[0], y_valid[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "Bkg5vsCxuM55",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#34.26 lesson 8\n",
        "y_valid.shape\n",
        "#rank one tensors are tuples and python puts a comma after it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "JoafeqDNuMzi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_valid[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "2wuY9SEcuhjD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#slicing into a tensor\n",
        "#34.45 lesson 8\n",
        "\n",
        "#we are slicing into the 1st axis (0)\n",
        "#grabig 10-14 inclusive rows, and\n",
        "#10-14 columns\n",
        "\n",
        "x_imgs[0,10:15,10:15]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "uA2dh7ai_sWk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#3d densor is represented as a list of 2d tensors\n",
        "x_imgs[0:2,10:15,10:15]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "ilc3H7uAAS2U",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#jagged array\n",
        "#where they dont have the same length\n",
        "\n",
        "if you wanted to get an item out of a jagged array you would generally write it like\n",
        "\n",
        "a[1][3] \n",
        "\n",
        "# in the a object (a) go into the second list item, and get the 3rd item out of it. \n",
        "\n",
        "[4, 5, 6]\n",
        "[5, 6, 7, 8, 9] #this is list 1,item 3 is 8, remember python starts at [0]\n",
        "[1]\n",
        "[1, 2, 3, 4, 5, 6, 7, 8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "ulLKtDP4uhdP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show(x_imgs[0,10:15,10:15])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "an4zC566uhXT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plots(x_imgs[:8], titles=y_valid[:8])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kb7iZLQ5uvL9"
      },
      "cell_type": "markdown",
      "source": [
        "###PyTorch"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "n54BUCLhuhR6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from fastai.metrics import *\n",
        "from fastai.model import *\n",
        "from fastai.dataset import *\n",
        "\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "e8_qzvVSuhLt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# nn.Sequential()    -  uses a list of the layers and runs in order\n",
        "# nn.Linear()     -  linear layer,esentally going to do y=ax+b (matrix multply,not univariate)\n",
        "#                   (imput of the matrix product(pixels we have) , output size)\n",
        "# LogSoftmax()    -  softmax layer, we get e^ for each output and we take that number and devide \n",
        "#                   by the sum, see softmax for info\n",
        "# .cuda()        -  put it on GPU, default is to run on cpu\n",
        "\n",
        "net = nn.Sequential(  \n",
        "    nn.Linear(28*28, 10), \n",
        "    nn.LogSoftmax()\n",
        ").cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "YbDkROprux7Q",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#adding a hidden layer\n",
        "\n",
        "net = nn.Sequential( \n",
        "    nn.Linear(28*28, 100), \n",
        "    nn.Linear(100, 10),\n",
        "    nn.LogSoftmax()\n",
        ").cuda() \n",
        "\n",
        "# the combination of two Linear layers is the same as one\n",
        "#  i need to find a good example that show how multiple linear layers are the same\n",
        "#  as one linear layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "KWua8uiLsAwF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#add non-linear(transformation/activation functions)\n",
        "  #sometimes called an nonlinearity\n",
        "\n",
        "# activation function\n",
        "#   an activation is the value that is calculated in a layer\n",
        "\n",
        "#nn.ReLU()  - rectified linear unit, take all the zeros and turns them to zero\n",
        "\n",
        "net = nn.Sequential( \n",
        "    nn.Linear(28*28, 100), \n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 10),\n",
        "    nn.LogSoftmax()\n",
        ").cuda() \n",
        "#this is a two hidden layer network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "ijRN4ZoauhET",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#creating a model data object \n",
        "  #it wraps up training, validaiton and test data(optional)\n",
        "  #\n",
        "  # ImageClassifierData =  is the classifer\n",
        "  #      .from_arrays   =  how data is formated, alternatives:'.from_csv','.from_dateframe' , etc\n",
        "  #          path       =  where data is located\n",
        "  #         (x,y)       =  training data\n",
        "  #(x_valid, y_valid)   =  validation data\n",
        "\n",
        "md = ImageClassifierData.from_arrays(path, (x,y), (x_valid, y_valid))\n",
        "\n",
        "#ImageClassifierData.from_arrays, is a pytorch data loader\n",
        "#then you define loss what loss function, metrics, and optimaizer you want to use,\n",
        "#then call fit on it\n",
        "\n",
        "#define loss, metrics, and optimizer\n",
        "loss    =  nn.NLLLoss()\n",
        "metrics =  [accuracy]\n",
        "opt     =  optim.SGD(net.parameters(), 1e-1, momentum=0.9, weight_decay=1e-3) \n",
        "   \n",
        "#fit\n",
        "  #  net      =  the neural network, sometimes call arcitecture(we defined above)\n",
        "  #  n_epochs =  is the number of epochs(times you want to go through the data)\n",
        "  #  crit     =  loss, is the loss fucntion but in pytorch it is called the criterian\n",
        "  #  opt      =  optimizer, we are useing SGD\n",
        "  #  metrics  =  metrics being used\n",
        "\n",
        "fit(net, md, n_epochs=5, crit=loss, opt=opt, metrics=metrics)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YK-31hUDNxdO"
      },
      "cell_type": "markdown",
      "source": [
        "###Loss functions and metrics\n",
        "In machine learning the loss function or cost function is representing the price paid for inaccuracy of predictions.\n",
        "\n",
        "The loss associated with one example in binary classification is given by: -(y * log(p) + (1-y) * log (1-p)) where y is the true label of x and p is the probability predicted by our model that the label is 1."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "q8CItojTO1ET",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def binary_loss(y, p):\n",
        "    return np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))\n",
        "  \n",
        "  #since this i binary 0 or 1, you an replace this with an if statement because \n",
        "  #0 * log(something) is zero\n",
        "  \n",
        "  \n",
        "#catigorical cross entropy /catagorical negative log liklyhood is the same as binary \n",
        "#but you add it up across all of the categories, for example it would be a binary function \n",
        "#for each of the 10 number digets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "B6hymorXO9X9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Example of how Negitive log likelyhood\n",
        "\n",
        "acts = np.array([1, 0, 0, 1]) #1 is cat, 0 is dog\n",
        "preds = np.array([0.9, 0.1, 0.2, 0.8])#predictions (90% cat/10% dog, 10% cat/90% dog, ...)\n",
        "binary_loss(acts, preds)\n",
        "\n",
        "#so for the first one it woudld be calculated as:\n",
        "#  -(1* log(.9)+0*log(.1))\n",
        "# etc."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "KCUtwMlCug9u",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss=nn.NLLLoss() #this is negative logliklyhood, aka cross entropy, \n",
        "#there are two types binary or categorical \n",
        "metrics=[accuracy]#what metric wie are using \n",
        "# opt=optim.SGD(net.parameters(), 1e-1, momentum=0.9)\n",
        "opt=optim.SGD(net.parameters(), 1e-1, momentum=0.9, weight_decay=1e-3) #optimization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "M9oGVszUWtsr"
      },
      "cell_type": "markdown",
      "source": [
        "###Predictions"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "zUQpxJr2XgKd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "preds = predict(net, md.val_dl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "QCKmSY0hXgjq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds.shape #this is (10000, 10) (rows, 1 predictions for each category)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "huE2e1FYXgEK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#np.argmax\n",
        "\n",
        "#argmax takes an array(preds), and figures out on this axis(1 is columns), \n",
        "#(0-9 in this example), and finds which one is higher and return the index\n",
        "preds.argmax(axis=1)[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "B7sflccHXf9R",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds = preds.argmax(1);preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "v-lm-tnHYckn"
      },
      "cell_type": "markdown",
      "source": [
        "Let's check how accurate this approach is on our validation set. You may want to compare this against other implementations of logistic regression, such as the one in sklearn. In our testing, this simple pytorch version is faster and more accurate for this problem!"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "FAzXvUQ8Xf3S",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#recreate accuracy\n",
        "\n",
        "  #(preds == y_valid) is an array of booleans, so the mean is the % correct\n",
        "\n",
        "\n",
        "np.mean(preds == y_valid) #when does preds equal the ground truth (correct answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "qYbHGzcBXfwG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Let's see how some of our predictions look!\n",
        "\n",
        "\n",
        "plots(x_imgs[:8], titles=preds[:8])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hW6Z7Hh6dhOD"
      },
      "cell_type": "markdown",
      "source": [
        "###Defining Logistic Regression"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "M6hSALh2Wtnc"
      },
      "cell_type": "markdown",
      "source": [
        "####Making nn.linear from scratch\n",
        "\n",
        "1hour19 lesson 8"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "c8M_QsipPzuN"
      },
      "cell_type": "markdown",
      "source": [
        "Above, we used pytorch's nn.Linear to create a linear layer. This is defined by a matrix multiplication and then an addition (these are also called affine transformations). Let's try defining this ourselves.\n",
        "\n",
        "Just as Numpy has np.matmul for matrix multiplication (in Python 3, this is equivalent to the @ operator), PyTorch has torch.matmul.\n",
        "\n",
        "Our PyTorch class needs two things: constructor (says what the parameters are) and a forward method (how to calculate a prediction using those parameters) The method forward describes how the neural net converts inputs to outputs.\n",
        "\n",
        "In PyTorch, the optimizer knows to try to optimize any attribute of type Parameter."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "8e1RLGsXXfpv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#you wouldnt normally do this but this is how you create a nn from scrach, almost scratch\n",
        "\n",
        "def get_weights(*dims): return nn.Parameter(torch.randn(dims)/dims[0])\n",
        "\n",
        "#  nn.Parameter\n",
        "    #  is a flag that we use to tell pytorch that this is a parameter, this is something \n",
        "    #  that we are trying to optimize, it is exactly the same a a regular pytorch variable\n",
        "    #\n",
        "\n",
        "# .randn()\n",
        "    #  .randn is a tensor filled with random numbers for a normal distriution\n",
        "  \n",
        "#  torch.randn(dims)/dims[0]\n",
        "    #  random numbers/rows is matix\n",
        "    #  keeps scale at mean 0 standard diviation 1\n",
        "    \n",
        "#  Why\n",
        "    #  when you are adding layers you have to be cardful that you arent adding layer\n",
        "    #  that are miking the output to big or too small, that is why pick and approate\n",
        "    #  weight matix so the output stays about the same size. \n",
        "    #  if you use a normally distributed random numbers(randn(dims)) and devide by number \n",
        "    #  of rows in weight matrix(dims[0]), it keeps them at about the right scale, this is \n",
        "    #  based on the idea in linear algebra, called climing initialization, which is if \n",
        "    #  the first eiganvalue is bigger than 1 it will cause the gradients to get bigger, \n",
        "    #  less than one smaller, this is called gradient explosion\n",
        "    #  main take away is that if you you this as a way to generate random numbers, \n",
        "    #  if you start with data with mean 0 and sd=1, your output will be around mean 0, \n",
        "    #  sd=1\n",
        "    \n",
        "    \n",
        "def softmax(x): return torch.exp(x)/(torch.exp(x).sum(dim=1)[:,None])\n",
        "\n",
        "\n",
        "#                           Create LogReg \n",
        "\n",
        "#defined a pytorch module\n",
        "\n",
        "class LogReg(nn.Module):  #this is inheritance, if you put something in ()\n",
        "                          #it get all of the functionality from that superclass\n",
        "                          #in this example we are getting all the functionality of \n",
        "                          #the nn.Module and then add functions to it. \n",
        "                          # you need to initialize the superclass\n",
        "          \n",
        "    def __init__(self):\n",
        "        super().__init__()#Initialize superclass,because you are using inheritance\n",
        "        self.l1_w = get_weights(28*28, 10)  # Layer 1 weights matix, to be used to\n",
        "                                            # multiply our data by, it is going to have\n",
        "                                            # 28x28 rows and 10 columns, because if we have \n",
        "                                            # and image which we flattened out into a 28 by\n",
        "                                            # 28 length vector, then we can multiply by this \n",
        "                                            # weight matrix to get back out a lenght 10 \n",
        "                                            # vector, which we can use as our set of \n",
        "                                            # predictions, we dont want yx, we want yx + b\n",
        "                                            # the b is the bias\n",
        "                        \n",
        "        self.l1_b = get_weights(10)         # Layer 1 bias, we need a bias for each output\n",
        "                                            #so we put in 10, it gets added to every row\n",
        "\n",
        "           \n",
        "#                            Picture it?          \n",
        "          \n",
        "#           data           weights     bias\n",
        "#\n",
        "#           28 x 28         10          10  \n",
        "#       [xxxxxxxxxxxxx]   [WWWW]     [BBBBBBB]\n",
        "#       [xxxxxxxxxxxxx]   [WWWW]\n",
        "#       [xxxxxxxxxxxxx]   [WWWW]\n",
        "# 10000 [xxxxxxxxxxxxx] x [WWWW]   +\n",
        "#       [xxxxxxxxxxxxx]   [WWWW] \n",
        "#       [xxxxxxxxxxxxx]   [WWWW]\n",
        "#       [xxxxxxxxxxxxx]   [wwww]\n",
        "#\n",
        " \n",
        "  \n",
        "  \n",
        "#                           Defining forward\n",
        "\n",
        "#  Forward has a special meaning in pytorch, that gets called every time your layer is \n",
        "#  calculated, if you are creating a layer or a neural net you need to define forward\n",
        "#  it is going to get passesd the information from the previous layer, (x),\n",
        "#  our definition is to do a matrix multiplicationn, take imput data * weights + biases\n",
        "  \n",
        "  def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)   #  .view is how you flatten in pytorch\n",
        "      \n",
        "            #  reshape 28 row by 28 column matrix to 28x28 vector this is the equlivalant\n",
        "            #  of .reshape, x.size(0), dont change the number of images \n",
        "            #  -1 mean as long as required, we are going to replace row by column \n",
        "            #  with a single axis\n",
        "                \n",
        "        x = (x @ self.l1_w) + self.l1_b   #   Linear Layer\n",
        "        #   x = torch.matmul(x, self.l1_w) + self.l1_b (alternative)\n",
        "      \n",
        "            # (x @ self.l1_w) =    input data(x) times weights(self.l1_w)\n",
        "            #  + self.l1_b    =    plus biases\n",
        "\n",
        "        x = torch.log(softmax(x)) # Non-linear (LogSoftmax) Layer\n",
        "        \n",
        "                #  output of softmax are uses as probabilities\n",
        "                #  we go e^ for each output and we take that number and devide by the sum\n",
        "                #  generally speaking this help us several way, it makes the sum = 1, it\n",
        "                #  makes sligtly bigger values in input,into much bigger values in output\n",
        "                #  so most of the time you will see one big number and a buch of small numbers\n",
        "                #  pytorch use the log of softmax, for mainly numerical stability\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "Ehigp32jXfkR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#same code but without notes\n",
        "\n",
        "def get_weights(*dims): return nn.Parameter(torch.randn(dims)/dims[0])\n",
        "def softmax(x): return torch.exp(x)/(torch.exp(x).sum(dim=1)[:,None])\n",
        "\n",
        "class LogReg(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1_w = get_weights(28*28, 10)  # Layer 1 weights\n",
        "        self.l1_b = get_weights(10)         # Layer 1 bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = (x @ self.l1_w) + self.l1_b  # Linear Layer\n",
        "        x = torch.log(softmax(x)) # Non-linear (LogSoftmax) Layer\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "MBY9-1KKP0A-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create a neural net(net2) and the optimizer(opt).\n",
        "  #(loss and metrics from above).\n",
        "\n",
        "net2 = LogReg().cuda() # instantiate and object of the LogReg() class, .cuda copy it to GPU\n",
        "opt=optim.Adam(net2.parameters()) #define optimizer \n",
        "  # use Adam optimiser, we called the .parameters function on class, but we never defined it\n",
        "  #but it was definded for us in nn.modual that we inherited, nn.Module atomaticly goes through\n",
        "  # the atributes that we created, \n",
        "  #(self.l1_w = get_weights(28*28, 10) , self.l1_b = get_weights(10) ), and uses the nn.Parameter\n",
        "  # that we defined in get_weights, as what we are optimizing, which is the weight matrix\n",
        "  # so when we defiened it we wraped it in nn.Parameter as a way of telling pytorch that \n",
        "  # this(weight matrix) is someting that we want to optimize. since we defined that nn.Parameter \n",
        "  # it goes throught everyting in the constructor(class LogReg()), and checks to see which are of \n",
        "  # type parameter and if so it set them to things that we are going to train with optimizer\n",
        "  \n",
        "  \n",
        "  #  another way of saying that is the when we are defining the optimizer \n",
        "  #  (optim.Adam(net2.parameters()), we are saying use Adam optimizer on this \n",
        "  #  net(net2=LogReg, which has weights and bias Parameters that need to be optimized)\n",
        "  #  pytorch know this becasue each contain .Parameter in their get_weights\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "bxi7wbsD8MUE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "u0pZBytLyoWK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "S88vu8qiP0RX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fit(net2, md, n_epochs=1, crit=loss, opt=opt, metrics=metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "qoKmyuwUP0bQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#md.trn_dl = train data loader\n",
        "#use iter to take that dataloader something that we can iterate through\n",
        "\n",
        "dl = iter(md.trn_dl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "1tdjH5q-P0oH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#min 31 lesson 9\n",
        "\n",
        "# us the standard next to grab the next item from that generator\n",
        "# xmb, grab the x's from mini batch\n",
        "# ymb, grab the y's from mini batch\n",
        "\n",
        "xmb,ymb = next(dl)\n",
        "\n",
        "#   alternatively you could use the generator in a for loop,\n",
        "#   most of the time it is using next in the backgroud\n",
        "\n",
        "#      for xmb, ymb in dl:\n",
        "#          ...\n",
        "# this is described as basically syntactic sugar for calling next, lots of times"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "cPGZRHvZFcKV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#x mini batch\n",
        "\n",
        "xmb\n",
        "#returns a tensor 64 X 7841 \n",
        "#fast ai picks mini batch sizes of 64\n",
        "# -0445 are the zero values but they are not zero since we normalized(- mean, / std.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "1sge2DBkRMH_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#wraping it in Variable, so we can pass it to the logistic regression\n",
        "\n",
        "#this is how we get automatic differentiation\n",
        "# pytorch can differentiate any tensor but it takes memory and time\n",
        "# to do automatic differentiation it need to keep track how it was calculated\n",
        "#  (we add thses things together, we mltiplied by that, we then took the sine, etc )\n",
        "# we need to track the steps becuase to do automatic differentation it has to take\n",
        "# the derivative of each step using the chain rule, multiply them all togeather\n",
        "# which is slow and time intensive, so we wrape it in variable so pytorch know to keep\n",
        "# track of the steps\n",
        "\n",
        "\n",
        "\n",
        "vxmb = Variable(xmb.cuda())\n",
        "#we use .cuda() to put it on the gpu because our net2 is on the gpu\n",
        "vxmb\n",
        "\n",
        "#looks exactly like a tensor except it says \"Variable containg:\"\n",
        "#anything you can do to a tensor you can do to a variable, but it is going to \n",
        "#keep track of what we did so we can take the derivative "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "yWJtNlciRMds",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pass the variable to the net\n",
        "\n",
        "#  you can treat the net(net2) as a function\n",
        "#  note how we are not using forward, \n",
        "# preds = net2.forward(vxmb).exp() would work but thats is not how pytorch works\n",
        "#  this is an important destiction because when we are defining our own architectures \n",
        "#  anywhere where you would put in a function you can put in a layer anywhere you can\n",
        "#  put in a layer you can put in a neural net, any where you put a neural net you can\n",
        "#  put in a function, pytorch veiw them all as things that are called, as if they are\n",
        "#  functions. this is important becuse this interchangeability you can creted some \n",
        "#  complex neural nets\n",
        "\n",
        "#  use .exp is to undo taking the log in softmax layer in the forward\n",
        "#  this will give us the probabilities\n",
        " \n",
        "preds = net2(vxmb).exp(); preds[:3] #grab the first three of them, for readability\n",
        "# note most of the predictions are close to zero, and some are much bigger,\n",
        "# this is what we would hope\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "IW5TKwIwRMn5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#to turn the predicted probabilities into a predicted diget we use argmax(.max in pytorch)\n",
        "#remember argmax takes an array(preds), and figures out on this axis(1 is columns), \n",
        "#(0-9 in this example), and finds which one is higher and return the index\n",
        "\n",
        "\n",
        "# pytorch\n",
        "#(1)= axis\n",
        "#[1]= get the indexs\n",
        "#.max returns two things, the actual max across the (1=columns)axis, and the index\n",
        "\n",
        "#preds_ = preds.data.max(1); preds_\n",
        "#preds_ = preds.data.max(1)[0]; preds_\n",
        "\n",
        "preds = preds.data.max(1)[1]; preds\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "arN876qxRMWt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#look at first 8 images\n",
        "\n",
        "preds = predict(net2, md.val_dl).argmax(1)\n",
        "plots(x_imgs[:8], titles=preds[:8])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "eq-wAUbQRgjR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.mean(preds == y_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1weAj8l09hFn"
      },
      "cell_type": "markdown",
      "source": [
        "###Broadcasting and Matrix Multiplication"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "FsivyPwj91Dx"
      },
      "cell_type": "markdown",
      "source": [
        "Digging into \"torch.matmul:\"  matrix multiplication.\n",
        "\n",
        ">Broadcasting and element-wise operations are supported in the same way by both numpy and pytorch.\n",
        "\n",
        ">Operators (+,-,*,/,>,<,==) are usually element-wise."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "6PHtJYIx9ei1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = np.array([10, 6, -4])\n",
        "b = np.array([2, 8, 7])\n",
        "\n",
        "\n",
        "a + b #this is called element wise, it will return array([12,14,3])\n",
        "\n",
        "same is true for pytorch, in fast ai you would just put a T\n",
        "\n",
        "a = T([10, 6, -4])\n",
        "b = T([2, 8, 7])\n",
        "\n",
        "when using a for loop even in pytorch it will still do it in pyton which is something like 1000 - 10000 times slowe than in C. \n",
        "\n",
        "the main reason you want optimized in c is that it takes advantage of what your CPUs do which is a thing called Simd, single instruction multiple data\n",
        "\n",
        "a cpu can take 8 thing in a vector and adding them up to another vector with 8 things in it in a single cpu instruction\n",
        "\n",
        "and you also have multiple cores, each doing 8 times faster, and doing it in c\n",
        "\n",
        "if you do it in pytorch on the cpu, it can do about 10000 things at a time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "t5kUMHJ99ee6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "nNKpetDg9eSC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(a < b).mean()#is percentage a<b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qhE06Oz_-iib"
      },
      "cell_type": "markdown",
      "source": [
        "###Broadcasting\n",
        "The term **broadcasting** describes how arrays with different shapes are treated during arithmetic operations. The term broadcasting was first used by Numpy, although is now used in other libraries such as Tensorflow and Matlab; the rules can vary by library.\n",
        "\n",
        "From the Numpy Documentation:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "The term broadcasting describes how numpy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C\n",
        "instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations.\n",
        "```\n",
        "\n",
        "In addition to the efficiency of broadcasting, it allows developers to write less code, which typically leads to fewer errors.\n",
        "\n",
        "This section was adapted from Chapter 4 of the fast.ai Computational Linear Algebra course."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "vyvTSpf4-hJq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "J7q4gR_6-fVm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "En8pK-T9_WNF"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "How are we able to do a > 0? 0 is being broadcast to have the same dimensions as a.\n",
        "\n",
        "Remember above when we normalized our dataset by subtracting the mean (a scalar) from the entire data set (a matrix) and dividing by the standard deviation (another scalar)? We were using broadcasting!\n",
        "\n",
        "Other examples of broadcasting with a scalar:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "o71NvInz9eOX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#add scalar\n",
        "a  +  1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "tZAqfODf9eKs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#make a matrix\n",
        "m = np.array([[1, 2, 3], [4,5,6], [7,8,9]]); m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "rNanrlxL9eHf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#broadcast\n",
        "#2 will become\n",
        "#     [2,2,2]\n",
        "#     [2,2,2]\n",
        "#     [2,2,2]\n",
        "\n",
        "2*m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "B19gxg9y_nOF"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Broadcasting a vector to a matrix\n",
        "We can also broadcast a vector to a matri"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "CTzqU1_p9eD5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#rank 1 tensor\n",
        "c = np.array([10,20,30]); c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "J-LnyMGm9d_K",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#add rank 2 tensor to rank 1 tensor\n",
        "m + c\n",
        "\n",
        "#     m         c   \n",
        "  [1,2,3]   [10,20,30]\n",
        "  [4,5,6]\n",
        "  [7,8,9]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "bqqvDPtC9d7h",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "c + m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Nk2Mgiu4_0e2"
      },
      "cell_type": "markdown",
      "source": [
        "Although numpy does this automatically, you can also use the broadcast_to method:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "vLCH-V8h9d35",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "c.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "MLhrTiLo9dvD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.broadcast_to(c[:,None], m.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "2tnQdoUS9dpr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.broadcast_to(np.expand_dims(c,0), (3,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "0HL1AbzG_9rO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "c.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "Ar8IfeLY_-S8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.expand_dims(c,0).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tplWHFGuAHTb"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "The numpy expand_dims method lets us convert the 1-dimensional array c into a 2-dimensional array (although one of those dimensions has value 1)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "cy9Ir-Y0_-N-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.expand_dims(c,0).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "fgOwnzU0_-J3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m + np.expand_dims(c,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "UGXHSUlF_-GA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.expand_dims(c,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "gPDRE3fu_98F",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "c[:, None].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "WXsmSlq6ATNe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "m + np.expand_dims(c,1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "igKALoP9ATju",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.broadcast_to(np.expand_dims(c,1), (3,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5SCH-qhVAe_I"
      },
      "cell_type": "markdown",
      "source": [
        "Broadcasting Rules"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "lGjWoFM3ATZz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "c[None]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "0_IW2QCjATVn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "c[:,None]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "PZ0ROaMc_94O",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "c[None] > c[:,None]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "-sMFVuaRAoRV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xg,yg = np.ogrid[0:5, 0:5]; xg,yg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "q6qEp9EOAoMe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xg+yg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "W6frcV4cAxr1"
      },
      "cell_type": "markdown",
      "source": [
        "when operating on two arrays, Numpy/PyTorch compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when\n",
        "\n",
        "they are equal, or\n",
        "one of them is 1\n",
        "Arrays do not need to have the same number of dimensions. For example, if you have a 256*256*3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\n",
        "\n",
        "Image  (3d array): 256 x 256 x 3\n",
        "Scale  (1d array):             3\n",
        "Result (3d array): 256 x 256 x 3\n",
        "\n",
        "The numpy documentation includes several examples of what dimensions can and can not be broadcast together."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MUIeya8LA1ZQ"
      },
      "cell_type": "markdown",
      "source": [
        "###Matrix Multiplication\n",
        "We are going to use broadcasting to define matrix multiplication."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "ET8q4MQ9AoBi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m, c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "IUqs9f_6An9L",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m @ c  # np.matmul(m, c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "QZpby6mRAn46",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "T(m) @ T(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "fVYEB_0dAn0o",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "The following is NOT matrix multiplication. What is it?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "GzVfiWuMCL0b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "m,c\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "kFIyA2Z-COhK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "m * c\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "KtyQvEN7CPXU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "(m * c).sum(axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "7anLu2tLCQQr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "c\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "kcijNuW7CRQe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "np.broadcast_to(c, (3,3))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "ZKRe38uUCSFP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "From a machine learning perspective, matrix multiplication is a way of creating features by saying how much we want to weight each input column. Different features are different weighted averages of the input columns.\n",
        "\n",
        "The website [matrixmultiplication.xyz](http://matrixmultiplication.xyz/) provides a nice visualization of matrix multiplcation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "H61x3B-NAnvL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n = np.array([[10,40],[20,0],[30,-5]]); n\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "jwQpAGazCGJA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "m\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "nmZS5rZXCHWV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "m @ n\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "T1uKdX4cCInG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "(m * n[:,0]).sum(axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "dqf5tekWCJqw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "(m * n[:,1]).sum(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WxTUYA1KCWbz"
      },
      "cell_type": "markdown",
      "source": [
        "Writing Our Own Training Loop¶\n",
        "As a reminder, this is what we did above to write our own logistic regression class (as a pytorch neural net):"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "cg_6LuB4AnrD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Our code from above\n",
        "class LogReg(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1_w = get_weights(28*28, 10)  # Layer 1 weights\n",
        "        self.l1_b = get_weights(10)         # Layer 1 bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = x @ self.l1_w + self.l1_b \n",
        "        return torch.log(softmax(x))\n",
        "\n",
        "net2 = LogReg().cuda()\n",
        "opt=optim.Adam(net2.parameters())\n",
        "\n",
        "fit(net2, md, n_epochs=1, crit=loss, opt=opt, metrics=metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-c4OHMAJCjjD"
      },
      "cell_type": "markdown",
      "source": [
        "Above, we are using the fastai method fit to train our model. Now we will try writing the training loop ourselves.\n",
        "\n",
        "Review question: What does it mean to train a model?\n",
        "\n",
        "We will use the LogReg class we created, as well as the same loss function, learning rate, and optimizer as before:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "EyT3XAC_Ancq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net2 = LogReg().cuda()\n",
        "loss=nn.NLLLoss()\n",
        "learning_rate = 1e-3\n",
        "optimizer=optim.Adam(net2.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "ytZWinBMCuts",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "md is the ImageClassifierData object we created above. We want an iterable version of our training data (question: what does it mean for something to be iterable?):\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "QJKlsKHuC1O2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "dl = iter(md.trn_dl) # Data loader\n",
        "First, we will do a forward pass, which means computing the predicted y by passing x to the model.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "MbY5h7neC4M1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "xt, yt = next(dl)\n",
        "y_pred = net2(Variable(xt).cuda())\n",
        "We can check the loss:\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "tW2CuG4lC6p-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "l = loss(y_pred, Variable(yt).cuda())\n",
        "print(l)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "yQ3pGgkKC9-9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "We may also be interested in the accuracy. We don't expect our first predictions to be very good, because the weights of our network were initialized to random values. Our goal is to see the loss decrease (and the accuracy increase) as we train the network:\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "Wf6H1QYvDBBB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "np.mean(to_np(y_pred).argmax(axis=1) == to_np(yt))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "IU0CJEdzDFZC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "Now we will use the optimizer to calculate which direction to step in. That is, how should we update our weights to try to decrease the loss?\n",
        "\n",
        "Pytorch has an automatic differentiation package (autograd) that takes derivatives for us, so we don't have to calculate the derivative ourselves! We just call .backward() on our loss to calculate the direction of steepest descent (the direction to lower the loss the most)."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "nDHdKsNODKBE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Before the backward pass, use the optimizer object to zero all of the\n",
        "# gradients for the variables it will update (which are the learnable weights\n",
        "# of the model)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# Backward pass: compute gradient of the loss with respect to model parameters\n",
        "l.backward()\n",
        "\n",
        "# Calling the step function on an Optimizer makes an update to its parameters\n",
        "optimizer.step()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "J-y2E5X5DW7Q",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Now, let's make another set of predictions and check if our loss is lower:\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "5hlJ2xTtDZ99",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xt, yt = next(dl)\n",
        "y_pred = net2(Variable(xt).cuda())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "2nyu722sDcrk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "l = loss(y_pred, Variable(yt).cuda())\n",
        "print(l)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "rW1zA69gDtMJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Note that we are using stochastic gradient descent, so the loss is not guaranteed to be strictly better each time. The stochasticity comes from the fact that we are using mini-batches; we are just using 64 images to calculate our prediction and update the weights, not the whole dataset.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "AOYMSJI5DsE8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "np.mean(to_np(y_pred).argmax(axis=1) == to_np(yt))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "bHPCwsG2DKTZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "If we run several iterations in a loop, we should see the loss decrease and the accuracy increase with time.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "bUkG9ZjXD1qk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for t in range(100):\n",
        "    xt, yt = next(dl)\n",
        "    y_pred = net2(Variable(xt).cuda())\n",
        "    l = loss(y_pred, Variable(yt).cuda())\n",
        "    \n",
        "    if t % 10 == 0:\n",
        "        accuracy = np.mean(to_np(y_pred).argmax(axis=1) == to_np(yt))\n",
        "        print(\"loss: \", l.data[0], \"\\t accuracy: \", accuracy)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4UWnSarfD8aq"
      },
      "cell_type": "markdown",
      "source": [
        "Put it all together in a training loop"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "ScrU0im_DKzB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def score(x, y):\n",
        "    y_pred = to_np(net2(V(x)))\n",
        "    return np.sum(y_pred.argmax(axis=1) == to_np(y))/len(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "3bcpFkRTDLHy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net2 = LogReg().cuda()\n",
        "loss=nn.NLLLoss()\n",
        "learning_rate = 1e-2\n",
        "optimizer=optim.SGD(net2.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(1):\n",
        "    losses=[]\n",
        "    dl = iter(md.trn_dl)\n",
        "    for t in range(len(dl)):\n",
        "        # Forward pass: compute predicted y and loss by passing x to the model.\n",
        "        xt, yt = next(dl)\n",
        "        y_pred = net2(V(xt))\n",
        "        l = loss(y_pred, V(yt))\n",
        "        losses.append(l)\n",
        "\n",
        "        # Before the backward pass, use the optimizer object to zero all of the\n",
        "        # gradients for the variables it will update (which are the learnable weights of the model)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        l.backward()\n",
        "\n",
        "        # Calling the step function on an Optimizer makes an update to its parameters\n",
        "        optimizer.step()\n",
        "    \n",
        "    val_dl = iter(md.val_dl)\n",
        "    val_scores = [score(*next(val_dl)) for i in range(len(val_dl))]\n",
        "    print(np.mean(val_scores))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dzm1XLLKEJrR"
      },
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent¶\n",
        "Nearly all of deep learning is powered by one very important algorithm: stochastic gradient descent (SGD). SGD can be seeing as an approximation of gradient descent (GD). In GD you have to run through all the samples in your training set to do a single itaration. In SGD you use only a subset of training samples to do the update for a parameter in a particular iteration. The subset used in each iteration is called a batch or minibatch.\n",
        "\n",
        "Now, instead of using the optimizer, we will do the optimization ourselves!"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "e9BIVAtRDKnn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net2 = LogReg().cuda()\n",
        "loss_fn=nn.NLLLoss()\n",
        "lr = 1e-2\n",
        "w,b = net2.l1_w,net2.l1_b\n",
        "\n",
        "for epoch in range(1):\n",
        "    losses=[]\n",
        "    dl = iter(md.trn_dl)\n",
        "    for t in range(len(dl)):\n",
        "        xt, yt = next(dl)\n",
        "        y_pred = net2(V(xt))\n",
        "        l = loss(y_pred, Variable(yt).cuda())\n",
        "        losses.append(loss)\n",
        "\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        l.backward()\n",
        "        w.data -= w.grad.data * lr\n",
        "        b.data -= b.grad.data * lr\n",
        "        \n",
        "        w.grad.data.zero_()\n",
        "        b.grad.data.zero_()   \n",
        "\n",
        "    val_dl = iter(md.val_dl)\n",
        "    val_scores = [score(*next(val_dl)) for i in range(len(val_dl))]\n",
        "    print(np.mean(val_scores))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "qrT7C4NWEcV2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "NH4-gH7yEcKD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "zh34euzgEcEi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CSZ1XzxYVsQG"
      },
      "cell_type": "markdown",
      "source": [
        "###Lesson 4\n",
        "\n",
        "00:00:0416 How to deal with version control and notebooks ? Make a copy and rename it with “tmp-blablabla” so it’s hidden from Git Pull\n",
        "\n",
        "\n",
        "00:18:502 Random Forest Interpretation lesson2-rf_interpretation,\n",
        "‘rf_feat_importance()’\n",
        "\n",
        "00:26:503 ‘to_keep = fi[fi.imp>0.005]’ to remove less important features,\n",
        "high cardinality variables 29m45s,\n",
        "\n",
        "00:32:155 Two reasons why Validation Score is not good or getting worse: overfitting, and validation set is not a random sample (something peculiar in it, not in Train),\n",
        "The meaning of the five numbers results in ‘print_score(m)’, RMSE of Training & Validation, R² of Train & Valid & OOB.\n",
        "We care about the RMSE of Validation set.\n",
        "\n",
        "00:35:505 How Feature Importance is normally done in Industry and Academics outside ML: they use Logistic Regression Coefficients, not Random Forests Feature/Variable Importance.\n",
        "\n",
        "00:39:503 Doing One-hot encoding for categorical variables,\n",
        "Why and how works ‘max_n_cat=7’ based on Cardinality 49m15s, ‘numericalize’\n",
        "\n",
        "00:55:053 Removing redundant features using a dendogram and '.spearmanr()'for rank correlation, ‘get_oob(df)’, ‘to_drop = []’ variables, ‘reset_rf_samples()’\n",
        "\n",
        "01:07:154 Partial dependence: how important features relate to the dependent variable, ‘ggplot() + stat_smooth()’, ‘plot_pdp()’\n",
        "\n",
        "01:21:50 What is the purpose of interpretation, what to do with that information ?\n",
        "\n",
        "01:30:154 What is EROPS / OROPS ?\n",
        "\n",
        "01:32:255 Tree interpreter\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ljUQMtb5BC9L"
      },
      "cell_type": "markdown",
      "source": [
        "####set_rf_samples()\n",
        "[00:01:50](https://youtu.be/0v93qHDqq_g?t=1m50s) Summarize the relationship between hyperparameters in Random Forests, overfitting and colinearity.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dwt51JOcBOR8"
      },
      "cell_type": "markdown",
      "source": [
        "#### oob_score = True"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "RYgYNhRFEcO-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NM2u6tyzBdrN"
      },
      "cell_type": "markdown",
      "source": [
        "####‘min_samples_leaf="
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "k1FKN4NkEbWs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cpgBn2qzDD5T"
      },
      "cell_type": "markdown",
      "source": [
        "####max_features="
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bVtWZco_DdNV"
      },
      "cell_type": "markdown",
      "source": [
        "####rf_feat_importance()\n",
        "[00:18:50](https://youtu.be/0v93qHDqq_g?t=18m50s)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5MPXlNATV1jg"
      },
      "cell_type": "markdown",
      "source": [
        "###Lesson 5\n",
        "\n",
        "00:00:0415 Review of Training, Test set and OOB score, intro to Cross-Validation (CV),\n",
        "In Machine Learning, we care about Generalization Accuracy/Error.\n",
        "\n",
        "00:11:353 Kaggle Public and Private test sets for Leaderboard,\n",
        "the risk of using a totally random validation set, rerun the model including Validation set.\n",
        "\n",
        "00:22:152 Is my Validation set truly representative of my Test set. Build 5 very different models and score them on Validation and on Test. Examples with Favorita Grocery.\n",
        "\n",
        "00:28:10 Why building a representative Test set is crucial in the Real World machine learning (not in Kaggle),\n",
        "Sklearn make train/test split or cross-validation = bad in real life (for Time Series) !\n",
        "\n",
        "00:31:045 What is Cross-Validation and why you shouldn’t use it most of the time (hint: random is bad)\n",
        "\n",
        "00:38:047 Tree interpretation revisited, lesson2-rf_interpreter.ipynb, waterfall plot for increase and decrease in tree splits,\n",
        "‘ti.predict(m, row)’\n",
        "\n",
        "00:48:508 Dealing with Extrapolation in Random Forests,\n",
        "RF can’t extrapolate like Linear Model, avoid Time variables as predictors if possible ?\n",
        "Trick: find the differences between Train and Valid sets, ie. any temporal predictor ? Build a RF to identify components present in Valid only and not in Train ‘x,y = proc_df(df_ext, ‘is_valid’)’,\n",
        "Use it in Kaggle by putting Train and Test sets together and add a column ‘is_test’, to check if Test is a random sample or not.\n",
        "\n",
        "00:59:152 Our final model of Random Forests, almost as good as Kaggle #1 (Leustagos & Giba)\n",
        "\n",
        "01:03:04 What to expect for the in-class exam\n",
        "\n",
        "01:05:04 Lesson3-rf_foundations.ipynb, writing our own Random Forests code.\n",
        "Basic data structures code, class ‘TreeEnsemble()’, np.random.seed(42)’ as pseudo random number generator\n",
        "How to make a prediction in Random Forests (theory) ?\n",
        "\n",
        "01:21:047 class ‘DecisionTree()’,\n",
        "Bonus: Object-Oriented-Programming (OOP) overview, critical for PyTorch"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1otf-gIuWDmo"
      },
      "cell_type": "markdown",
      "source": [
        "###Lesson 6\n",
        "\n",
        "Note: this lesson has a VERY practical discussion with USF students about the use of Machine Learning in business/corporation, Jeremy shares his experience as a business consultant (McKinsey) and entrepreneur in AI/ML. Deffo not PhD’s stuff, too real-life.\n",
        "\n",
        "00:00:0416 Review of previous lessons: Random Forests interpretation techniques,\n",
        "Confidence based on tree variance,\n",
        "Feature importance,\n",
        "Removing redundant features,\n",
        "Partial dependence…\n",
        "And why do we do Machine Learning, what’s the point ?\n",
        "Looking at PowerPoint ‘intro.ppx’ in Fastai GitHub: ML applications (horizontal & vertical) in real-life.\n",
        "Churn (which customer is going to leave) in Telecom: google “jeremy howard data products”,\n",
        "drive-train approach with ‘Defined Objective’ -> ‘Company Levers’ -> ‘Company Data’ -> ‘Models’\n",
        "\n",
        "00:10:011 \"In practice, you’ll care more about the results of your simulation than your predictive model directly \",\n",
        "Example with Amazon 'not-that-smart’recommendations vs optimization model.\n",
        "More on Churn and Machine Learning Applications in Business\n",
        "\n",
        "00:20:30 Why is it hard/key to define the problem to solve,\n",
        "ICYMI: read “Designing great data products” from Jeremy in March 28, 2012 ^!^\n",
        "Healthcare applications like ‘Readmission risk’. Retail applications examples.\n",
        "There’s a lot more than what you read about Facebook or Google applications in Tech media.\n",
        "Machine Learning in Social Sciences today: not much.\n",
        "\n",
        "00:37:15 More on Random Forests interpretation techniques.\n",
        "Confidence based on tree variance\n",
        "\n",
        "00:42:304 Feature importance, and Removing redundant features\n",
        "\n",
        "00:50:45 Partial dependence (or dependance)\n",
        "\n",
        "01:02:454 Tree interpreter (and a great example of effective technical communications by a student)\n",
        "Using Excel waterfall chart from Chris\n",
        "Using ‘hub.github.com5’, a command-line wrapper for git that makes you better at GitHub.\n",
        "\n",
        "01:16:15 Extrapolation, with a 20 mins session of live coding by Jeremy"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QqErEBANWYxr"
      },
      "cell_type": "markdown",
      "source": [
        "###Lesson 7\n",
        "\n",
        "00:00:0115 Review of Random Forest previous lessons,\n",
        "Lots of historical/theoritical techniques in ML that we don’t use anymore (like SVM)\n",
        "Use of ML in Industry vs Academia, Decision-Trees Ensemble\n",
        "\n",
        "00:05:301 How big the Validation Set needs to be ? How much the accuracy of your model matters ?\n",
        "Demo with Excel, T-distribution and n>22 observations in every class\n",
        "Standard Deviation : np(1-p), Standard Error (stdev mean): stdev/sqrt(n)\n",
        "\n",
        "00:18:452 Back to Random Forest from scratch.\n",
        "“Basic data structures” reviewed\n",
        "\n",
        "00:32:451 Single Branch\n",
        "Find the best split given variable with ‘find_better_split’, using Excel demo again\n",
        "\n",
        "00:45:30 Speeding things up\n",
        "\n",
        "00:55:00 Full single tree\n",
        "\n",
        "01:01:30 Predictions with ‘predict(self,x)’,\n",
        "and ‘predict_row(self, xi)’\n",
        "\n",
        "01:09:05 Putting it all together,\n",
        "Cython an optimising static compiler for Python and C\n",
        "\n",
        "01:18:012 “Your mission, for next class, is to implement”:\n",
        "Confidence based on tree variance,\n",
        "Feature importance,\n",
        "Partial dependence,\n",
        "Tree interpreter.\n",
        "\n",
        "01:20:15 Reminder: How to ask for Help on Fastai forums\n",
        "http://wiki.fast.ai/index.php/How_to_ask_for_Help1\n",
        "Getting a screenshot, resizing it.\n",
        "For lines of code, create a “Gist”, using the extension ‘Gist-it’ for “Create/Edit Gist of Notebook” with ‘nbextensions_configurator’ on Jupyter Notebook, ‘Collapsible Headings’, ‘Chrome Clipboard’, ‘Hide Header’\n",
        "\n",
        "01:23:158 We’re done with Random Forests, now we move on to Neural Networks.\n",
        "Random Forests can’t extrapolate, it just averages data that it has already seen, Linear Regression can but only in very limited ways.\n",
        "Neural Networks give us the best of both worlds.\n",
        "Intro to SGD for MNIST, unstructured data.\n",
        "Quick comparison with Fastai/Jeremy’s Deep Learning Course."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "SM5EOKoqWhxP"
      },
      "cell_type": "markdown",
      "source": [
        "###Lesson 8\n",
        "\n",
        "00:00:4519 Moving from Decision Trees Ensemble to Neural Nets with Mnist\n",
        "lesson4-mnist_sgd.ipynb notebook\n",
        "\n",
        "00:08:203 About Python ‘pickle()’ pros & cons for Pandas, vs ‘feather()’,\n",
        "Flatten a tensor\n",
        "\n",
        "00:13:451 Reminder on the jargon: a vector in math is a 1d array in CS,\n",
        "a rank 1 tensor in deep learning.\n",
        "A matrix is a 2d array or a rank 2 tensor, rows are axis 0 and columns are axis 1\n",
        "\n",
        "00:17:451 Normalizing the data: subtracting off the mean and dividing by stddev\n",
        "Important: use the mean and stddev of Training data for the Validation data as well.\n",
        "Use the ‘np.reshape()’ function\n",
        "\n",
        "00:34:252 Slicing into a tensor, ‘plots()’ from Fastai lib.\n",
        "\n",
        "00:38:201 Overview of a Neural Network\n",
        "Michael Nielsen universal approximation theorem: a visual proof that neural nets can compute any function\n",
        "Why you should blog (by Rachel Thomas)\n",
        "\n",
        "00:47:151 Intro to PyTorch & Nvidia GPUs for Deep Learning\n",
        "Website to buy a laptop with a good GPU: xoticpc.com3\n",
        "Using cloud services like Crestle.com1 or AWS (and how to gain access EC2 w/ “Request limit increase”)\n",
        "\n",
        "00:57:454 Create a Neural Net for Logistic Regression in PyTorch\n",
        "‘net = nn.Sequential(nn.Linear(28*28, 10), nn.LogSoftmax()).cuda()’\n",
        "‘md = ImageClassifierData.from_arrays(path, (x,y), (x_valid, y_valid))’\n",
        "Loss function such as ‘nn.NLLLoss()’ or Negative Log Likelihood Loss or Cross-Entropy (binary or categorical)\n",
        "Looking at Loss with Excel\n",
        "\n",
        "01:09:05 Let’s fit the model then make predictions on Validation set.\n",
        "‘fit(net, md, epochs=1, crit=loss, opt=opt, metrics=metrics)’\n",
        "Note: PyTorch doesn’t use the word “loss” but the word “criterion”, thus ‘crit=loss’\n",
        "‘preds = predict(net, md.val_dl)’\n",
        "‘preds.shape’ -> (10000, 10)\n",
        "‘preds.argmax(axis=1)[:5]’, argmax will return the index of the value which is the number itself.\n",
        "‘np.mean(preds == y_valid)’ to check how accurate the model is on Validation set.\n",
        "\n",
        "01:16:053 A second pass on “Michael Nielsen universal approximation theorem”\n",
        "A Neural Network can approximate any other function to close accuracy, as long as it’s large enough.\n",
        "\n",
        "01:18:151 Defining Logistic Regression ourselves, from scratch, not using PyTorch ‘nn.Sequential()’\n",
        "Demo explanation with drawings by Jeremy.\n",
        "Look at Excel ‘entropy_example.xlsx’ for Softmax and Sigmoid\n",
        "\n",
        "01:31:053 Assignements for the week, student question on ‘Forward(self, x)’\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "m6WodnuqWqmJ"
      },
      "cell_type": "markdown",
      "source": [
        "###Lesson 9\n",
        "\n",
        "\n",
        "\n",
        "Back to the course.\n",
        "\n",
        "00:09:012 Why write a post on your learning experience, for you and for newcomers.\n",
        "\n",
        "00:09:506 Using SGD on MNIST for digit recognition\n",
        ". lesson4-mnist_sgd.ipynb notebook\n",
        "\n",
        "00:11:301 Training the simplest Neural Network in PyTorch\n",
        "(long step-by-step demo, 30 mins approx)\n",
        "\n",
        "00:46:55 Intro to Broadcasting: “The MOST important programming concept in this course and in Machine Learning”\n",
        ". Performance comparison between C and Python\n",
        ". SIMD: “Single Instruction Multiple Data”\n",
        ". Multiple processors/cores and CUDA\n",
        "\n",
        "00:52:10 Broadcasting in details\n",
        "\n",
        "01:05:50 Broadcasting goes back to the days of APL (1950’s) and Jsoftware\n",
        ". More on Broadcasting\n",
        "\n",
        "01:12:30 Matrix Multiplication -and not-.\n",
        ". Writing our own training loop.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hxyJT0LdW0aq"
      },
      "cell_type": "markdown",
      "source": [
        "###Lesson 10\n",
        "\n",
        "00:00:0110 Fast.ai is now available on PIP !\n",
        "And more USF students publications: class-wise Processing in NLP, Class-wise Regex Functions\n",
        ". Porto Seguro’s Safe Driver Prediction (Kaggle): 1st place solution with zero feature engineering !\n",
        "Dealing with semi-supervised-learning (ie. labeled and unlabeled data)\n",
        "Data augmentation to create new data examples by creating slightly different versions of data you already have.\n",
        "In this case, he used Data Augmentation by creating new rows with 15% randomly selected data.\n",
        "Also used “auto-encoder”: the independant variable is the same as the dependant variable, as in “try to predict your input” !\n",
        "\n",
        "00:08:302 Back to a simple Logistic Regression with MNIST summary\n",
        "‘lesson4-mnist_sgd.ipynb’ notebook\n",
        "\n",
        "00:11:30 PyTorch tutorial on Autograd\n",
        "\n",
        "00:15:301 “Stream Processing” and “Generator Python”\n",
        ". “l.backward()”\n",
        ". “net2 = LogReg().cuda()”\n",
        "\n",
        "00:32:303 Building a complete Neural Net, from scratch, for Logistic Regression in PyTorch, with “nn.Sequential()”\n",
        "\n",
        "00:58:001 Fitting the model in ‘lesson4-mnist_sgd.ipynb’ notebook\n",
        "The secret in modern ML (as covered in the Deep Learning course): massively over-paramaterized the solution to your problem, then use Regularization.\n",
        "\n",
        "01:02:101 Starting NLP with IMDB dataset and the sentiment classification task\n",
        "NLP = Natural Language Processing\n",
        "\n",
        "01:03:102 Tokenizing and ‘term-document matrix’ & \"Bag-of-Words’ creation\n",
        "“trn, trn_y = texts_from_folders(f’{PATH}train’, names)” from Fastai library to build arrays of reviews and labels\n",
        "Throwing the order of words with Bag-of-Words !\n",
        "\n",
        "01:08:501 sklearn “CountVectorizer()”\n",
        "“fit_transform(trn)” to find the vocabulary in the training set and build a term-document matrix.\n",
        "“transform(val)” to apply the same transformation to the validation set.\n",
        "\n",
        "01:12:30 What is a ‘sparse matrix’ to store only key info and save memory.\n",
        "More details in Rachel’s “Computational Algebra” course on Fastai\n",
        "\n",
        "01:16:402 Using “Naive Bayes” for “Bag-of-Words” approaches.\n",
        "Transforming words into features, and dealing with the bias/risk of “zero probabilities” from the data.\n",
        "Some demo/discussion about calculating the probabilities of classes.\n",
        "\n",
        "01:25:001 Why is it called “Naive Bayes”\n",
        "\n",
        "01:30:00 The difference between theory and practice for “Naive Bayes”\n",
        "Using Logistic regression where the features are the unigrams\n",
        "\n",
        "01:35:40 Using Bigram & Trigram with Naive Bayes (NB) features"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CCYFOCaHW_-d"
      },
      "cell_type": "markdown",
      "source": [
        "###Lesson 11\n",
        "\n",
        "00:00:0112 Review of optimizing multi-layer functions with SGD\n",
        "“d(h(g(f(x)))) / dw = 0,6”\n",
        "\n",
        "00:09:453 Review of Naive Bayes & Logistic Regression for NLP with lesson5-nlp.ipynb notebook\n",
        "\n",
        "00:16:301 Cross-Entropy as a popular Loss Function for Classification (vs RMSE for Regression)\n",
        "\n",
        "00:21:301 Creating more NLP features with Ngrams (bigrams, trigrams)\n",
        "\n",
        "00:23:015 Going back to Naive Bayes and Logistic Regression,\n",
        "then ‘We do something weird but actually not that weird’ with “x_nb = x.multiply®”\n",
        "Note: watch the whole 15 mins segment for full understanding.\n",
        "\n",
        "00:39:453 ‘Baselines and Bigrams: Simple, Good Sentiment and Topic Classification’ paper by Sida Wang and Christopher Manning, Stanford U.\n",
        "\n",
        "00:43:313 Improving it with PyTorch and GPU, with Fastai Naive Bayes or ‘Fastai NBSVM++’ and “class DotProdNB(nn.Module):”\n",
        "Note: this long section includes lots of mathematical demonstration and explanation.\n",
        "\n",
        "01:17:305 Deep Learning: Structured and Time-Series data with Rossmann Kaggle competition, with the 3rd winning solution ‘Entity Embeddings of Categorical Variables’ by Guo/Berkhahn.\n",
        "\n",
        "01:21:30 Rossmann Kaggle: data cleaning & feature engineering.\n",
        "Using Pandas to join tables with ‘Left join’"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UHbW9qc8XIYz"
      },
      "cell_type": "markdown",
      "source": [
        "###Lesson 12\n",
        "\n",
        "Note: you may want to pay specific attention to the second part of this final lesson, where Jeremy brings up delicate issues on Data Science & Ethics.\n",
        "This goes beyond what most courses on DS cover.\n",
        "\n",
        "00:00:0113 Final lesson program !\n",
        "\n",
        "00:01:011 Review of Rossmann Kaggle competition with ‘lesson3-rossman.ipynb’\n",
        "Using “df.apply(lambda x:…)” and “create_promo2since(x)”\n",
        "\n",
        "00:04:301 Durations function “get_elapsed(fld, pre):” using “zip()”\n",
        "Check the notebook for detailed explanations.\n",
        "\n",
        "00:16:103 Rolling function (or windowing function) for moving-average\n",
        "Hint: learn the Pandas API for Time-Series, it’s extremely diverse and powerful\n",
        "\n",
        "00:21:40 Create Features, assign to ‘cat_vars’ and ‘contin_vars’\n",
        "‘joined_samp’, ‘do_scale=True’, ‘mapper’,\n",
        "‘yl = np.log(y)’ for RMSPE (Root Mean Squared Percent Error)\n",
        "Selecting a most recent Validation set in Time-Series, if possible of the exact same length as Test set.\n",
        "Then dropping the Validation set with ‘val_idx = [0]’ for final training of the model.\n",
        "\n",
        "00:32:30 How to create our Deep Learning algorithm (or model), using ‘ColumnarModelData.from_data_frame()’\n",
        "Use the cardinality of each variable to decide how large to make its embeddings.\n",
        "Jeremy’s Golden Rule on difference between modern ML and old ML:\n",
        "“In old ML, we controlled complexity by reducing the number of parameters.\n",
        "In modern ML, we control it by regularization. We are not much concerned about Overfitting because we use increasing Dropout or Weight-Decay to avoid it”\n",
        "\n",
        "00:39:201 Checking our submission vs Kaggle Public Leaderboard (not great), then Private Leaderboard (great!).\n",
        "Why Kaggle Public LB (LeaderBoard) is NOT a good replacement to your own Validation set.\n",
        "What is the relation between Kaggle Public LB and Private LB ?\n",
        "\n",
        "00:44:154 Course review (lessons 1 to 12)\n",
        "Two ways to train a model: one by building a tree, one with SGD (Stochastic Gradient Descent)\n",
        "Reminder: Tree-building can be combined with Bagging (Random Forests) or Boosting (GBM)\n",
        "\n",
        "00:46:153 How to represent Categorical variables with Decision Trees\n",
        "One-hot encoding a vector and its relation with embedding\n",
        "\n",
        "00:55:50 Interpreting Decision Trees, Random Forests in particular, with Feature Importance.\n",
        "Use the same techniques to interpret Neural Networks, shuffling Features.\n",
        "\n",
        "00:59:001 Why Jeremy usually doesn’t care about ‘Statistical Significant’ in ML, due to Data volume, but more about ‘Practical Significance’.\n",
        "\n",
        "01:03:101 Jeremy talks about “The most important part in this course: Ethics and Data Science, it matters.”\n",
        "How does Machine Learning influence people’s behavior, and the responsibility that comes with it ?\n",
        "As a ML practicioner, you should care about the ethics and think about them BEFORE you are involved in one situation.\n",
        "BTW, you can end up in jail/prison as a techie doing “his job”.\n",
        "\n",
        "01:08:151 IBM and the “Death’s Calculator” used in gas chamber by the Nazis.\n",
        "Facebook data science algorithm and the ethnic cleansing in Myanmar’s Rohingya crisis: the Myth of Neutral Platforms.\n",
        "Facebook lets advertisers exclude users by race enabled advertisers to reach “Jew Haters”.\n",
        "Your algorithm/model could be exploited by trolls, harassers, authoritarian governements for surveillance, for propaganda or disinformation.\n",
        "\n",
        "01:16:45 Runaway feedback loops: when Recommendation Systems go bad.\n",
        "Social Network algorithms are distorting reality by boosting conspiracy theories.\n",
        "Runaway feedback loops in Predictive Policing: an algorithm biased by race and impacting Justice.\n",
        "\n",
        "01:21:45 Bias in Image Software (Computer Vision), an example with Faceapp or Google Photos. The first International Beauty Contest judged by A.I.\n",
        "\n",
        "01:25:151 Bias in Natural Language Processing (NLP)\n",
        "Another example with an A.I. built to help US Judicial system.\n",
        "Taser invests in A.I. and body-cameras to “anticipate criminal activity”.\n",
        "\n",
        "01:34:302 Questions you should ask yourself when you work on A.I.\n",
        "You have options !"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hVl324eXKgF_"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TyAelftfK-5k"
      },
      "cell_type": "markdown",
      "source": [
        "# xgboost"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "m_QyIyGnLDaV"
      },
      "cell_type": "markdown",
      "source": [
        "[NEW R package: The XGBoost Explainer](https://medium.com/applied-data-science/new-r-package-the-xgboost-explainer-51dd7d1aa211)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Y8eP2TSdKiPD"
      },
      "cell_type": "markdown",
      "source": [
        "# Kaggle competitions (structured data)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bX5laQ2vOfMF"
      },
      "cell_type": "markdown",
      "source": [
        "##Rossmann\n",
        ">[Youtube Presentation](https://youtu.be/S9-0VKFfZas)\n",
        "\n",
        "[Github h20 and R](https://github.com/aviyashchin/Kaggle-Project-Rossman-Stores)\n",
        "\n",
        "[a fast ai notebook wtih XGBOOST](https://github.com/fastai/courses/blob/master/deeplearning2/rossman.ipynb)\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kU_dW45rZpTD"
      },
      "cell_type": "markdown",
      "source": [
        "####fix to cuda issue I had add .cuda()"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Glk8DYgKaFAc"
      },
      "cell_type": "markdown",
      "source": [
        "mixedinputmodel = MixedInputModel(emb_szs, len(df.columns)-len(cat_vars), 0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)\n",
        "\n",
        "bm = BasicModel(mixedinputmodel, 'mixedInputRegression')\n",
        "\n",
        "md = ColumnarModelData.from_data_frame(PATH, val_idx, df, yl.astype(np.float32), cat_flds=cat_vars, bs=128, test_df=df_test)\n",
        "\n",
        "learn = StructuredLearner(md, bm)\n",
        "\n",
        "learn.lr_find()"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RDs9NheDQVhq"
      },
      "cell_type": "markdown",
      "source": [
        "##Corporación Favorita Grocery Sales Forecasting\n",
        "\n",
        "[Kaggle kernal 1st place NN Model](https://www.kaggle.com/shixw125/1st-place-nn-model-public-0-507-private-0-513)\n",
        "\n",
        "[Memory optimization and EDA on entire dataset](https://www.kaggle.com/jagangupta/memory-optimization-and-eda-on-entire-dataset?scriptVersionId=1805310)\n",
        "\n",
        "\n",
        "[simple models are sometime better - kevindewalt](https://github.com/kevindewalt/ai-notebooks/blob/master/Corporaci%C3%B3n%2BFavorita%2BGrocery%2BSales%2BForecasting.ipynb)[(still Kevin but better results)](https://github.com/kevindewalt/ai-notebooks/blob/master/beat_the_winner%20(2).ipynb)\n",
        "\n",
        "alot of people used a specific moving average' technique presnted by Paulo Pinto, [here](https://www.kaggle.com/paulorzp/log-ma-and-days-of-week-means-lb-0-529/code)\n",
        "\n",
        "one of the strangest things about the contest leaders and winner is that they use kind of unorthodox approach with no dropout and huge batch sizes of 500,000.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gtYQmmaKZOEn"
      },
      "cell_type": "markdown",
      "source": [
        "###What the winners did"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "W1nhQkd-ZTWm"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/sjvasquez/web-traffic-forecasting/blob/master/cnn.py\n",
        "https://www.kaggle.com/senkin13/lstm-starter/code\n",
        "https://www.kaggle.com/tunguz/lgbm-one-step-ahead-lb-0-513\n",
        "https://www.kaggle.com/ceshine/lgbm-starter\n",
        "Like the Rossmann competiton, the private leaderboard shaked up again this time. I think luck is on our side finally.\n",
        "\n",
        "Sample Selection\n",
        "we used only 2017 data to extract features and construct samples.\n",
        "\n",
        "train data：20170531 - 20170719 or 20170614 - 20170719， different models are trained with different data set.\n",
        "\n",
        "validition: 20170726 - 20170810\n",
        "\n",
        "In fact, we tried to use more data but failed. The gap between public and private leadboard is not very stable. If we train a single model for data of 16 days, the gap will be smaller(0.002-0.003).\n",
        "\n",
        "Preprocessing\n",
        "We just filled missing or negtive promotion and target values with 0.\n",
        "\n",
        "Feature Engineering\n",
        "basic features\n",
        "category features: store, item, famlily, class, cluster...\n",
        "promotion\n",
        "dayofweek(only for model 3)\n",
        "statitical features: we use some methods to stat some targets for different keys in different time windows\n",
        "time windows\n",
        "nearest days: [1,3,5,7,14,30,60,140]\n",
        "equal time windows: [1] 16, [7] 20...\n",
        "key：store x item, item, store x class\n",
        "target: promotion, unit_sales, zeros\n",
        "method\n",
        "mean, median, max, min, std\n",
        "days since last appearance\n",
        "difference of mean value between adjacent time windows(only for equal time windows)\n",
        "useless features\n",
        "holidays\n",
        "other keys such as: cluster x item, store x family...\n",
        "Single Model\n",
        "model_1 : 0.506 / 0.511 , 16 lgb models trained for each day source code\n",
        "model_2 : 0.507 / 0.513 , 16 nn models trained for each day source code\n",
        "model_3 : 0.512 / 0.515，1 lgb model for 16 days with almost same features as model_1\n",
        "model_4 : 0.517 / 0.519，1 nn model based on @sjv's code\n",
        "Ensemble\n",
        "Stacking doesn't work well this time, our best model is linear blend of 4 single models.\n",
        "\n",
        "final submission = 0.42model_1 + 0.28 model_2 + 0.18 model_3 + 0.12 model_4\n",
        "\n",
        "public = 0.504 , private = 0.509"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kXje1kqzLd5Q"
      },
      "cell_type": "markdown",
      "source": [
        "##Grupo Bimbo\n",
        "Grupo Bimbo -NYC Data Science Academy\n",
        ">[(  Blog post  )](https://nycdatascience.com/blog/student-works/predicting-demand-historical-sales-data-grupo-bimbo-kaggle-competition/)\n",
        ">[(  Youtube presentation  )](https://youtu.be/_Pg0nIe2hsM)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "iHQhPaAE2DWa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = \"/content/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "etRTmLb73tZJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "4KHLvrRhEIVu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xIjxGvc-H1yQ"
      },
      "cell_type": "markdown",
      "source": [
        "Kaggle cli wasnt accepting my user name"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "NA-XsG0o5AcI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#holidays_events.csv\n",
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header=\"Accept-Language: en-US,en;q=0.9\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle/7391/holidays_events.csv.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1536147602&Signature=aY9TB5O4VAcLrzq%2FDFfdLgUGLfwS3Rc8pgD72DdsDF8qUYOzWqEYp39CD34IG%2BKTpXosizlr2wfSnxozasx4wjzWXX2yjbbzFyPuxAM5jqsa0IzzQvvPlrCwbM2%2BVld8sXbbTUYx2baYiEitObc3oW7dOAsPaB%2FrSSYEll4CnYUOCsLPOBxp%2FC3e4yeBqjEEet24x7FQmxiUJ0p7FWqnQ%2BBpXiLsdrbaObPo3pMSXchxymddYY6EJGL4N9MB%2F5PSL5b8u2k4QsNzJJQB%2BQV9OmfxVvrjlQpa%2BLbCGA9%2FMKY0n8Zeiyk%2F6JwzT5103n18wmaltIpBjb3Ckj7Xi7mwsw%3D%3D\" -O \"holidays_events.csv.7z\" -c\t\n",
        "\n",
        "#items.csv\n",
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header=\"Accept-Language: en-US,en;q=0.9\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle/7391/items.csv.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1536147768&Signature=XprrpPj9E7e6ddrkYfB%2FWs3IpxRAlldCfwWa2Hl0xUEyT8nQ5EqArN1Pz6HXwDWzPfn3UmcFqjdw6hmg0aarJiNO6F5e6gK1qo1rDtEFMiWt7i8kiNewP8B0fo%2BTJ7wsWKrUxKPBC%2FOGXDpMsQHTA6e5%2B2%2FtL3J0Patfa8cvYf0WfMw%2FBSVnuIMJbpzeYCR2Q6Y6JLSAFVhN%2FjxIo35ZXNQeeYJsuiGnx%2Fms12%2F2Tzw1EhBDmMgBJ9Du7af8IPzh%2B9TcDeqMs9jcgEEBrLhdNWRNUsUboUjYjImFXmtxxHxdyAGFCfzqC90l5qCzaMEp2rvaENsEtGewQV68TvM6Eg%3D%3D\" -O \"items.csv.7z\" -c\n",
        "\n",
        "#oil.csv\t\n",
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header=\"Accept-Language: en-US,en;q=0.9\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle/7391/oil.csv.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1536147874&Signature=iTkq99kNl1DZx2ePSXpMm%2FvUgxN1ysL6kFO5Ck2olk0hkvwBmJ%2B%2FrLld8yHH15EpbapgnALIr2OUnJTb4qu%2BesLdcHjmSs9i%2FT8Ed%2FvQrOdG9ygY3w1AVBj6RVcc%2B8MPmADE6%2FambK6yZzYhQYuSWjEzyC2bpCqnzhd0NYHyTBYfku9jZJYl%2BX9G0glpXHdrEGEcdFbm%2B7HPOoHknm536AcnXcUkLe31KDGPYplohGtnPVDYjCGAK8Ny%2FqvoQsPgHxMf1YHqbI8XA4rf85G8nzrcuyoLGVoXTXoccA2yFed2Q0TYbFghYCT0sD50VaZPap%2BoRfW8hceldVFuigoDjw%3D%3D\" -O \"oil.csv.7z\" -c\n",
        "\n",
        "#sample_submission.csv\n",
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header=\"Accept-Language: en-US,en;q=0.9\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle/7391/sample_submission.csv.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1536147984&Signature=UVdjbTzwCm2gBQFi1TW%2F63GRZl0JbC8mGr0zAJEVPFP5%2FhAso887uf%2F1HOKTg62RQXqQ7hVnptAoed8wiqgvbTskXvpSWFgT2hVWKOnW7btQRzmoz6Yx5rIGXpnxUygRHy%2BiUOI2svdh%2BbkMaiX4I5xYQoPPV%2FBwh5L3U8sBrWqUbyAzH12KbbLQ3eNXDbn44OiwgynpCdlFd6Hox0yXraRDDlijeS4vYMWWIiYs%2FtonTKDTucylU8%2BogU0TyIxxj2S3rri8B%2FdOTD%2Faw57A3vYcbHaipWy9bdA5pDIdHoOs%2FLqnnMVaXqBwjIpTmaIFU7%2BBrBN7w4qs5874DJzj4A%3D%3D\" -O \"sample_submission.csv.7z\" -c\n",
        "\n",
        "#stores.csv\t\n",
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header=\"Accept-Language: en-US,en;q=0.9\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle/7391/stores.csv.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1536148024&Signature=sxMItptUQ%2FZAR2Tj0n57VVvNJlmyf2px3Uribad7ZyH0X6vRpV4njq5y3O3sE2tOJCQGwEoh4Kj75Ip7oD2bv%2BVdI0QAzruUV8OormOERXMGh18ttCzOLUsYlV2QCOUR%2BH2BOUrqcKAH07SMPeMJEgJE8%2FBQBPSax9VdWqZW7o%2Ft%2BWuiORXMiJgJITouGfkdk8fA9M%2Buh6NNOeQ9R1XeCQzx98YI1UJCSImzZcjw3COwAkLAXOlHWqwrotiopfzO%2BqqU%2BUYRrB%2Bxl6qXfWv7Fb%2BQDcE1rGwOcjnEZcjqnmSHVkRwO5vwX8daDKoE9%2FZ%2BbGsatDZx1XYzWo9sALbuSw%3D%3D\" -O \"stores.csv.7z\" -c\n",
        "\n",
        "#test.csv\t\n",
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header=\"Accept-Language: en-US,en;q=0.9\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle/7391/test.csv.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1536148082&Signature=l34ktm%2F9g7SeZIp%2BuXWPrQB3kxMXhn2i7g9iZdbWxZnF038WRe2pudRmxeeIzI1amgdS%2Fclcdmb293aVr4cuoJNV82Qqqvudpzn1d8I7yA6qaJcoD1HgdqdJ2h3T8KAGx%2Blu1wvU%2FW%2FAKh28IE9v33DQUSf3PE0sXOMsqmNda8l5nhBnciLURknFMSOQ6GsohXpYsr5h0AZlZW0jBpTAcUW6d3SlfLUCr2TPqbqbl4pG9NutZWK2AkOtkm5uN%2Fr9GIriL9CYXvo%2FypI6Sc9f%2F82wdGtfpEekxiLdNOLJxKOWeivBBqNuXLmMu4VJiot%2BFEvFjgKIsdInYPiI4L0Yzg%3D%3D\" -O \"test.csv.7z\" -c\n",
        "\n",
        "#train.csv\t\n",
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header=\"Accept-Language: en-US,en;q=0.9\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle/7391/train.csv.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1536148201&Signature=Wqmht%2BUPfzbtCxfpZ9oQ9DE4%2B8MYz7LvFcGDi5PBNTRCX4N1xrN35fB5eEs15xkfLrlMFupl7YQuK48VPVEEiT39vsuvcE9hH%2FJFn3slK%2FlWSzboCuY2h3H3%2FYZYJgVSxPRY2lLog4XvOOrjDH4KdcqGaUmFN0c8C84sVoQNubPkCPZWk6FtHQ4xlZ5i%2FNwB3XA5LtMp4V2D42UfiScQj2HlM32h0jXFzN6BHHNnA5HtTXhFXref0pVRedYOQfZhe%2BUZlQaaKCJqLLqp9pKEFA0937xVWJT142QGbV10ggfcrSPBPWnWrln85YXxL253LievGDJ72zFBUb2OBScAjQ%3D%3D\" -O \"train.csv.7z\" -c\n",
        "\n",
        "#transactions.csv\n",
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header=\"Accept-Language: en-US,en;q=0.9\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle/7391/transactions.csv.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1536148134&Signature=seXepdUDNa1NWAYR3bLYZD%2FCM2NTQoV0ZZutXMWDURu85Ix8MiFcSj%2FGoRge2UvEZ8RAV8x0KTzDj3lh0V3bLKBIWDjqARnOkZz3LP1jGNnJJfeLzlSRe6vzscZiE1bdI8EnhbLxiJrsfp4baKYyllRqi0sgDmSpb5K4XbYmmUN1w%2BaKRIJCfISPUAid4iEjXdwISCksJFeBIliY4l5ESff6%2FR3FFoS7UxSOP1QZFw1SBg4i3sO33VFD6ZKj65x5%2FZ5t3alOGkybboSV1DOr8rnT2nPxx8VuOWyapo2WpXDtscqWfIoVfs6Q1agjn7yb9aGlw61oqmWipYLRDJUGSg%3D%3D\" -O \"transactions.csv.7z\" -c\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S-TdYD-5H_us",
        "outputId": "3ec3eba2-551e-4925-d624-206554380ead",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  \u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34minput\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  sample_submission.csv.7z\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "qpM7BRkbJGws",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p data/kaggle/favorita-grocery-sales-forecasting"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jbpHfdYELgqT",
        "outputId": "ecefb58a-0483-4901-9beb-a11617157f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "ls data/kaggle/favorita-grocery-sales-forecasting/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "holidays_events.csv  oil.csv     test.csv   transactions.csv\r\n",
            "items.csv            stores.csv  train.csv\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Su1DQc_eLYdw",
        "outputId": "9cb6cc2f-6c1f-4ef8-9ab0-61805f97f087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "# In order to unzip the 7z files, need to install p7zip\n",
        "# This was helpful: http://forums.fast.ai/t/unzipping-tar-7z-files-in-google-collab-notebook/14857/4\n",
        "!apt-get install p7zip-full"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "p7zip-full is already the newest version (16.02+dfsg-4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "jQsB2jJtLvyt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Unzip the 7zip files\n",
        "# -d: which file to un7zip\n",
        "#depending on what you are using you might need to but a ~ infront of the path, this is to signify root, but this is not used in colab\n",
        "# example is data/kaggle/favorita-grocery-sales-forecasting  vs ~/data/kaggle/favorita-grocery-sales-forecasting\n",
        "\n",
        "!p7zip -d items.csv.7z   && mv items.csv   data/kaggle/favorita-grocery-sales-forecasting\n",
        "!p7zip -d transactions.csv.7z   && mv transactions.csv   data/kaggle/favorita-grocery-sales-forecasting\n",
        "!p7zip -d stores.csv.7z   && mv stores.csv   data/kaggle/favorita-grocery-sales-forecasting   \n",
        "!p7zip -d oil.csv.7z    && mv oil.csv   data/kaggle/favorita-grocery-sales-forecasting\n",
        "!p7zip -d train.csv.7z    && mv train.csv   data/kaggle/favorita-grocery-sales-forecasting\n",
        "!p7zip -d test.csv.7z    && mv test.csv   data/kaggle/favorita-grocery-sales-forecasting\n",
        "!p7zip -d holidays_events.csv.7z    && mv holidays_events.csv   data/kaggle/favorita-grocery-sales-forecasting\n",
        "\n",
        "#if the files are already in the correct folder you would just choose which files to unzip\n",
        "# example   !p7zip -d ~/data/planet/train-jpg.tar.7z "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "HyQ7XXSyScPJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mv holidays_events.csv   data/kaggle/favorita-grocery-sales-forecasting"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "0EtmRn-nEUAx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#1 eda\n",
        "\n",
        "#basic\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "#viz\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "color = sns.color_palette()\n",
        "\n",
        "#others\n",
        "import subprocess\n",
        "from subprocess import check_output\n",
        "import gc\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8OdaDvp9EfMC",
        "outputId": "34af4060-4104-4c3c-80f0-8d072eaf3460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "#1 eda\n",
        "\n",
        "files=check_output([\"ls\", \"data/kaggle/favorita-grocery-sales-forecasting\"]).decode(\"utf8\")\n",
        "#Check the number of row of each file\n",
        "for file in files.split(\"\\n\"):\n",
        "    path='data/kaggle/favorita-grocery-sales-forecasting/'+file\n",
        "    popenobj=subprocess.Popen(['wc', '-l', path], stdout=subprocess.PIPE, \n",
        "                                              stderr=subprocess.PIPE)\n",
        "    result,error= popenobj.communicate()\n",
        "    print(\"The file :\",file,\"has :\",result.strip().split()[0],\"rows\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The file : holidays_events.csv has : b'351' rows\n",
            "The file : items.csv has : b'4101' rows\n",
            "The file : oil.csv has : b'1219' rows\n",
            "The file : stores.csv has : b'55' rows\n",
            "The file : test.csv has : b'3370465' rows\n",
            "The file : train.csv has : b'125497041' rows\n",
            "The file : transactions.csv has : b'83489' rows\n",
            "The file :  has : b'0' rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "9WGFzrUTdGyO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for df in pd.read_csv('matrix.txt',sep=',', header = None, chunksize=1):\n",
        "    #do something\n",
        "    \n",
        "df = pd.read_csv('matrix.txt',sep=',', header = None, skiprows=1000, chunksize=1000)\n",
        "\n",
        "\n",
        "df = pd.read_csv('matrix.txt',sep=',', header = None, skiprows= 1000, nrows=1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "WlvI90oKEL8V",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PATH = 'data/kaggle/favorita-grocery-sales-forecasting/'\n",
        "#create a list of tables so yoy can loop through them to bring them into pandas\n",
        "\n",
        "table_names = ['holidays_events', 'items', 'oil', 'stores', 'test', 'train', 'transactions']\n",
        "\n",
        "#read the tables\n",
        "#optional param for read_csv\n",
        "#sep=',', \n",
        "#header = None, \n",
        "#skiprows= 1000, \n",
        "#nrows=1000\n",
        "\n",
        "tables = [pd.read_csv(f'{PATH}{fname}.csv', low_memory=False, chunksize=1000) for fname in table_names]\n",
        "\n",
        "\n",
        "holidays_events, items, oil, stores, test, train, transactions = tables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "_g68XG9YWOxD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#check memory use for the two biggest files - train and test\n",
        "mem_train = train.memory_usage(index=True).sum()\n",
        "mem_test=test.memory_usage(index=True).sum()\n",
        "print(\"train dataset uses \",mem_train/ 1024**2,\" MB\")\n",
        "print(\"test dataset uses \",mem_test/ 1024**2,\" MB\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "t1qkFoU2bATl"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "i7ljjyFmbZzg"
      },
      "cell_type": "markdown",
      "source": [
        "##Corpi"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rJjOtpZMfAPO"
      },
      "cell_type": "markdown",
      "source": [
        "###[multiNLI](https://www.nyu.edu/projects/bowman/multinli/)\n",
        ">might be better version of SNLI"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6_UUNqA1dWci"
      },
      "cell_type": "markdown",
      "source": [
        "###[paraphrase.org pairs database](http://paraphrase.org/#/download)\n",
        "\n",
        "In addition, a few people are now trying to create more pairs using a technique known as back-translation (e.g. see https://arxiv.org/abs/1706.018471). The idea is you use machine translation to translate a sentence from, say, English to French, and then translate back again from French to English to generate a sentence with the same meaning, but different words."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "o1P2-rxYcYjm"
      },
      "cell_type": "markdown",
      "source": [
        "###[Stanford Natural Language Inference corpus (SNLI)](https://nlp.stanford.edu/projects/snli/)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rMPX6fzqfXMg"
      },
      "cell_type": "markdown",
      "source": [
        "[multiNLI](https://www.nyu.edu/projects/bowman/multinli/), might be better"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lkAoUfBPcRh6"
      },
      "cell_type": "markdown",
      "source": [
        "###[Microsoft Paraphrase Corpus](https://www.microsoft.com/en-us/download/details.aspx?id=52398)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7AbJCyKnclD1"
      },
      "cell_type": "markdown",
      "source": [
        "Said to be too small to be of much use training a neural net"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pnkKueaZbphY"
      },
      "cell_type": "markdown",
      "source": [
        "###[Quora pairs corpus](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qEWStwNMb-QD"
      },
      "cell_type": "markdown",
      "source": [
        "apparently it is not a good representation of real world data"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Yt9aaGd1bD0M"
      },
      "cell_type": "markdown",
      "source": [
        "## Sentence similarity"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "g2X3M3iCfz4e"
      },
      "cell_type": "markdown",
      "source": [
        "[Siamese architecture](https://www.quora.com/What-are-Siamese-neural-networks-what-applications-are-they-good-for-and-why) is pretty good for similarity measurements."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uSLSA9o_bI5e"
      },
      "cell_type": "markdown",
      "source": [
        "[nice fast.ai post tons of info](http://forums.fast.ai/t/sentence-similarity/16541)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CZH4m5xcW-MQ"
      },
      "cell_type": "markdown",
      "source": [
        "# Tips\n",
        "\n",
        "start training with smaller data sets, train, and add more features and data. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "quuMunImELLw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NSeKpBk7EMF8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "trNS3XBREMBU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Lbm93Og_EL8j",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-sLIrSgvEL29",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZvBnSMSYELxu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qvDagH4xELrI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "R37p646lELmM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}